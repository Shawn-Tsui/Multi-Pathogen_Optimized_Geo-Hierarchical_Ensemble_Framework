{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZvEaSkDvuLpf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718320187063,"user_tz":-480,"elapsed":30661,"user":{"displayName":"Shawn XU","userId":"03794891599117456463"}},"outputId":"ae77e439-178c-457a-a2d3-c0742acdb01a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# Model Function"],"metadata":{"id":"xn9c-rXDz311"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import pandas as pd\n","import torch\n","import pandas as pd\n","from torch import nn\n","import torch.nn.functional as F\n","pd.options.mode.chained_assignment = None\n","def prepare_data_main_model(df, seq_length, output_size, date_range):\n","    '''\n","    df: pandas df contain all the data\n","    seq_length: number of days consider as input\n","    output_size: number of days to predict\n","    date_range: length of history to consider\n","    Output:\n","    - full_data: Prepared data as a list of tuples (input_sequence, target_labels).\n","    - state_ordered: List of unique states in the data.\n","    '''\n","    full_data = []\n","    state_ordered = []\n","\n","    for state in df.index.get_level_values('fips').unique():\n","        df_state = df.iloc[df.index.get_level_values('fips') == state]\n"," # If the length of data is less than or equal to date_range, consider all data\n","        if len(df_state) <= date_range:\n","            L = len(df_state.to_numpy())\n","            train_state = []\n","            for i in range(L - seq_length - output_size + 1):\n","                train_seq = df_state.to_numpy()[i:i + seq_length]\n","                train_label = df_state.to_numpy()[i:i + seq_length + output_size][seq_length:seq_length + output_size,\n","                              0]\n","                train_state.append((train_seq, train_label))\n","\n","            for x in train_state:\n","                full_data.append(x)\n","            state_ordered.append(state)\n","# If data is larger than date_range, consider the most recent date_range data\n","        else:\n","            df_state = df.iloc[df.index.get_level_values('fips') == state][-date_range:]\n","\n","            train_state = []\n","\n","            L = len(df_state.to_numpy())\n","            for i in range(L - seq_length - output_size + 1):\n","                train_seq = df_state.to_numpy()[i:i + seq_length]\n","                train_label = df_state.to_numpy()[i:i + seq_length + output_size][seq_length:seq_length + output_size,\n","                              0]\n","                train_state.append((train_seq, train_label))\n","\n","            for x in train_state:\n","                full_data.append(x)\n","            state_ordered.append(state)\n","    return full_data, state_ordered\n","\n","def splitdata(full_data, ratio, batch_size):\n","    '''\n","    Split the prepared data into training and testing datasets.\n","\n","    Parameters:\n","    - full_data: Prepared data as a list of tuples (input_sequence, target_labels).\n","    - ratio: Ratio of data to be used for training (e.g., 0.8 for 80% training, 20% testing).\n","    - batch_size: Size of mini-batches for DataLoader.\n","\n","    Output:\n","    - train_loader: DataLoader for the training dataset.\n","    - test_loader: DataLoader for the testing dataset.\n","    '''\n","    train_size = int(ratio * len(full_data))\n","    test_size = len(full_data) - train_size\n","    train_dataset, test_dataset = torch.utils.data.random_split(full_data, [train_size, test_size])\n","\n","    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                               batch_size=batch_size,\n","                                               shuffle=True, drop_last=True)\n","\n","    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                              batch_size=batch_size,\n","                                              shuffle=True, drop_last=True)\n","\n","    return train_loader, test_loader\n","\n","\n","class LSTM(nn.Module):\n","\n","    def __init__(self, input_size, hidden_layer_size, num_layers, output_size, dropout_rate):\n","        super().__init__()\n","\n","        self.input_size = input_size\n","\n","        self.hidden_layer_size = hidden_layer_size\n","\n","        self.num_layers = num_layers\n","\n","        self.output_size = output_size\n","\n","        self.lstm = nn.LSTM(self.input_size, hidden_layer_size, num_layers, batch_first=True, dropout=dropout_rate)\n","\n","        self.linear = nn.Linear(hidden_layer_size, 1000)\n","\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","        self.linear2 = nn.Linear(1000, output_size)\n","\n","    def forward(self, input_seq):\n","        h = (torch.zeros(self.num_layers, input_seq.size(0), self.hidden_layer_size).to(device),\n","             torch.zeros(self.num_layers, input_seq.size(0), self.hidden_layer_size).to(device))\n","\n","        lstm_out, self.hidden_cell = self.lstm(input_seq, h)\n","\n","        # only return the results for last sequence\n","        lstm_out = lstm_out[:, -1, :]\n","        predictions = self.linear(lstm_out)\n","        #predictions = F.relu(predictions)\n","        #predictions = F.softplus(predictions)\n","        predictions = F.leaky_relu(predictions, negative_slope=0.01)\n","        predictions = self.dropout(predictions)\n","        predictions = self.linear2(predictions)\n","\n","        return predictions"],"metadata":{"id":"Km4eioHNz9I7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" ### Training"],"metadata":{"id":"elGsAXQSODba"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import torch\n","from sklearn.preprocessing import MinMaxScaler\n","from torch import nn\n","from tqdm import trange\n","from datetime import date, timedelta\n","\n","from collections import OrderedDict\n","from collections import namedtuple\n","from itertools import product\n","\n","import torch.nn.functional as F\n","\n","def LSTM_train(test_week,option):\n","\n","    data_file = '/content/drive/MyDrive/Covid forecasting/Data/smoothed_rate_withcovidww.csv'\n","\n","    model_file = '/content/drive/MyDrive/Covid forecasting/Result/US level/model/'\n","    prediction_file = '/content/drive/MyDrive/Covid forecasting/Result/US level/raw output/'\n","    summary_file ='/content/drive/MyDrive/Covid forecasting/Result/US level/raw output/'\n","    def perdelta(start, end, delta):\n","        curr = start\n","        while curr < end:\n","            yield curr\n","            curr += delta\n","    global device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    # perform grid search to find a best model\n","    if option == 0:\n","      params = OrderedDict(\n","          target = ['covid_rate'], ### Our target variable\n","          lr = [0.0001,0.0005],\n","          batch_size = [2],\n","          seq_length = [4,6,8,10], # length of input sequence to predict values\n","          output_size = [4], # we are predicting next four weeks admissions\n","          num_pred_features = [1],\n","          week_range = [300], # number of historical data (per state) used to train the model\n","          input_size = [15], # total number of input features\n","          hidden_layer_size = [32,64,128,256], #test\n","          num_layers = [1], #test\n","          ratio = [0.8],  # ratio of training set and validation set\n","          num_epochs = [50],\n","          dropout_rate = [0.8],\n","          lossfunc = [nn.SmoothL1Loss(beta=0.05, reduction = 'sum'),\n","\n","                      ]\n","      )\n","\n","\n","      class RunBuilder():\n","          @staticmethod\n","          def get_runs(params):\n","\n","              Run = namedtuple('Run', params.keys())\n","\n","              runs = []\n","              for v in product(*params.values()):\n","                  runs.append(Run(*v))\n","\n","              return runs\n","\n","      pd.options.mode.chained_assignment = None\n","\n","      runs = RunBuilder.get_runs(params)\n","#directly run the best model\n","    elif option == 1:\n","      params = OrderedDict(\n","          target = ['rate'], ### Our target variable\n","          lr = [0.0001],\n","          batch_size = [2],\n","          seq_length = [8], # length of input sequence to predict values\n","          output_size = [4], # we are predicting next four weeks admissions\n","          num_pred_features = [1],\n","          week_range = [64], # number of historical data (per state) used to train the model\n","          input_size = [3], # total number of input features\n","          hidden_layer_size = [128], #test\n","          num_layers = [1], #test\n","          ratio = [0.8],  # ratio of training set and validation set\n","          num_epochs = [50],\n","          dropout_rate = [0.8],\n","          lossfunc = [nn.SmoothL1Loss(beta=0.05, reduction = 'sum'),\n","\n","                      ]\n","      )\n","\n","\n","      class RunBuilder():\n","          @staticmethod\n","          def get_runs(params):\n","\n","              Run = namedtuple('Run', params.keys())\n","\n","              runs = []\n","              for v in product(*params.values()):\n","                  runs.append(Run(*v))\n","\n","              return runs\n","\n","      pd.options.mode.chained_assignment = None\n","\n","      runs = RunBuilder.get_runs(params)\n","\n","\n","\n","\n","    # output dict\n","    columns = [\n","        'Model',\n","        'lr',\n","        'batch_size',\n","        'seq_length',\n","        #'week_range',\n","        'hidden_layer_size',\n","        'num_layers',\n","        'lossfunc',\n","        'mae_validation',\n","        'mae_validation_1w',\n","        'mae_validation_2w',\n","        'mae_validation_3w',\n","        'mae_validation_4w',\n","        'Target'\n","    ]\n","    # output dataframe\n","    df_summary = pd.DataFrame(columns=columns)\n","\n","\n","    pd.options.mode.chained_assignment = None\n","\n","\n","\n","    for run in runs:\n","        if run.target == 'covid_rate':\n","            results_file = 'covid_rate'\n","\n","        validation_predictions = []\n","        validation_labels = []\n","        # read data\n","        df = pd.read_csv(data_file)\n","        #df = df.rename(columns={'End Date': 'Week_end', 'State': 'fips'})\n","        df['Week_end'] = pd.to_datetime(df['Week_end'])\n","        # too many missing values in State'11'\n","        df =  df[df['fips'] != 11]\n","        df = df.drop(['PRCP_mean', 'SNOW_mean', 'TMAXDELTA', 'TMINDELTA','s05_value','scontrol_value'], axis=1)\n","    # select region\n","        df['fips'] = df['fips'].astype(str).str.zfill(2)\n","        #df = df[['fips','Week_end','difference','rate','smoothed_admission']]\n","        # test set is the week from 2024-01-20\n","        start_date1 = pd.to_datetime(test_week) - timedelta(weeks=run.seq_length )\n","        end_date1 = pd.to_datetime(test_week) + timedelta(weeks= 1 )\n","\n","\n","        df_test = df[(df['Week_end'] > start_date1) & (df['Week_end'] < end_date1)] # Use data after 2023-11-01 as testing set\n","        df_test = df_test.set_index(['fips', 'Week_end'])\n","        df_test1 = df_test.copy()\n","        #choose data before 2024-01-06 as training data and validation data and after 2022-08-01\n","        df_train = df[(df['Week_end'] < pd.to_datetime(test_week) )& (df['Week_end'] >pd.to_datetime('2022-08-01')) ] # Use data before 11/1 as training set\n","        #df_train = df[(df['Week_end'] < pd.to_datetime('2023-05-01') )& (df['Week_end'] >pd.to_datetime('2022-08-01')) ]\n","        df_train = df_train.set_index(['fips', 'Week_end'])\n","\n","        #rescale the data from 0 to 1, normalization\n","        first_col = df_train.pop(run.target)\n","        df_train.insert(0, run.target, first_col)\n","        first_col_2 = df_test.pop(run.target)\n","        df_test.insert(0, run.target, first_col_2)\n","\n","        scaler = MinMaxScaler(feature_range=(0, 1))\n","\n","        scaler.fit(df_train.iloc[:, 1:])\n","        train_features_normalized = scaler.transform(df_train.iloc[:, 1:])\n","        test_feature_normalized = scaler.transform(df_test.iloc[:, 1:])\n","\n","        scaler_target = MinMaxScaler(feature_range=(0, 1))\n","        scaler_target.fit(np.asarray(df_train.iloc[:, 0]).reshape(-1, 1))\n","        train_target_normalized = scaler_target.transform(np.asarray(df_train.iloc[:, 0]).reshape(-1, 1))\n","        test_target_normalized = scaler_target.transform(np.asarray(df_test.iloc[:, 0]).reshape(-1, 1))\n","        #train_target_normalized = np.log(df_train.iloc[:, 0] + 1)\n","        #test_target_normalized = np.log(df_test.iloc[:, 0] + 1)\n","\n","        df_train.iloc[:, 1:] = train_features_normalized\n","        df_train.iloc[:, 0] = train_target_normalized\n","        df_test.iloc[:, 1:] = test_feature_normalized\n","        df_test.iloc[:, 0] = test_target_normalized\n","        '''\n","\n","        desired_order = ['rate', 'difference', 'smoothed_admission'] + [col for col in df_train.columns if col not in ['rate', 'difference', 'smoothed_admission']]\n","        df_train = df_train[desired_order]\n","        df_test = df_test[desired_order]\n","        #rescale the data from 0 to 1, normalization\n","        scaler_general = MinMaxScaler(feature_range=(0, 1))\n","        scaler_diff_smooth = MinMaxScaler(feature_range=(0, 100))\n","        scaler_target = MinMaxScaler(feature_range=(0, 1))\n","\n","        # Assuming the first column 'target' is your target variable\n","        # Scale the target variable\n","        train_target_normalized = scaler_target.fit_transform(df_train.iloc[:, 0].values.reshape(-1, 1))\n","        test_target_normalized = scaler_target.transform(df_test.iloc[:, 0].values.reshape(-1, 1))\n","\n","        # Scale 'difference' and 'smoothed_admission' with a specific range [0, 10]\n","        scaler_diff_smooth.fit(df_train.iloc[:, 1:3])  # Now correctly indexes 'difference' and 'smoothed_admission'\n","        train_diff_smooth_normalized = scaler_diff_smooth.transform(df_train.iloc[:, 1:3])\n","        test_diff_smooth_normalized = scaler_diff_smooth.transform(df_test.iloc[:, 1:3])\n","\n","        # Scale the rest of the features (excluding target and the two specific features) normally\n","        scaler_general.fit(df_train.iloc[:, 3:])  # Correctly indexes the remaining features\n","        train_features_normalized = scaler_general.transform(df_train.iloc[:, 3:])\n","        test_features_normalized = scaler_general.transform(df_test.iloc[:, 3:])\n","\n","        # Reassemble the dataframes\n","        df_train.iloc[:, 0] = train_target_normalized.ravel()  # Target column\n","        df_train.iloc[:, 1:3] = train_diff_smooth_normalized  # 'difference' and 'smoothed_admission'\n","        df_train.iloc[:, 3:] = train_features_normalized  # The rest of the features\n","\n","        df_test.iloc[:, 0] = test_target_normalized.ravel()\n","        df_test.iloc[:, 1:3] = test_diff_smooth_normalized\n","        df_test.iloc[:, 3:] = test_features_normalized\n","        '''\n","        '''\n","        Training\n","        '''\n","        # prepare the training and validation data (sequance format)\n","        full_data_main, state_ordered = prepare_data_main_model(df_train, run.seq_length,\n","                                                                run.output_size, run.week_range)\n","\n","        model_main = LSTM(run.input_size, run.hidden_layer_size, run.num_layers, run.output_size ,\n","                          run.dropout_rate).to(device)\n","        # split the training data into training set and validation set\n","        train_loader_main, test_loader_main = splitdata(full_data_main, run.ratio, run.batch_size)\n","\n","        loss_function = run.lossfunc\n","      # add l2 regularization\n","        optimizer_main = torch.optim.Adam(model_main.parameters(), lr=run.lr,weight_decay=1e-5)\n","\n","        track_loss_train = []\n","        track_loss_test = []\n","        best_loss = 100000\n","        ###\n","\n","        best_mae_numerical = None\n","        mae_numerical_weekly = [None] * 4\n","\n","        best_mae_numerical_weekly = [None] * 4\n","        for i in trange(run.num_epochs):\n","    # Set the model in training mode\n","            model_main.train()\n","            epoch_loss_train = 0\n","    # Iterate over the training data\n","            for i, (seq, labels) in enumerate(train_loader_main):\n","                seq, labels = seq.to(device), labels.to(device)\n","                optimizer_main.zero_grad()\n","                seq = torch.as_tensor(seq).reshape(-1, run.seq_length, run.input_size)\n","                # Initialize the hidden state of the LSTM\n","                model_main.hidden_cell = (torch.zeros(run.num_layers, seq.size()[0], run.hidden_layer_size).to(device),\n","                              torch.zeros(run.num_layers, seq.size()[0], run.hidden_layer_size).to(device))\n","\n","                # Forward pass to get predictions\n","                y_pred = model_main(seq.float())\n","\n","                # Calculate the loss and perform backpropagation\n","                single_loss = loss_function(y_pred, torch.as_tensor(labels).float())\n","                single_loss.backward()\n","                optimizer_main.step()\n","\n","                epoch_loss_train += single_loss.item()\n","            # Track the training loss for this epoch\n","            track_loss_train.append(epoch_loss_train)\n","\n","            with torch.no_grad():\n","              # Set the model in evaluation mode\n","                model_main.eval()\n","                epoch_loss_test = 0\n","                ########\n","                # Initialize lists to store validation predictions and labels\n","                validation_predictions = []\n","                validation_labels = []\n","                for i, (seq, labels) in enumerate(test_loader_main):\n","                    seq, labels = seq.to(device), labels.to(device)\n","                    seq = torch.as_tensor(seq).reshape(-1, run.seq_length, run.input_size)\n","                    model_main.hidden_cell = (torch.zeros(run.num_layers, seq.size(0), run.hidden_layer_size).to(device),\n","                                    torch.zeros(run.num_layers, seq.size(0), run.hidden_layer_size).to(device))\n","                    y_pred = model_main(seq.float())\n","\n","                    single_loss = loss_function(y_pred, torch.as_tensor(labels).float())\n","                    epoch_loss_test += single_loss.item()\n","                    ######\n","                    # Store predictions and labels for later evaluation\n","                    validation_predictions.extend(y_pred.cpu().numpy())\n","                    validation_labels.extend(labels.cpu().numpy())\n","                track_loss_test.append(epoch_loss_test)\n","            # Combine validation predictions and labels, and perform inverse scaling to get true value\n","            validation_predictions = np.concatenate(validation_predictions, axis=0)\n","            validation_labels = np.concatenate(validation_labels, axis=0)\n","            validation_predictions = scaler_target.inverse_transform(validation_predictions.reshape(-1, 1)).reshape(-1, 4)\n","            validation_labels = scaler_target.inverse_transform(validation_labels.reshape(-1, 1)).reshape(-1, 4)\n","            #validation_predictions = (np.exp(validation_predictions) - 1).reshape(-1, 4)\n","            #validation_labels = (np.exp(validation_labels) - 1).reshape(-1, 4)\n","            # Calculate total and weekly MSE and accuracy for validation set\n","            #mse_numerical = np.mean((validation_predictions - validation_labels)**2)\n","            mae_numerical = np.mean(np.abs(validation_predictions - validation_labels))\n","            for week in range(4):\n","                # Calculate weekly MSE\n","                #mse_numerical_week = np.mean((validation_predictions[:, week] - validation_labels[:, week])**2)\n","                #mse_numerical_weekly[week] = mse_numerical_week\n","                mae_numerical_week = np.mean(np.abs(validation_predictions[:, week] - validation_labels[:, week]))\n","                mae_numerical_weekly[week] = mae_numerical_week\n","            if epoch_loss_test  < best_loss:\n","              # Update the best loss and save the model; perform early stopping\n","                best_mae_numerical = mae_numerical\n","                best_mae_numerical_weekly = mae_numerical_weekly\n","\n","                best_loss = epoch_loss_test\n","                print('Train Loss: ', epoch_loss_train)\n","                print('Test Loss: ', epoch_loss_test)\n","                es = 0\n","                torch.save(model_main.state_dict(),\n","                              model_file + run.target +'_' +test_week+'_'  +\n","                              str(run.lr) + '_' + str(run.hidden_layer_size)\\\n","                              + '_' + str(run.seq_length) + '_' + str(run.num_layers)\\\n","                              + '_LSTM_weights.pt')\n","            else:\n","                es += 1\n","                print(\"Counter {} of 5\".format(es))\n","                print('Train Loss: ', epoch_loss_train)\n","                print('Test Loss: ', epoch_loss_test)\n","\n","\n","            if es > 4:\n","                print(\"Early stopping with best_loss: \", best_loss, \"and test_loss for this epoch: \",\n","                          epoch_loss_test,\n","                          \"...\")\n","\n","                break\n","\n","        '''\n","        Prediction\n","        '''\n","        df_output = pd.DataFrame(columns=['fips', 'Week_end', 'Prediction_1w',\n","                                          'Prediction_2w', 'Prediction_3w',\n","                                          'Prediction_4w'])\n","      #test set starts with 2023-11-01\n","        test_weeks = df_test[df_test.index.get_level_values('Week_end') \\\n","                          >= pd.to_datetime(test_week)].index.get_level_values('Week_end').unique()\n","        test_states = df_test.index.get_level_values('fips').unique()\n","\n","        m_state_dict_main = torch.load(model_file + run.target+'_' +test_week+ '_' +\n","                                      str(run.lr) + '_' + str(run.hidden_layer_size) \\\n","                                      + '_' + str(run.seq_length) + '_' + str(run.num_layers) \\\n","                                      + '_LSTM_weights.pt')\n","\n","        model_main = LSTM(run.input_size, run.hidden_layer_size, run.num_layers,\n","                          run.output_size, run.dropout_rate).to(device)\n","        model_main.load_state_dict(m_state_dict_main)\n","\n","        with torch.no_grad():\n","            #model_main.eval()\n","            for fips in test_states:\n","                for week in test_weeks:\n","                    seq = df_test[(df_test.index.get_level_values('fips') == fips)\\\n","                    & (df_test.index.get_level_values('Week_end') <= week)][-run.seq_length:].to_numpy()\n","\n","                    seq = torch.tensor(seq).reshape(-1, run.seq_length, run.input_size).to(device)\n","\n","\n","                    model_main.hidden_cell = (torch.zeros(run.num_layers, seq.size(0), run.hidden_layer_size).to(device),\n","                                    torch.zeros(run.num_layers, seq.size(0), run.hidden_layer_size).to(device))\n","                    prediction = model_main(seq.float())\n","\n","                    prediction = scaler_target.inverse_transform(prediction.cpu().detach().numpy().reshape(-1, 1))\n","                    #prediction = np.exp(prediction.cpu().detach().numpy().reshape(-1, 1)) - 1\n","                    # save prediction results\n","                    dic = {\n","                        'fips' : fips,\n","                        'Week_end' : week,\n","                        'Prediction_1w' : prediction[0].item(),\n","                        'Prediction_2w' : prediction[1].item(),\n","                        'Prediction_3w' : prediction[2].item(),\n","                        'Prediction_4w' : prediction[3].item()\n","\n","                    }\n","\n","                    df_output = pd.concat([df_output, pd.DataFrame([dic])], ignore_index=True)\n","\n","\n","            df_output.to_csv(prediction_file + run.target + '_'  +\n","                                str(run.lr) + '_' + str(run.hidden_layer_size) \\\n","                                + '_' + str(run.seq_length) + '_' + str(run.num_layers) \\\n","                                + '.csv')\n","\n","            '''\n","            Evaluation\n","            '''\n","\n","\n","\n","\n","\n","            dic_lstm = {\n","                'Model': 'LSTM',\n","                'lr': run.lr,\n","                'batch_size': run.batch_size,\n","                'seq_length': run.seq_length,\n","                #'week_range': run.week_range,\n","                'hidden_layer_size': run.hidden_layer_size,\n","                'num_layers': run.num_layers,\n","                'lossfunc': run.lossfunc,\n","\n","                'mae_validation': best_mae_numerical,\n","\n","                'mae_validation_1w': best_mae_numerical_weekly[0],\n","                'mae_validation_2w': best_mae_numerical_weekly[1],\n","                'mae_validation_3w': best_mae_numerical_weekly[2],\n","                'mae_validation_4w': best_mae_numerical_weekly[3],\n","\n","                'Target': run.target\n","            }\n","\n","            print('mae_validation:')\n","            print(dic_lstm['mae_validation'])\n","\n","\n","            df_summary = pd.concat([df_summary, pd.DataFrame([dic_lstm])], ignore_index=True)\n","\n","    df_summary.to_csv(summary_file  +test_week+ 'summarytest.csv')\n","    df_summary = df_summary.sort_values(by='mae_validation', ascending=True)\n","    return df_summary,df_output"],"metadata":{"id":"y2XDOra4z2Ml"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Forecasting"],"metadata":{"id":"F-i8KmNNBv-4"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import torch\n","from sklearn.preprocessing import MinMaxScaler\n","from torch import nn\n","from tqdm import trange\n","from datetime import date, timedelta\n","\n","from collections import OrderedDict\n","from collections import namedtuple\n","from itertools import product\n","\n","import torch.nn.functional as F\n","\n","\n","def LSTM_Prediction(test_week,variable_combinations):\n","    n_result = pd.DataFrame()\n","    data_file = '/content/drive/MyDrive/Covid forecasting/Data/smoothed_rate_withcovidww.csv'\n","\n","    model_file = '/content/drive/MyDrive/Covid forecasting/Result/US level/model/'\n","    prediction_file = '/content/drive/MyDrive/Covid forecasting/Result/US level/raw output/1000 times/'\n","    summary_file ='/content/drive/MyDrive/Covid forecasting/Result/US level/raw output/1000 times/'\n","    global device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    def perdelta(start, end, delta):\n","        curr = start\n","        while curr < end:\n","            yield curr\n","            curr += delta\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    constant_params = {\n","        'target': 'covid_rate',\n","        'batch_size': 2,\n","        'input_size': 15, # total number of features\n","        'output_size': 4, # number of weeks to predict\n","        'week_range': 300,\n","        'num_pred_features': 1,\n","        'ratio': 0.8,\n","        'num_epochs':50,\n","        'num_layers': 1,\n","        'dropout_rate': 0.8,\n","        'lossfunc': nn.SmoothL1Loss(beta=0.05, reduction = 'sum')\n","\n","\n","    }\n","\n","\n","    class RunBuilder():\n","        @staticmethod\n","        def get_runs(constant_params, variable_combinations):\n","            # Ensure variable_combinations is a list containing a single dictionary\n","            if isinstance(variable_combinations, dict):\n","                variable_combinations = [variable_combinations]\n","\n","            # Dynamically create a Run namedtuple based on keys\n","            Run = namedtuple('Run', list(constant_params.keys()) + list(variable_combinations[0].keys()))\n","\n","            runs = []\n","            for combo in variable_combinations:\n","                # Merge the constant and variable parameters\n","                params = {**constant_params, **combo}\n","                runs.append(Run(**params))\n","\n","            return runs\n","\n","    # Generate runs with selected combinations\n","    runs = RunBuilder.get_runs(constant_params, variable_combinations)\n","\n","    pd.options.mode.chained_assignment = None\n","    for run in runs:\n","        if run.target == 'covid_rate':\n","            results_file = 'covid_rate'\n","\n","        validation_predictions = []\n","        validation_labels = []\n","        # read data\n","        df = pd.read_csv(data_file)\n","        #df = df.rename(columns={'End Date': 'Week_end', 'State': 'fips'})\n","        df['Week_end'] = pd.to_datetime(df['Week_end'])\n","        # too many missing values in State'11'\n","        df =  df[df['fips'] != 11]\n","        df = df.drop(['PRCP_mean', 'SNOW_mean', 'TMAXDELTA', 'TMINDELTA','s05_value','scontrol_value'], axis=1)\n","    # select region\n","        df['fips'] = df['fips'].astype(str).str.zfill(2)\n","        #df = df[['fips','Week_end','difference','rate','smoothed_admission']]\n","\n","        # test set is the week from 2024-01-20\n","        start_date1 = pd.to_datetime(test_week) - timedelta(weeks=run.seq_length )\n","        end_date1 = pd.to_datetime(test_week) + timedelta(weeks= 1 )\n","        df_test = df[(df['Week_end'] > start_date1) & (df['Week_end'] < end_date1)] # Use data after 2023-11-01 as testing set\n","        df_test = df_test.set_index(['fips', 'Week_end'])\n","        df_test1 = df_test.copy()\n","        #choose data before 2024-01-06 as training data and validation data and after 2022-08-01\n","        df_train = df[(df['Week_end'] < pd.to_datetime(test_week) )& (df['Week_end'] >pd.to_datetime('2022-08-01')) ] # Use data before 11/1 as training set\n","\n","        df_train = df_train.set_index(['fips', 'Week_end'])\n","\n","        #rescale the data from 0 to 1, normalization\n","        first_col = df_train.pop(run.target)\n","        df_train.insert(0, run.target, first_col)\n","        first_col_2 = df_test.pop(run.target)\n","        df_test.insert(0, run.target, first_col_2)\n","\n","        scaler = MinMaxScaler(feature_range=(0, 1))\n","\n","        scaler.fit(df_train.iloc[:, 1:])\n","        train_features_normalized = scaler.transform(df_train.iloc[:, 1:])\n","        test_feature_normalized = scaler.transform(df_test.iloc[:, 1:])\n","\n","        scaler_target = MinMaxScaler(feature_range=(0, 1))\n","        scaler_target.fit(np.asarray(df_train.iloc[:, 0]).reshape(-1, 1))\n","        train_target_normalized = scaler_target.transform(np.asarray(df_train.iloc[:, 0]).reshape(-1, 1))\n","        test_target_normalized = scaler_target.transform(np.asarray(df_test.iloc[:, 0]).reshape(-1, 1))\n","        #train_target_normalized = np.log(df_train.iloc[:, 0] + 1)\n","        #test_target_normalized = np.log(df_test.iloc[:, 0] + 1)\n","\n","        df_train.iloc[:, 1:] = train_features_normalized\n","        df_train.iloc[:, 0] = train_target_normalized\n","        df_test.iloc[:, 1:] = test_feature_normalized\n","        df_test.iloc[:, 0] = test_target_normalized\n","\n","        for i in range(1000):\n","          df_output = pd.DataFrame(columns=['fips', 'Week_end', 'Prediction_1w',\n","                                            'Prediction_2w', 'Prediction_3w',\n","                                            'Prediction_4w'])\n","        #test set starts with 2024-01-06\n","          test_weeks = df_test[df_test.index.get_level_values('Week_end') \\\n","                              >= pd.to_datetime(test_week)].index.get_level_values('Week_end').unique()\n","          test_states = df_test.index.get_level_values('fips').unique()\n","\n","          m_state_dict_main = torch.load(model_file + run.target +'_' +test_week+ '_'  +\n","                                          str(run.lr) + '_' + str(run.hidden_layer_size) \\\n","                                          + '_' + str(run.seq_length) + '_' + str(run.num_layers) \\\n","                                          + '_LSTM_weights.pt')\n","\n","          model_main = LSTM(run.input_size, run.hidden_layer_size, run.num_layers,\n","                            run.output_size, run.dropout_rate).to(device)\n","          model_main.load_state_dict(m_state_dict_main)\n","\n","          with torch.no_grad():\n","              #model_main.eval()\n","              for fips in test_states:\n","                  for week in test_weeks:\n","                      seq = df_test[(df_test.index.get_level_values('fips') == fips)\\\n","                        & (df_test.index.get_level_values('Week_end') <= week)][-run.seq_length:].to_numpy()\n","\n","                      seq = torch.tensor(seq).reshape(-1, run.seq_length, run.input_size).to(device)\n","\n","\n","                      model_main.hidden_cell = (torch.zeros(run.num_layers, seq.size(0), run.hidden_layer_size).to(device),\n","                                        torch.zeros(run.num_layers, seq.size(0), run.hidden_layer_size).to(device))\n","                      prediction = model_main(seq.float())\n","\n","                      prediction = scaler_target.inverse_transform(prediction.cpu().detach().numpy().reshape(-1, 1))\n","                      #prediction = np.exp(prediction.cpu().detach().numpy().reshape(-1, 1)) - 1\n","                      # save prediction results\n","                      dic = {\n","                          'fips' : fips,\n","                          'Week_end' : week,\n","                          'Prediction_1w' : prediction[0].item(),\n","                          'Prediction_2w' : prediction[1].item(),\n","                          'Prediction_3w' : prediction[2].item(),\n","                          'Prediction_4w' : prediction[3].item()\n","\n","                      }\n","\n","                      df_output = pd.concat([df_output, pd.DataFrame([dic])], ignore_index=True)\n","\n","\n","              df_output.to_csv(prediction_file + run.target +'_' +test_week+ '_'  +\n","                                  str(run.lr) + '_' + str(run.hidden_layer_size) \\\n","                                  + '_' + str(run.seq_length) + '_' + str(run.num_layers) \\\n","                                  + '.csv')\n","              n_result= pd.concat([n_result, df_output], ignore_index=True)\n","\n","\n","\n","    n_result = n_result.sort_values(by='fips')\n","    n_result.to_pickle(summary_file + test_week+ '_allresult.pkl')\n","    return n_result"],"metadata":{"id":"Ta4u3Si5BVqX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Forecasting"],"metadata":{"id":"_niCFM_5K1-x"}},{"cell_type":"markdown","source":["##model selection"],"metadata":{"id":"6OO2hTXiY7JF"}},{"cell_type":"code","source":["from datetime import datetime, timedelta\n","\n","\n","#start_date = datetime.strptime('2023-10-07', '%Y-%m-%d')\n","start_date = datetime.strptime('2024-02-03', '%Y-%m-%d')\n","end_date = datetime.strptime('2024-02-24', '%Y-%m-%d')\n","current_date = start_date\n","dates_list = []\n","\n","while current_date <= end_date:\n","    dates_list.append(current_date.strftime('%Y-%m-%d'))\n","    current_date += timedelta(days=7)\n","dates_list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cAitL7vZFkVe","executionInfo":{"status":"ok","timestamp":1718320204531,"user_tz":-480,"elapsed":894,"user":{"displayName":"Shawn XU","userId":"03794891599117456463"}},"outputId":"f12b90fe-719a-44e5-da6f-40c4ddfce116"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['2024-02-03', '2024-02-10', '2024-02-17', '2024-02-24']"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["test_weeks = dates_list\n","option = 0\n","for week in test_weeks:\n","    summary_table, b = LSTM_train(week, option)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ieyPdOa0TLg","outputId":"46bebdb2-a0a7-49d6-bd9e-82606103bb48","executionInfo":{"status":"ok","timestamp":1718333802623,"user_tz":-480,"elapsed":14967,"user":{"displayName":"Shawn XU","userId":"03794891599117456463"}}},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  0%|          | 0/50 [00:00<?, ?it/s]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  981.8316319286823\n","Test Loss:  144.6177259683609\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  4%|▍         | 2/50 [00:13<04:50,  6.06s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  683.2232583723962\n","Test Loss:  120.00508761405945\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:16<03:52,  4.94s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  548.8848761804402\n","Test Loss:  105.10384814068675\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:20<03:24,  4.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  489.5421230169013\n","Test Loss:  98.49051996320486\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:23<03:04,  4.09s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  459.3537318771705\n","Test Loss:  92.03003723360598\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:27<02:52,  3.92s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  432.85456762835383\n","Test Loss:  90.12278645811602\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:31<02:42,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  417.1308389129117\n","Test Loss:  86.78077467251569\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:34<02:34,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  411.59289822075516\n","Test Loss:  84.96617262018844\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:37<02:28,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  399.2097309278324\n","Test Loss:  83.29579332936555\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:41<02:22,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  388.4592929575592\n","Test Loss:  83.03678833600134\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:44<02:18,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  382.6775493356399\n","Test Loss:  81.0086272759363\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:48<02:13,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  383.19799911323935\n","Test Loss:  80.80456589348614\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:51<02:08,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  373.60836854344234\n","Test Loss:  79.23690910916775\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:55<02:05,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  362.8720018067397\n","Test Loss:  82.33791604172438\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:58<02:02,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  361.6516112801619\n","Test Loss:  78.17610319890082\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [01:02<01:58,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  358.42265316518024\n","Test Loss:  76.96156446263194\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:05<01:55,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  356.74021486658603\n","Test Loss:  76.95719671342522\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:09<01:53,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  358.4272068394348\n","Test Loss:  75.74732648581266\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:12<01:49,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  348.74936174135655\n","Test Loss:  75.06227571098134\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:16<01:46,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  339.3671816345304\n","Test Loss:  74.96168191824108\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:20<01:42,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  343.86751074064523\n","Test Loss:  73.22572202188894\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:23<01:38,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  337.0147151686251\n","Test Loss:  72.92240702384152\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:27<01:35,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  333.2121579949744\n","Test Loss:  70.98564943764359\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:30<01:32,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  328.38124427665025\n","Test Loss:  72.27556164935231\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:34<01:28,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  336.09853501291946\n","Test Loss:  72.7340845814906\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:37<01:25,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  330.1165772266686\n","Test Loss:  71.69100261898711\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:41<01:21,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  332.64632789278403\n","Test Loss:  69.77664874494076\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:44<01:17,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  329.77829975495115\n","Test Loss:  73.260947057046\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:48<01:14,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  324.178398700431\n","Test Loss:  70.32182207610458\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:51<01:10,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  323.8541354043409\n","Test Loss:  70.07831947412342\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:55<01:06,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  325.13069098442793\n","Test Loss:  69.45096673630178\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:58<01:03,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  318.36017265357077\n","Test Loss:  69.42804618761875\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [02:02<00:59,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  321.68809086317196\n","Test Loss:  68.08520019520074\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [02:05<00:55,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  317.8137248987332\n","Test Loss:  68.84788911230862\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:09<00:52,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  310.91838317597285\n","Test Loss:  69.26898748520762\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:12<00:49,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  318.4579238612205\n","Test Loss:  67.68117344286293\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:16<00:45,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  321.5647340719588\n","Test Loss:  70.5931936269626\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:20<00:42,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  314.7233729995787\n","Test Loss:  69.79299576859921\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:23<00:38,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  310.33361641922966\n","Test Loss:  72.49538776651025\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:26<00:34,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  309.2057004463859\n","Test Loss:  69.64822799526155\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:30<00:31,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.3463507145643\n","Test Loss:  67.25574879115447\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:33<00:27,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  310.5763569539413\n","Test Loss:  67.89868945069611\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:37<00:24,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  312.8394843256101\n","Test Loss:  66.34202768513933\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:40<00:20,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  306.29733166028745\n","Test Loss:  67.85237620584667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:44<00:17,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  310.53263068851084\n","Test Loss:  65.21098612295464\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 92%|█████████▏| 46/50 [02:47<00:13,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  306.5386162409559\n","Test Loss:  66.40406182501465\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 94%|█████████▍| 47/50 [02:50<00:10,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  311.71488626953214\n","Test Loss:  66.33254516078159\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 96%|█████████▌| 48/50 [02:54<00:06,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  301.65787940844893\n","Test Loss:  73.5113386712037\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 49/50 [02:57<00:03,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  310.9863891322166\n","Test Loss:  67.53527389653027\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 49/50 [03:01<00:03,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  303.3881718441844\n","Test Loss:  65.90431409375742\n","Early stopping with best_loss:  65.21098612295464 and test_loss for this epoch:  65.90431409375742 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","0.99752094149128\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  0%|          | 0/50 [00:00<?, ?it/s]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  853.1309096626937\n","Test Loss:  117.69500224106014\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  4%|▍         | 2/50 [00:07<03:00,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  554.1280442476273\n","Test Loss:  102.63276532292366\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:52,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  480.17999414447695\n","Test Loss:  92.91962382569909\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:46,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  443.5532857775688\n","Test Loss:  89.1706834724173\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:42,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  418.364037681371\n","Test Loss:  86.31675680354238\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:41,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  399.2320464393124\n","Test Loss:  84.12894068844616\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:37,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  383.5843606730923\n","Test Loss:  87.27191335055977\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:29<02:32,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  369.12605361267924\n","Test Loss:  80.8512752102688\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:32<02:27,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  366.0567395747639\n","Test Loss:  81.80099525954574\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:36<02:22,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  357.2074337908998\n","Test Loss:  80.6377719277516\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:39<02:19,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  349.42923631402664\n","Test Loss:  78.82643623882905\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:43<02:15,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  349.5572872881312\n","Test Loss:  78.75207798276097\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:47<02:12,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  340.10917131695896\n","Test Loss:  74.33480112440884\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:50<02:08,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  332.65234751440585\n","Test Loss:  79.53329542744905\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:54<02:03,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  325.4450682257302\n","Test Loss:  77.29022873286158\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:57<01:59,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  331.6761236800812\n","Test Loss:  74.33779074344784\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:01<01:57,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  320.13046640343964\n","Test Loss:  75.76638929825276\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:04<01:54,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  319.8803845634684\n","Test Loss:  71.92007831018418\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:08<01:51,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  315.0544736441225\n","Test Loss:  71.62111374782398\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:12<01:47,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  318.7848876528442\n","Test Loss:  71.77934982138686\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:15<01:43,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  317.88457016739994\n","Test Loss:  72.97668586764485\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:19<01:39,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  311.08304422535\n","Test Loss:  73.68037992715836\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:22<01:35,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.4970792317763\n","Test Loss:  71.5677431859076\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:26<01:31,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  316.02210646052845\n","Test Loss:  75.05904920911416\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:29<01:28,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  312.4524564300664\n","Test Loss:  76.12228970881552\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:33<01:24,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  308.9010108788498\n","Test Loss:  70.72488133050501\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:36<01:21,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  306.64943984895945\n","Test Loss:  71.84090720815584\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:40<01:17,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  309.77169747510925\n","Test Loss:  71.50236877426505\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:43<01:14,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  305.5455520085525\n","Test Loss:  70.54272043285891\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:47<01:10,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  297.96767196059227\n","Test Loss:  71.6263977875933\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:50<01:07,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  306.0441196542233\n","Test Loss:  69.21253609703854\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:54<01:03,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  304.99080495676026\n","Test Loss:  69.17704534856603\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:57<00:59,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  300.60940305609256\n","Test Loss:  69.35806826781482\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [02:01<00:56,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  301.18031570967287\n","Test Loss:  77.1794629925862\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:04<00:52,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  299.2981208288111\n","Test Loss:  69.15483564743772\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:08<00:48,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  298.6715242154896\n","Test Loss:  69.39725181367248\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:11<00:45,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  303.00254824757576\n","Test Loss:  70.29456940665841\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:15<00:42,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  296.38614799268544\n","Test Loss:  69.02232536673546\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:18<00:38,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  295.52557479962707\n","Test Loss:  68.54346111556515\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:22<00:35,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  295.3397461459972\n","Test Loss:  68.0492232190445\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:25<00:31,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  297.0129679478705\n","Test Loss:  72.25221315654926\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:29<00:28,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  289.9655594169162\n","Test Loss:  68.27547133341432\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:32<00:24,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  293.5060460790992\n","Test Loss:  68.05227330327034\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:36<00:21,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  287.97447721962817\n","Test Loss:  70.37833639327437\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:39<00:21,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  294.1418991917744\n","Test Loss:  68.52941901795566\n","Early stopping with best_loss:  68.0492232190445 and test_loss for this epoch:  68.52941901795566 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","1.0239600662009813\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  0%|          | 0/50 [00:00<?, ?it/s]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  786.3957214877009\n","Test Loss:  126.20578739792109\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  4%|▍         | 2/50 [00:08<03:14,  4.04s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  513.2964632222429\n","Test Loss:  111.22661949694157\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<03:02,  3.88s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  458.0764449900016\n","Test Loss:  97.75915428902954\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<02:53,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  430.3192796241492\n","Test Loss:  101.12518234364688\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:19<02:47,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  409.7025677980855\n","Test Loss:  90.09319965541363\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:43,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  389.6689216112718\n","Test Loss:  87.33361207414418\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:38,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  379.690619780682\n","Test Loss:  85.20811243075877\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:30<02:33,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  364.0472926059738\n","Test Loss:  83.0560147864744\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:33<02:28,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  353.5669060219079\n","Test Loss:  79.20583548303694\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:37<02:24,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  350.437511193566\n","Test Loss:  74.91858576517552\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:40<02:20,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  343.7477068565786\n","Test Loss:  74.8357371557504\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:44<02:17,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  332.2320008184761\n","Test Loss:  73.65811281371862\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:47<02:12,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  334.8788512190804\n","Test Loss:  73.79855144349858\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:51<02:08,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  325.9724386804737\n","Test Loss:  73.66625836212188\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:55<02:05,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  328.37515407893807\n","Test Loss:  69.54649298731238\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:58<02:02,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  322.5393042508513\n","Test Loss:  72.74126165127382\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:02<01:58,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  320.45939214807004\n","Test Loss:  70.75859497208148\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:06<01:55,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  319.02233241079375\n","Test Loss:  74.99221150390804\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:09<01:52,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  312.23990583373234\n","Test Loss:  68.85300964629278\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:13<01:48,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  309.663251417689\n","Test Loss:  69.18850674759597\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:16<01:44,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  308.117259462364\n","Test Loss:  68.34802533057518\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:20<01:40,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  308.11455508950166\n","Test Loss:  72.28542702808045\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:24<01:37,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.54964962136\n","Test Loss:  68.1455902075395\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:27<01:34,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  305.57415389688686\n","Test Loss:  67.52420991193503\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:31<01:30,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  301.16444513434544\n","Test Loss:  69.01601547840983\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:34<01:26,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  296.94645226188004\n","Test Loss:  72.59369939286262\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:38<01:22,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  299.5084949680604\n","Test Loss:  68.85013112984598\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:42<01:18,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  298.9910109350458\n","Test Loss:  66.4909274749225\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:45<01:15,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  295.57940003369004\n","Test Loss:  66.26315675815567\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:49<01:12,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  299.007959747687\n","Test Loss:  66.1906751152128\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:52<01:08,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  299.3913969649002\n","Test Loss:  70.69851269666106\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:56<01:05,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  296.1074396278709\n","Test Loss:  72.76138104638085\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [02:00<01:01,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  291.1655643796548\n","Test Loss:  68.09407958155498\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [02:03<00:57,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  296.47582695959136\n","Test Loss:  70.20801779069006\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:07<00:53,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  297.736848494038\n","Test Loss:  65.52382433228195\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:10<00:50,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  289.71419110894203\n","Test Loss:  66.10140756471083\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:14<00:46,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  293.3436168283224\n","Test Loss:  66.5955001260154\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:18<00:42,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  289.6317731272429\n","Test Loss:  65.77817622944713\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:21<00:40,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  287.84137304872274\n","Test Loss:  64.8382404781878\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:25<00:36,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  280.26257433299907\n","Test Loss:  66.99648148100823\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:29<00:32,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  287.83974788850173\n","Test Loss:  64.9554484942928\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:32<00:28,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  282.7729173321277\n","Test Loss:  64.4230803065002\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:36<00:25,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  287.54856961313635\n","Test Loss:  66.75432096701115\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:39<00:21,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  287.1511157657951\n","Test Loss:  64.43671459285542\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:43<00:18,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  283.60044708848\n","Test Loss:  64.60338251013309\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 92%|█████████▏| 46/50 [02:47<00:14,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  279.1286961801816\n","Test Loss:  63.30096111726016\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 94%|█████████▍| 47/50 [02:50<00:10,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  284.71004946064204\n","Test Loss:  64.23150122445077\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 96%|█████████▌| 48/50 [02:54<00:07,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  280.3850183747709\n","Test Loss:  66.20445617754012\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 49/50 [02:58<00:03,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  275.6161067369394\n","Test Loss:  64.03302419558167\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 50/50 [03:01<00:00,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  281.7918171961792\n","Test Loss:  64.59847810445353\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","0.9769681399685811\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  0%|          | 0/50 [00:00<?, ?it/s]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  731.0403405427933\n","Test Loss:  113.15789013542235\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  4%|▍         | 2/50 [00:08<03:30,  4.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  486.11547178030014\n","Test Loss:  97.30420273635536\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:12<03:14,  4.14s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  438.383592069149\n","Test Loss:  95.0067188590765\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:16<03:04,  4.01s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  412.82263313420117\n","Test Loss:  96.47825763002038\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:20<02:59,  3.99s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  383.8332022903487\n","Test Loss:  84.74579100310802\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:24<02:54,  3.97s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  373.39425531681627\n","Test Loss:  89.34386796876788\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:28<02:50,  3.95s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  348.00585203198716\n","Test Loss:  83.1589562445879\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:32<02:45,  3.95s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  342.7679203390144\n","Test Loss:  75.2570341322571\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:36<02:41,  3.93s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  333.70454403944314\n","Test Loss:  78.6533615840599\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:40<02:36,  3.92s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  331.47577408421785\n","Test Loss:  73.65188627783209\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:44<02:32,  3.92s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  334.7688389616087\n","Test Loss:  75.60357215441763\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:47<02:29,  3.92s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  320.67863979004323\n","Test Loss:  77.8224763404578\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:51<02:24,  3.90s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  319.93365310365334\n","Test Loss:  71.30515755014494\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:55<02:20,  3.90s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  314.9077626224607\n","Test Loss:  78.19107100507244\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:59<02:16,  3.89s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  312.3403146741912\n","Test Loss:  69.60022416338325\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [01:03<02:12,  3.89s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  315.4874092079699\n","Test Loss:  71.54606675542891\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:07<02:08,  3.90s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  302.8355593327433\n","Test Loss:  73.28237028187141\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:11<02:05,  3.92s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  312.94936741748825\n","Test Loss:  80.85651833284646\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:15<02:01,  3.91s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  305.4686375018209\n","Test Loss:  69.14358319807798\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:19<01:57,  3.91s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  294.0457622911781\n","Test Loss:  72.53006947692484\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:23<01:53,  3.92s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  297.90686668176204\n","Test Loss:  69.58580476371571\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:27<01:49,  3.92s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  294.8940944429487\n","Test Loss:  73.52909362036735\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:30<01:45,  3.92s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  297.93432149710134\n","Test Loss:  70.58976532751694\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:35<01:51,  4.13s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  294.68804416712373\n","Test Loss:  73.92689666245133\n","Early stopping with best_loss:  69.14358319807798 and test_loss for this epoch:  73.92689666245133 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","1.042294889599248\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  0%|          | 0/50 [00:00<?, ?it/s]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  961.7240969762206\n","Test Loss:  137.3172442279756\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  4%|▍         | 2/50 [00:07<03:05,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  621.7822681553662\n","Test Loss:  107.73327692784369\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:54,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  507.3332337606698\n","Test Loss:  97.14601988531649\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:46,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  466.27740760333836\n","Test Loss:  90.68851227127016\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:38,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  434.3730032937601\n","Test Loss:  85.89019760861993\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:33,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  415.25314903771505\n","Test Loss:  83.70651582675055\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:29,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  399.08922776021063\n","Test Loss:  81.6422933768481\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:25,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  395.910583069548\n","Test Loss:  78.74155749473721\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:21,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  383.8383608814329\n","Test Loss:  77.98607701994479\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:35<02:18,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  365.84441431891173\n","Test Loss:  76.18975558690727\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:38<02:14,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  366.01326054520905\n","Test Loss:  75.08912166580558\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:42<02:10,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  356.0479798829183\n","Test Loss:  72.9877067990601\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:45<02:06,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  359.2732450386975\n","Test Loss:  72.4698719768785\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:49<02:02,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  351.4701832588762\n","Test Loss:  76.40505200228654\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:52<01:59,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  346.12744731642306\n","Test Loss:  69.94103395286947\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:55<01:56,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  338.6609449442476\n","Test Loss:  71.54976624529809\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:59<01:53,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  336.3130262554623\n","Test Loss:  68.51723951939493\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:02<01:50,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  329.63023067452013\n","Test Loss:  67.30179966147989\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:06<01:47,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  322.3313282537274\n","Test Loss:  67.13126405747607\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:09<01:43,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  330.278549421113\n","Test Loss:  70.00536782993004\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:13<01:40,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  321.6402008808218\n","Test Loss:  67.22622015606612\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:16<01:37,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  318.9582067960873\n","Test Loss:  67.7672759629786\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:20<01:33,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  323.5683663818054\n","Test Loss:  67.22306096833199\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:23<01:30,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.3327604145743\n","Test Loss:  66.4831033507362\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:27<01:26,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.01039593853056\n","Test Loss:  65.54385593952611\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:30<01:23,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  310.4355835178867\n","Test Loss:  64.79387267073616\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:34<01:19,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  312.84675394557416\n","Test Loss:  64.7233277997002\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:37<01:16,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  309.27967121475376\n","Test Loss:  66.16148492973298\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:41<01:13,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  308.8031009947881\n","Test Loss:  65.95612374646589\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:44<01:09,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  307.1073719682172\n","Test Loss:  64.06307627167553\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:48<01:06,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  301.0382250826806\n","Test Loss:  67.04032726213336\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:51<01:02,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  294.75517020653933\n","Test Loss:  63.82584092114121\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:55<00:59,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  300.44991800002754\n","Test Loss:  66.43581574037671\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:58<00:55,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  301.1523584704846\n","Test Loss:  66.01169010903686\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:02<00:52,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  296.43114071921445\n","Test Loss:  69.57364329276606\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:05<00:48,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  296.8838290227577\n","Test Loss:  66.95809192629531\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:08<00:44,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  295.87697109393775\n","Test Loss:  63.54469598084688\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:12<00:41,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  297.2279390208423\n","Test Loss:  64.3192238304764\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:15<00:37,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  293.66109909210354\n","Test Loss:  64.5221628122963\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:19<00:34,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  294.762177482713\n","Test Loss:  64.31037395074964\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:22<00:31,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  296.20938137406483\n","Test Loss:  62.12653325218707\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:26<00:28,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  294.35491279885173\n","Test Loss:  65.76707794237882\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:29<00:24,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  296.1163646215573\n","Test Loss:  63.48213279247284\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:33<00:20,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  286.0188828371465\n","Test Loss:  63.94218600494787\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:36<00:17,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  288.6031421299558\n","Test Loss:  63.56900774035603\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:40<00:17,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  286.9495342131704\n","Test Loss:  64.59098939457908\n","Early stopping with best_loss:  62.12653325218707 and test_loss for this epoch:  64.59098939457908 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","0.991252547394368\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:04<03:28,  4.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  863.6817376837134\n","Test Loss:  132.14284092746675\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:05,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  564.0846522822976\n","Test Loss:  101.17185109294951\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:52,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  463.09024735353887\n","Test Loss:  93.18005771841854\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:45,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  429.58792290091515\n","Test Loss:  93.85650084726512\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:40,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  413.41836937330663\n","Test Loss:  85.86713004857302\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:34,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  395.8776514660567\n","Test Loss:  87.06847475096583\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:31,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  369.9454409480095\n","Test Loss:  83.22123293671757\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:28,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  365.8794740452431\n","Test Loss:  79.89051070809364\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:32<02:24,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  354.85906687844545\n","Test Loss:  75.78198653180152\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:35<02:20,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  343.4175986824557\n","Test Loss:  77.22169577889144\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:39<02:17,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  340.7214380495716\n","Test Loss:  74.21037106355652\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:42<02:14,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  330.5287311379798\n","Test Loss:  77.85236093169078\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:46<02:11,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  331.8516661915928\n","Test Loss:  74.42427371116355\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:50<02:07,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  319.32284585619345\n","Test Loss:  71.04484079265967\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:53<02:03,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  317.16273836325854\n","Test Loss:  70.10307394620031\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:57<02:00,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  319.94631772488356\n","Test Loss:  72.72279285080731\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:00<01:56,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  312.55476380046457\n","Test Loss:  74.27167475828901\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:04<01:52,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  306.4160897321999\n","Test Loss:  70.96859970595688\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:07<01:48,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  312.1681169946678\n","Test Loss:  80.60481829755008\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:11<01:56,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  310.44351821392775\n","Test Loss:  70.49893872952089\n","Early stopping with best_loss:  70.10307394620031 and test_loss for this epoch:  70.49893872952089 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","1.0791231060522093\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:55,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  765.5788232162595\n","Test Loss:  124.48312897793949\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:50,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  512.9110713177361\n","Test Loss:  94.54823076725006\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:47,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  450.74956465512514\n","Test Loss:  99.0022998880595\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:42,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  413.15724063292146\n","Test Loss:  87.96777169592679\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:40,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  390.08705582655966\n","Test Loss:  82.28082109591924\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:36,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  366.7573320211377\n","Test Loss:  82.1563246017322\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:32,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  361.05507659260184\n","Test Loss:  74.84970962163061\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:27,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  356.35507159121335\n","Test Loss:  89.96886928845197\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:23,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  341.6394623396918\n","Test Loss:  76.06587660452351\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:35<02:20,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  332.55445135058835\n","Test Loss:  70.63090420234948\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:38<02:16,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  326.27809139480814\n","Test Loss:  71.93821502663195\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:42<02:12,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  319.8973322296515\n","Test Loss:  69.54561464954168\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:45<02:10,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  314.5337367611937\n","Test Loss:  68.29957223124802\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:49<02:08,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  313.1917970087379\n","Test Loss:  71.67381248879246\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:53<02:04,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  314.57209987170063\n","Test Loss:  67.74658320052549\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:56<02:00,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  305.9877544725314\n","Test Loss:  65.86574599146843\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:59<01:56,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  304.5114889573306\n","Test Loss:  66.53419105987996\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:03<01:52,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  301.6586626134813\n","Test Loss:  69.02466698596254\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:06<01:47,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  305.1357793114148\n","Test Loss:  70.87289892043918\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:10<01:44,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  298.0179279646836\n","Test Loss:  63.96242943732068\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:13<01:40,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  298.902521724347\n","Test Loss:  65.23332124901935\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:17<01:37,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  297.6386516755447\n","Test Loss:  73.70360705535859\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:20<01:33,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  295.197222661227\n","Test Loss:  66.19551702262834\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:24<01:30,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  295.55099549982697\n","Test Loss:  68.44421961624175\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:27<01:27,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  290.3435211861506\n","Test Loss:  63.32141398079693\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:31<01:23,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  288.57261471636593\n","Test Loss:  66.23642484191805\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:34<01:20,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  287.4332687468268\n","Test Loss:  64.71142893610522\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:38<01:16,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  286.46918112644926\n","Test Loss:  63.04086028889287\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:41<01:12,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  284.61883848393336\n","Test Loss:  64.52392903156579\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:45<01:09,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  284.771075071767\n","Test Loss:  63.753300095908344\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:48<01:06,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  282.5385760255158\n","Test Loss:  62.80749039538205\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:52<01:02,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  282.32233915291727\n","Test Loss:  67.40939441788942\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:55<00:59,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  282.02518648980185\n","Test Loss:  70.67430106457323\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:59<00:56,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  277.449609096162\n","Test Loss:  63.56572974938899\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:02<00:52,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  276.3681138451211\n","Test Loss:  64.36401358433068\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:06<00:48,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  274.3961049495265\n","Test Loss:  62.30466285906732\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:09<00:44,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  277.3154034540057\n","Test Loss:  62.186460263095796\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:12<00:41,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  273.00818032585084\n","Test Loss:  61.57155888155103\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:16<00:38,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  279.45248473482206\n","Test Loss:  61.13908639829606\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:19<00:34,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  271.2812696797773\n","Test Loss:  61.14081647153944\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:23<00:31,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  273.4930849606171\n","Test Loss:  59.652059162501246\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:26<00:27,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  271.5624144754838\n","Test Loss:  63.93896823283285\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:30<00:24,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  272.79264454334043\n","Test Loss:  64.68364818557166\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:33<00:20,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  264.05050226766616\n","Test Loss:  64.26398540940136\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:37<00:17,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  270.65924497996457\n","Test Loss:  61.564422903582454\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:40<00:17,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  267.7398663763888\n","Test Loss:  59.96407259441912\n","Early stopping with best_loss:  59.652059162501246 and test_loss for this epoch:  59.96407259441912 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","0.9547076605385915\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:02,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  733.3953611049801\n","Test Loss:  102.99860335420817\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:59,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  487.5986154079437\n","Test Loss:  89.41802094038576\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:56,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  429.49558989703655\n","Test Loss:  103.77289783209562\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<02:54,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  400.68172592809424\n","Test Loss:  80.02584449201822\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:19<02:53,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  377.65646072570235\n","Test Loss:  72.60950314998627\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:49,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  347.63221859093755\n","Test Loss:  73.59967514919117\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:46,  3.88s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  336.52235473599285\n","Test Loss:  69.80046997684985\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:30<02:42,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  334.42712043970823\n","Test Loss:  67.5422447193414\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:34<02:36,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  329.7795407245867\n","Test Loss:  67.87503005377948\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:38<02:32,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  316.9486388703808\n","Test Loss:  67.97548266500235\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:41<02:28,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  318.59527265420184\n","Test Loss:  66.87120198272169\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:45<02:24,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  315.30287563987076\n","Test Loss:  72.10701880883425\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:49<02:20,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  309.5287067696918\n","Test Loss:  68.6430879062973\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:53<02:16,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  313.74404288223013\n","Test Loss:  64.8903444595635\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:57<02:11,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  303.78174879495054\n","Test Loss:  65.32269457541406\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [01:00<02:08,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  295.116479865741\n","Test Loss:  75.06137699820101\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:04<02:04,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  295.14787292759866\n","Test Loss:  63.74161541624926\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:08<02:01,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  287.5280869482085\n","Test Loss:  63.66613171529025\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:12<01:57,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  297.7299952856265\n","Test Loss:  65.23952659126371\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:15<01:52,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  283.88399031292647\n","Test Loss:  63.757633975706995\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:19<01:49,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  292.7329462028574\n","Test Loss:  63.36805574037135\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:23<01:45,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  285.00272827781737\n","Test Loss:  70.40649583470076\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:27<01:41,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  282.7981337257661\n","Test Loss:  63.506933780387044\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:31<01:38,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  281.89094262057915\n","Test Loss:  63.074665383435786\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:34<01:34,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  275.0112025407143\n","Test Loss:  62.25803575664759\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:38<01:31,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  283.0733431954868\n","Test Loss:  61.7503075459972\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:42<01:27,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  276.3968910132535\n","Test Loss:  67.68259447487071\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:46<01:22,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  277.9664041772485\n","Test Loss:  61.890639998950064\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:50<01:19,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  279.5636929846369\n","Test Loss:  62.650326481088996\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:53<01:16,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  275.06190354097635\n","Test Loss:  60.76040586223826\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:57<01:12,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  273.38432474434376\n","Test Loss:  63.53390665072948\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [02:01<01:08,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  269.1975218276493\n","Test Loss:  63.67979079158977\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [02:05<01:04,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  268.1872183890082\n","Test Loss:  59.95432587200776\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [02:08<01:00,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  268.80552637018263\n","Test Loss:  65.33955415152013\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:12<00:56,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  268.33614596747793\n","Test Loss:  66.67130445037037\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:16<00:52,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  264.7411340200342\n","Test Loss:  59.930482282303274\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:20<00:49,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  269.134104563389\n","Test Loss:  65.14061947586015\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:24<00:45,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  264.0443562185392\n","Test Loss:  59.88714792113751\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:27<00:41,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  266.3140286761336\n","Test Loss:  62.59176027867943\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:31<00:37,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  262.23310352955014\n","Test Loss:  62.95648642559536\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:35<00:34,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  261.61775415437296\n","Test Loss:  59.23367641400546\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:39<00:30,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  262.04181196540594\n","Test Loss:  61.01012529945001\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:43<00:26,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  261.96296850871295\n","Test Loss:  60.07278615003452\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:47<00:23,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  263.09797158651054\n","Test Loss:  65.31366466265172\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:50<00:19,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  257.4203931041993\n","Test Loss:  60.59970170073211\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:54<00:19,  3.88s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  262.6499992548488\n","Test Loss:  60.19279429130256\n","Early stopping with best_loss:  59.23367641400546 and test_loss for this epoch:  60.19279429130256 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","0.9535869542603022\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:42,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  943.3131519556046\n","Test Loss:  131.69583041965961\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:37,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  638.8701787143946\n","Test Loss:  99.01918591372669\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:34,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  511.81835125386715\n","Test Loss:  89.93724100245163\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:31,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  457.47777020931244\n","Test Loss:  86.06033797934651\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:27,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  428.02005450613797\n","Test Loss:  88.28517028875649\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:24,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  406.5537446755916\n","Test Loss:  80.38303527794778\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:21,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  389.48225191701204\n","Test Loss:  79.59412060119212\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:19,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  377.2039246689528\n","Test Loss:  76.39568828418851\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:29<02:15,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  366.764779440593\n","Test Loss:  76.21059797145426\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:12,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  351.12536542583257\n","Test Loss:  70.08118607290089\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:36<02:08,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  350.168775443919\n","Test Loss:  69.48021580092609\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:39<02:04,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  341.7758564602118\n","Test Loss:  68.25932702235878\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:42<02:00,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  343.7059040609747\n","Test Loss:  67.20648601930588\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:46<01:57,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  333.9888194752857\n","Test Loss:  68.63151021953672\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:49<01:54,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  327.4218231854029\n","Test Loss:  66.75926277413964\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:52<01:51,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  327.93168370611966\n","Test Loss:  66.06432357942685\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:55<01:47,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  317.74098622449674\n","Test Loss:  64.78182543907315\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [00:59<01:45,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  317.6567031778395\n","Test Loss:  69.70440171752125\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:02<01:41,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  311.470960422419\n","Test Loss:  65.15214111842215\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:05<01:37,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.5887271682732\n","Test Loss:  64.3343344181776\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:08<01:35,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  303.8338157264516\n","Test Loss:  63.33439933042973\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:12<01:32,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  306.2277947680559\n","Test Loss:  63.10623378213495\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:15<01:29,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  309.0771041708067\n","Test Loss:  68.41776580177248\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:19<01:26,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  297.73859173059464\n","Test Loss:  62.37224434874952\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:22<01:23,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  297.97493987716734\n","Test Loss:  62.36452264431864\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:25<01:19,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  295.1322270706296\n","Test Loss:  63.657580888830125\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:28<01:15,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  294.28958383994177\n","Test Loss:  61.495689096394926\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:32<01:12,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  295.5822761710733\n","Test Loss:  65.06781725864857\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:35<01:09,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  292.65862477058545\n","Test Loss:  60.777001318056136\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:38<01:06,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  289.1527827391401\n","Test Loss:  74.01993194408715\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:42<01:02,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  292.3853353979066\n","Test Loss:  61.08394237142056\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:45<00:59,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  287.7245703935623\n","Test Loss:  63.0185111281462\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:48<00:56,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  289.16851069265977\n","Test Loss:  60.66175633203238\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:51<00:52,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  286.6840587668121\n","Test Loss:  61.49686271883547\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:55<00:49,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  286.64108311897144\n","Test Loss:  66.84077273961157\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [01:58<00:46,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  279.6405308940448\n","Test Loss:  60.97842766530812\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:01<00:43,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  279.6344768041745\n","Test Loss:  60.44159932527691\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:05<00:40,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  280.9289980424801\n","Test Loss:  60.66027413064148\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:08<00:37,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  276.782890021801\n","Test Loss:  60.05871517583728\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:12<00:34,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  281.6800173493102\n","Test Loss:  59.09377135010436\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:15<00:30,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  281.83271602820605\n","Test Loss:  59.49839541246183\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:18<00:26,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  278.08957764552906\n","Test Loss:  60.481207171455026\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:22<00:23,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  281.0035642646253\n","Test Loss:  66.2262636097148\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:25<00:20,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  277.49604521971196\n","Test Loss:  63.31260438449681\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:29<00:20,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  277.4577621240169\n","Test Loss:  62.1865987055935\n","Early stopping with best_loss:  59.09377135010436 and test_loss for this epoch:  62.1865987055935 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","0.9714836011513645\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:44,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  822.9098821878433\n","Test Loss:  133.57281692698598\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:40,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  563.6396672744304\n","Test Loss:  97.66818054392934\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:38,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  459.16128802485764\n","Test Loss:  87.70770320948213\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:34,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  421.7994556929916\n","Test Loss:  84.05294075189158\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:31,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  388.6085487026721\n","Test Loss:  83.47082313895226\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:28,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  379.27445418573916\n","Test Loss:  83.11482441611588\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:25,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  363.6385295279324\n","Test Loss:  80.8241099906154\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:21,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  344.4720971705392\n","Test Loss:  76.06302215578035\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:17,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  336.2214501220733\n","Test Loss:  77.71117934770882\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:15,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  330.35815455485135\n","Test Loss:  72.93283854145557\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:11,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  325.55395105248317\n","Test Loss:  72.17361090984195\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:07,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  325.2904198396718\n","Test Loss:  73.54135342943482\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:03,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  313.4874837542884\n","Test Loss:  71.04531015828252\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:47<02:01,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  304.4182411786169\n","Test Loss:  70.41557097109035\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:50<01:57,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.0672065867111\n","Test Loss:  69.93382386304438\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:53<01:54,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  304.3853124016896\n","Test Loss:  69.07149365358055\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:57<01:51,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  294.37095755932387\n","Test Loss:  71.46655076509342\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:00<01:48,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  292.1184786083177\n","Test Loss:  71.2731687061023\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:04<01:45,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  295.924861213658\n","Test Loss:  70.47193139605224\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:07<01:41,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  285.2393294617068\n","Test Loss:  68.35289692971855\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:10<01:38,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  285.6125103137456\n","Test Loss:  69.41198638640344\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:14<01:34,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  287.64270390663296\n","Test Loss:  66.14095536037348\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:17<01:30,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  278.2000500452705\n","Test Loss:  66.47301807859913\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:20<01:27,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  279.81812532339245\n","Test Loss:  65.63627730449662\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:24<01:24,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  281.8577523622662\n","Test Loss:  66.23877320438623\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:27<01:21,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  279.9661878813058\n","Test Loss:  66.41888775816187\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:31<01:17,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  274.5354638793506\n","Test Loss:  65.4673626772128\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:34<01:14,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  272.6246628332883\n","Test Loss:  66.83629548083991\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:37<01:10,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  280.16487117437646\n","Test Loss:  65.05110763385892\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:41<01:07,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  280.29080481128767\n","Test Loss:  64.74101426964626\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:44<01:04,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  272.36514574242756\n","Test Loss:  64.54770324937999\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:48<01:01,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  271.80140845826827\n","Test Loss:  63.651897058822215\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:51<00:58,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  274.03655339288525\n","Test Loss:  63.755585353588685\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:55<00:55,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  270.7226171698421\n","Test Loss:  67.30121175618842\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:58<00:51,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  269.664635499008\n","Test Loss:  64.87984470371157\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:02<00:48,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  265.58135537663475\n","Test Loss:  65.52750868536532\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:05<00:48,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  266.5945462891832\n","Test Loss:  69.8193697500974\n","Early stopping with best_loss:  63.651897058822215 and test_loss for this epoch:  69.8193697500974 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","1.0182739226764146\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:48,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  733.5928170084953\n","Test Loss:  127.723582778126\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:46,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  481.8034733682871\n","Test Loss:  107.95258828811347\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:43,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  423.57481022179127\n","Test Loss:  95.72921965271235\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:40,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  380.3363992138766\n","Test Loss:  91.5318747330457\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:36,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  361.8375367624685\n","Test Loss:  88.3199577825144\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:32,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  348.42069094162434\n","Test Loss:  86.28765724552795\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:28,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  334.5730249979533\n","Test Loss:  83.7248332281597\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:26,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  318.7677501961589\n","Test Loss:  84.50902230851352\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:22,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  324.9825799902901\n","Test Loss:  85.2368541052565\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:34<02:18,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.729318741709\n","Test Loss:  79.51939792744815\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:38<02:14,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  306.01129667740315\n","Test Loss:  79.51932506402954\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:41<02:10,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  308.7331377505325\n","Test Loss:  77.09412617702037\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:44<02:06,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  286.7925577731803\n","Test Loss:  76.6828169664368\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:48<02:03,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  292.60557217523456\n","Test Loss:  75.54933167994022\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:51<02:00,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  284.0485466332175\n","Test Loss:  74.73337295651436\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:55<01:56,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  286.52856834605336\n","Test Loss:  73.00395654560998\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:58<01:53,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  282.14394492749125\n","Test Loss:  72.65160953812301\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:02<01:49,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  275.85754133667797\n","Test Loss:  72.18946594651788\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:05<01:45,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  277.4674982530996\n","Test Loss:  71.63538166554645\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:08<01:42,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  277.8279175227508\n","Test Loss:  72.44914366584271\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:12<01:39,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  274.78068048413843\n","Test Loss:  75.54993272479624\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:15<01:36,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  277.6064282201696\n","Test Loss:  71.58918869215995\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:19<01:33,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  272.45430534752086\n","Test Loss:  69.91124200867489\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:22<01:30,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  272.9806927302852\n","Test Loss:  72.023425221676\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:26<01:25,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  268.1674642721191\n","Test Loss:  78.8789481818676\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:29<01:22,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  261.3255072548054\n","Test Loss:  74.03383397031575\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:32<01:18,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  261.6144912673626\n","Test Loss:  72.1699932096526\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:36<01:22,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  259.1287736080121\n","Test Loss:  73.06864801328629\n","Early stopping with best_loss:  69.91124200867489 and test_loss for this epoch:  73.06864801328629 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","1.0885571096768427\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:06,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  704.6174860931933\n","Test Loss:  129.80008483701386\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:03,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  469.3672140510753\n","Test Loss:  94.16838908940554\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:59,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  411.69159565726295\n","Test Loss:  77.8127967454493\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<02:54,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  375.0755150737241\n","Test Loss:  97.56454615760595\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:50,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  360.336660921108\n","Test Loss:  71.33303325995803\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:47,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  341.0058020907454\n","Test Loss:  68.56808307301253\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:43,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  334.02779056783766\n","Test Loss:  68.06736239837483\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:30<02:40,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  322.5154345771298\n","Test Loss:  72.5882682017982\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:34<02:38,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  316.569232203532\n","Test Loss:  62.98282589297742\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:38<02:34,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  312.64517317712307\n","Test Loss:  62.22849372960627\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:42<02:32,  3.90s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  307.8000905539375\n","Test Loss:  65.90515329432674\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:46<02:27,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  300.74978461896535\n","Test Loss:  67.79932937957346\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:49<02:22,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  296.6425278382376\n","Test Loss:  65.38379053492099\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:53<02:17,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  292.4294138038531\n","Test Loss:  65.14317417889833\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:57<02:27,  4.10s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  290.56692982045934\n","Test Loss:  65.29472128860652\n","Early stopping with best_loss:  62.22849372960627 and test_loss for this epoch:  65.29472128860652 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","1.0073506498517064\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:35,  3.18s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  908.7036221548915\n","Test Loss:  135.65343952178955\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:33,  3.20s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  631.6142045259476\n","Test Loss:  106.50101719424129\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:31,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  510.1496740207076\n","Test Loss:  98.39081534184515\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:12<02:28,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  457.6211794037372\n","Test Loss:  89.99247757531703\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:25,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  420.26300824061036\n","Test Loss:  86.9718676302582\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:22,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  402.0791343348101\n","Test Loss:  83.2323987018317\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:22<02:18,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  384.21151698566973\n","Test Loss:  80.76429717428982\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:25<02:13,  3.19s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  368.62088032625616\n","Test Loss:  78.58110154047608\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:28<02:10,  3.18s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  367.85760988108814\n","Test Loss:  77.21674134582281\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:32<02:08,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  347.5310145658441\n","Test Loss:  77.24216146674007\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:35<02:05,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  345.3127730647102\n","Test Loss:  78.65017976611853\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:38<02:01,  3.20s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  337.486990025267\n","Test Loss:  73.16659820405766\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:41<01:58,  3.20s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  332.6236587278545\n","Test Loss:  70.73291448201053\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:44<01:55,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  326.7734513338655\n","Test Loss:  72.58672195672989\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:48<01:51,  3.20s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  319.30025159567595\n","Test Loss:  77.13435621932149\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:51<01:48,  3.20s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  315.9282816853374\n","Test Loss:  70.61226513236761\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:54<01:46,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  312.29366470640525\n","Test Loss:  73.75572511553764\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [00:57<01:42,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  308.430363221094\n","Test Loss:  70.48012575926259\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:00<01:38,  3.19s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  298.18839423684403\n","Test Loss:  68.86991951614618\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:04<01:36,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  301.80344829009846\n","Test Loss:  67.9401622270234\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:07<01:34,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  300.49202165938914\n","Test Loss:  67.00443362444639\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:10<01:30,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  295.2034840164706\n","Test Loss:  67.22326508909464\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:13<01:27,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  294.68738808948547\n","Test Loss:  66.86614143149927\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:17<01:23,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  291.7341770622879\n","Test Loss:  66.92791024129838\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:20<01:21,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  293.2211890765466\n","Test Loss:  65.62721352512017\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:23<01:17,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  287.22784404456615\n","Test Loss:  65.92689746827818\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:26<01:13,  3.20s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  287.0319435740821\n","Test Loss:  66.1201587729156\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:29<01:10,  3.20s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  280.25701277982444\n","Test Loss:  69.01042390754446\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:33<01:07,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  277.0582334762439\n","Test Loss:  64.98201113566756\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:36<01:04,  3.20s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  275.58316177595407\n","Test Loss:  68.89741980470717\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:39<01:00,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  274.0197173487395\n","Test Loss:  64.7302710423246\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:42<00:58,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  280.90347379492596\n","Test Loss:  65.52183011639863\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:46<00:55,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  273.4511787015945\n","Test Loss:  64.15196979139\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:49<00:51,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  273.0340441807639\n","Test Loss:  64.46265723230317\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:52<00:48,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  275.8803873038851\n","Test Loss:  64.84665973763913\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [01:56<00:45,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  271.29253752320074\n","Test Loss:  63.96322770603001\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [01:59<00:43,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  274.14951989706606\n","Test Loss:  65.4515363839455\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:02<00:39,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  270.16399729344994\n","Test Loss:  63.92652315553278\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:06<00:36,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  264.44794675242156\n","Test Loss:  65.54134204238653\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:09<00:33,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  270.55464398348704\n","Test Loss:  63.875382237136364\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:12<00:30,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  264.60880284849554\n","Test Loss:  64.20969899790362\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:16<00:26,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  264.6234504696913\n","Test Loss:  68.94042213819921\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:19<00:22,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  264.58691648067906\n","Test Loss:  63.65950579382479\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:22<00:19,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  264.69271270232275\n","Test Loss:  68.43319162260741\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:25<00:16,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  265.9779181946069\n","Test Loss:  63.12105353595689\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 92%|█████████▏| 46/50 [02:28<00:12,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  265.2246764479205\n","Test Loss:  64.94429069198668\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 94%|█████████▍| 47/50 [02:32<00:09,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  264.80611791368574\n","Test Loss:  63.569099268876016\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 96%|█████████▌| 48/50 [02:35<00:06,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  260.74096820130944\n","Test Loss:  63.67467479594052\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 49/50 [02:38<00:03,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  262.4201963483356\n","Test Loss:  62.67261152295396\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 50/50 [02:41<00:00,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  257.9624190381728\n","Test Loss:  62.47128669731319\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","1.0386332827434537\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:47,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  784.9422071427107\n","Test Loss:  125.75635305140167\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:42,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  533.0545621551573\n","Test Loss:  97.45440228283405\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:37,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  447.1802083682269\n","Test Loss:  88.74363648891449\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:33,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  415.4758838554844\n","Test Loss:  82.86992791946977\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:29,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  383.77121612243354\n","Test Loss:  79.30207659536973\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:26,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  361.33504454325885\n","Test Loss:  75.68518730299547\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:21,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  356.145457258448\n","Test Loss:  75.70964004797861\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:18,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  340.87941295839846\n","Test Loss:  71.33198643568903\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:29<02:15,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  336.1388138276525\n","Test Loss:  86.63303473219275\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:11,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  333.4241584856063\n","Test Loss:  67.2728119827807\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:36<02:08,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  323.8996283202432\n","Test Loss:  65.92229838483036\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:39<02:05,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  317.75416918681003\n","Test Loss:  64.29953990131617\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:01,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  312.79543402651325\n","Test Loss:  65.56585167441517\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:46<01:58,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  306.5162153139245\n","Test Loss:  63.658053361810744\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:49<01:55,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  300.0108247231692\n","Test Loss:  62.89199733128771\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:52<01:52,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  300.6971364878118\n","Test Loss:  63.11901964060962\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:56<01:49,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  294.2868032786064\n","Test Loss:  62.63760774768889\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [00:59<01:45,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  288.75332659156993\n","Test Loss:  62.813328858464956\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:02<01:42,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  291.9994169352576\n","Test Loss:  59.65152466855943\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:06<01:39,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  286.8935759291053\n","Test Loss:  60.809158680960536\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:09<01:35,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  289.668930192478\n","Test Loss:  60.355736561119556\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:12<01:31,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  283.5957452999428\n","Test Loss:  58.14848095923662\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:15<01:28,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  274.92060653027147\n","Test Loss:  61.04119299445301\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:19<01:25,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  277.5061557823792\n","Test Loss:  57.2758996586781\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:22<01:21,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  275.773467597086\n","Test Loss:  60.79804644174874\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:25<01:18,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  277.6815725923516\n","Test Loss:  59.12882269825786\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:29<01:15,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  271.6730185840279\n","Test Loss:  57.683886603452265\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:32<01:11,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  274.0248677995987\n","Test Loss:  57.67100354610011\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:35<01:08,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  271.32536449376494\n","Test Loss:  56.30103531898931\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:38<01:04,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  267.3871309333481\n","Test Loss:  57.712469649966806\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:42<01:03,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  270.29538245871663\n","Test Loss:  63.24394903937355\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:45<00:59,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  268.78310947632417\n","Test Loss:  62.243721056729555\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:48<00:56,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  262.1717629721388\n","Test Loss:  55.75021703960374\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:52<00:53,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  263.8963249004446\n","Test Loss:  59.07550839334726\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:55<00:49,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  266.32621585112065\n","Test Loss:  56.753107488155365\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [01:58<00:46,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  267.8177714715712\n","Test Loss:  56.709754227194935\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:01<00:42,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  257.5489457026124\n","Test Loss:  57.95793121820316\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:05<00:44,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  268.4219676100183\n","Test Loss:  58.35564246494323\n","Early stopping with best_loss:  55.75021703960374 and test_loss for this epoch:  58.35564246494323 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","0.9540413271235348\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:39,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  737.0857888497412\n","Test Loss:  114.27730053476989\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:36,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  493.24799504317343\n","Test Loss:  91.21904179267585\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:34,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  421.477602195926\n","Test Loss:  84.3213112950325\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:31,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  392.41350953187793\n","Test Loss:  76.7063859179616\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:27,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  372.92107130400836\n","Test Loss:  79.28845548257232\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:23,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  355.22599214175716\n","Test Loss:  70.84104368649423\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:22<02:20,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  338.3328033662401\n","Test Loss:  68.74155051168054\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:20,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  333.2189635508694\n","Test Loss:  68.24998394073918\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:20,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  318.44112364389\n","Test Loss:  66.24531001690775\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:15,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  319.02937605511397\n","Test Loss:  66.6261139716953\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:36<02:12,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  307.5491974311881\n","Test Loss:  66.59081637114286\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:09,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  306.86050273664296\n","Test Loss:  66.47061444818974\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:05,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  301.9830832062289\n","Test Loss:  62.50983357615769\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:46<02:02,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  296.8367945831269\n","Test Loss:  62.266497631557286\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:50<01:58,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  292.4901503585279\n","Test Loss:  60.217601732350886\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:53<01:56,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  290.6036722494755\n","Test Loss:  60.636485123075545\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:57<01:52,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  283.2218615899328\n","Test Loss:  63.00868702959269\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:00<01:49,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  279.4216513633728\n","Test Loss:  60.03025303874165\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:04<01:47,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  279.50275858771056\n","Test Loss:  63.518221164122224\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:07<01:43,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  276.0794387701899\n","Test Loss:  61.344045005738735\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:11<01:40,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  270.90594694018364\n","Test Loss:  64.47184268198907\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:14<01:36,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  277.7500396645628\n","Test Loss:  58.024464044719934\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:17<01:32,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  263.7601024452597\n","Test Loss:  59.005106201861054\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:21<01:28,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  267.9983103647828\n","Test Loss:  59.10548268817365\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:24<01:24,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  265.2709880582988\n","Test Loss:  60.439555464312434\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:27<01:20,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  263.1453633373603\n","Test Loss:  63.224318232387304\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:31<01:16,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  262.787861589808\n","Test Loss:  57.33035158412531\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:34<01:12,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  259.08495443081483\n","Test Loss:  55.47940365318209\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:37<01:09,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  256.25981563306414\n","Test Loss:  57.21497679501772\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:41<01:06,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  258.95346907060593\n","Test Loss:  56.29968105908483\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:44<01:02,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  249.41415920248255\n","Test Loss:  58.954925352707505\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:47<00:59,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  254.0813850047998\n","Test Loss:  55.99298179894686\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:50<01:02,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  258.92302662087604\n","Test Loss:  56.457207983825356\n","Early stopping with best_loss:  55.47940365318209 and test_loss for this epoch:  56.457207983825356 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","0.958868771406574\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:06,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  670.8907797355205\n","Test Loss:  107.17064385861158\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:06,  3.90s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  447.91102001816034\n","Test Loss:  88.41446060687304\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<03:02,  3.88s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  394.04036397486925\n","Test Loss:  83.72320306673646\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<02:57,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  361.9299643691629\n","Test Loss:  78.23807531781495\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:19<02:51,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  339.3675192957744\n","Test Loss:  76.36511708144099\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:46,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  322.29191721137613\n","Test Loss:  71.34088727273047\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:42,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  323.81487608421594\n","Test Loss:  68.19404284376651\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:30<02:37,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  307.4142643874511\n","Test Loss:  67.1094296942465\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:34<02:33,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  300.0158442379907\n","Test Loss:  74.87320813443512\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:37<02:28,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  295.96837970102206\n","Test Loss:  63.90845926711336\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:41<02:23,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  290.640496365726\n","Test Loss:  68.20369633287191\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:45<02:19,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  287.9747335065622\n","Test Loss:  63.36077015893534\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:48<02:15,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  279.052797163371\n","Test Loss:  61.284053994575515\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:52<02:13,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  277.6596121774055\n","Test Loss:  64.22261738963425\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:56<02:10,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  279.8465503070038\n","Test Loss:  63.00278013269417\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:59<02:07,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  271.37843511789106\n","Test Loss:  61.868544036056846\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:03<02:03,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  265.29802270932123\n","Test Loss:  59.69961387431249\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:07<01:59,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  270.22970249457285\n","Test Loss:  60.07232639484573\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:11<01:56,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  259.54813638236374\n","Test Loss:  61.83986032358371\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:15<01:53,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  261.52150979358703\n","Test Loss:  59.17812839662656\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:18<01:49,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  256.98925667081494\n","Test Loss:  57.893382794223726\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:22<01:46,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  256.7192908502184\n","Test Loss:  58.584947142750025\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:26<01:41,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  255.26854499336332\n","Test Loss:  57.64204926462844\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:30<01:37,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  251.28904191288166\n","Test Loss:  57.20267552416772\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:33<01:33,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  252.74975413503125\n","Test Loss:  57.31159795727581\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:37<01:30,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  250.8380531547591\n","Test Loss:  62.49830856663175\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:41<01:26,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  245.52419316070154\n","Test Loss:  57.44432558910921\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:45<01:22,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  247.9497916814871\n","Test Loss:  58.04707322502509\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:48<01:18,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  243.52529684035107\n","Test Loss:  56.07786257751286\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:52<01:14,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  245.27839522529393\n","Test Loss:  57.06401939783245\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:56<01:10,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  242.59854069422\n","Test Loss:  62.35554188629612\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [02:00<01:07,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  243.1216176985763\n","Test Loss:  55.78979478776455\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [02:03<01:03,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  236.37786203762516\n","Test Loss:  55.485405188053846\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [02:07<01:00,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  238.19075565505773\n","Test Loss:  57.43355078808963\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:11<00:56,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  230.29663355555385\n","Test Loss:  53.63364702370018\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:15<00:52,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  238.54626906616613\n","Test Loss:  53.39944067271426\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:18<00:48,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  231.53134614787996\n","Test Loss:  53.93662593374029\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:22<00:45,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  236.86862022895366\n","Test Loss:  58.98036733735353\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:26<00:41,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  226.97954167798162\n","Test Loss:  59.241661716252565\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:30<00:37,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  227.57637269981205\n","Test Loss:  56.375697940588\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:34<00:38,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  225.3685390916653\n","Test Loss:  57.049265345558524\n","Early stopping with best_loss:  53.39944067271426 and test_loss for this epoch:  57.049265345558524 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","0.9257056953924409\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:58,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  703.8880014736205\n","Test Loss:  89.29227192513645\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:55,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  460.048717148602\n","Test Loss:  87.6125082494691\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:50,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  416.63333195634186\n","Test Loss:  80.76442388957366\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:47,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  394.93584054522216\n","Test Loss:  75.4465093142353\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:42,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  378.5924304537475\n","Test Loss:  72.01761620352045\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:37,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  355.71288354694843\n","Test Loss:  72.52679495606571\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:33,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  354.9953191643581\n","Test Loss:  74.7421562471427\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:28,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  353.7736603487283\n","Test Loss:  73.64101306162775\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:32<02:25,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  356.44684108067304\n","Test Loss:  84.10525990463793\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:35<02:43,  3.98s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  345.03741474822164\n","Test Loss:  77.77728986740112\n","Early stopping with best_loss:  72.01761620352045 and test_loss for this epoch:  77.77728986740112 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","1.0750086395746334\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:55,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  664.07200403139\n","Test Loss:  118.06214977614582\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:52,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  438.2318122670986\n","Test Loss:  89.55255985446274\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:48,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  390.9507932672277\n","Test Loss:  120.23596182838082\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:45,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  361.123185233213\n","Test Loss:  87.37912209518254\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:42,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  357.8865793617442\n","Test Loss:  80.66957920324057\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:39,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  351.2484995191917\n","Test Loss:  89.98889749031514\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:36,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  348.9010694292374\n","Test Loss:  83.81456306390464\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:32,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  335.4963474571705\n","Test Loss:  82.3222283674404\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:32<02:28,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  337.02470448613167\n","Test Loss:  79.22151839639992\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:36<02:23,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  327.70697013475\n","Test Loss:  84.21549059450626\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:39<02:19,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  330.9710956555791\n","Test Loss:  74.9779659868218\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:43<02:16,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  322.0192661387846\n","Test Loss:  75.57600581273437\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:46<02:13,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  317.8086521171499\n","Test Loss:  77.2323269629851\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:50<02:09,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  322.5843092557043\n","Test Loss:  78.87632251996547\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:54<02:05,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  314.1178359389305\n","Test Loss:  73.70379261113703\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:57<02:02,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  313.02819305984303\n","Test Loss:  71.32192035485059\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:01<01:57,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  321.1722653862089\n","Test Loss:  72.08728490676731\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:04<01:54,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  312.80458405893296\n","Test Loss:  87.49163310509175\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:08<01:50,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  311.62277995515615\n","Test Loss:  72.65972139406949\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:11<01:46,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  311.0835175225511\n","Test Loss:  71.89466425962746\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:15<01:53,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  311.4361866042018\n","Test Loss:  74.22227485943586\n","Early stopping with best_loss:  71.32192035485059 and test_loss for this epoch:  74.22227485943586 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","1.065832069740951\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:02,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  616.7957032304257\n","Test Loss:  104.16226818040013\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:56,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  441.04645010922104\n","Test Loss:  81.91200965922326\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:53,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  398.5408836323768\n","Test Loss:  77.28902447409928\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:48,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  374.15062484843656\n","Test Loss:  73.4726872202009\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:44,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  358.84370317310095\n","Test Loss:  71.37165836337954\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:41,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  349.5435126135126\n","Test Loss:  68.36267972597852\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:37,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  349.1291063372046\n","Test Loss:  70.75364707224071\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:29<02:33,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  340.2679487178102\n","Test Loss:  67.21583325695246\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:32<02:29,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  343.2610249314457\n","Test Loss:  78.21108669787645\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:36<02:26,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  333.8237633658573\n","Test Loss:  68.5133068356663\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:40<02:22,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  333.09842111729085\n","Test Loss:  66.51160307484679\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:43<02:18,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  326.7373500801623\n","Test Loss:  67.60207431297749\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:47<02:15,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  325.91465071588755\n","Test Loss:  68.0054258722812\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:51<02:10,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  321.5702322591096\n","Test Loss:  67.23024477623403\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:54<02:06,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  316.5234203469008\n","Test Loss:  65.80016235355288\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:58<02:03,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  318.68650120776147\n","Test Loss:  67.92038794700056\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:02<01:59,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  327.2909499257803\n","Test Loss:  70.072697177995\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:05<01:55,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  319.3813757789321\n","Test Loss:  61.971155086299405\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:09<01:51,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  317.0160900433548\n","Test Loss:  63.07528078556061\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:12<01:46,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  314.3117822567001\n","Test Loss:  75.29907935904339\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:16<01:44,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  314.1325049540028\n","Test Loss:  61.727498922962695\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:19<01:40,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  312.7287270994857\n","Test Loss:  62.97503221593797\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:23<01:37,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  308.73558895103633\n","Test Loss:  62.697739829774946\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:27<01:32,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  309.8457806929946\n","Test Loss:  61.83221948519349\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:30<01:28,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  300.8136837249622\n","Test Loss:  62.70105573441833\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:34<01:34,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  306.52970104292035\n","Test Loss:  63.02509744092822\n","Early stopping with best_loss:  61.727498922962695 and test_loss for this epoch:  63.02509744092822 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","0.9603414721906188\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:02,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  591.6804404016584\n","Test Loss:  93.24955027736723\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:00,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  426.52343847975135\n","Test Loss:  90.11107129510492\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:57,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  383.83177219424397\n","Test Loss:  85.9961152090691\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<02:53,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  375.21026943065226\n","Test Loss:  76.911326254718\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:49,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  362.15180681226775\n","Test Loss:  76.45349355135113\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:47,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  353.3136028451845\n","Test Loss:  71.55959099903703\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:44,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  346.8020659564063\n","Test Loss:  71.80992723535746\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:30<02:40,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  334.09286899026483\n","Test Loss:  70.38613334996626\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:34<02:35,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  336.35855699982494\n","Test Loss:  78.2977156797424\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:37<02:32,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  334.2604932235554\n","Test Loss:  72.67877234052867\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:41<02:27,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  338.0258339671418\n","Test Loss:  76.85042097140104\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:45<02:22,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  321.67709885071963\n","Test Loss:  73.83667755685747\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:49<02:35,  4.10s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  326.64907575398684\n","Test Loss:  73.32262983173132\n","Early stopping with best_loss:  70.38613334996626 and test_loss for this epoch:  73.32262983173132 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","1.0539386160065802\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:42,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  724.8372220285237\n","Test Loss:  95.34564434643835\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:38,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  480.65004922356457\n","Test Loss:  82.01362323062494\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:35,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  406.1452648192644\n","Test Loss:  86.75133428908885\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:32,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  372.96022225916386\n","Test Loss:  75.13074117572978\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:29,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  360.2476633528713\n","Test Loss:  71.82729994133115\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:27,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  352.6964113339782\n","Test Loss:  78.50847455859184\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:24,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  341.6573242172599\n","Test Loss:  80.33129401132464\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:21,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  330.7830893518403\n","Test Loss:  72.0681696915999\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:18,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  332.258792294655\n","Test Loss:  74.14840644889046\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:14,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  329.2604176560417\n","Test Loss:  67.35049452446401\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:13,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  327.3316946849227\n","Test Loss:  79.17991889826953\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:08,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  322.08214090391994\n","Test Loss:  87.4902628082782\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:04,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  327.77868142025545\n","Test Loss:  65.19443468842655\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:47<02:01,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  324.0681373318657\n","Test Loss:  68.03427639929578\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:50<01:57,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  313.17017969302833\n","Test Loss:  70.16885319445282\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:53<01:53,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  314.3434921996668\n","Test Loss:  69.68174550309777\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:56<01:49,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  309.23005120269954\n","Test Loss:  69.50554143963382\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:00<01:57,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  307.39053664170206\n","Test Loss:  71.77331524528563\n","Early stopping with best_loss:  65.19443468842655 and test_loss for this epoch:  71.77331524528563 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","1.0174339468506217\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:44,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  636.8273311890662\n","Test Loss:  94.3247217386961\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:41,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  429.2536983098835\n","Test Loss:  74.09793657343835\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:39,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  392.98585540801287\n","Test Loss:  77.32558717764914\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:36,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  357.5016307933256\n","Test Loss:  72.02908202074468\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:31,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  348.05665944330394\n","Test Loss:  69.39898226037621\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:28,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  342.2363903527148\n","Test Loss:  71.5696407482028\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:25,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  344.57316380459815\n","Test Loss:  64.06987649388611\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:21,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  336.20089562330395\n","Test Loss:  70.31201548036188\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:18,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  328.30500082462095\n","Test Loss:  66.41773558687419\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:15,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  324.5010168533772\n","Test Loss:  64.25050253607333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:11,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  317.2710403446108\n","Test Loss:  66.43688790081069\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:07,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  322.516150563024\n","Test Loss:  63.61862179078162\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:04,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  318.8022288614884\n","Test Loss:  68.08825343474746\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:47<02:02,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  316.02854929305613\n","Test Loss:  67.32026078458875\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:50<01:58,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  316.7530019087717\n","Test Loss:  62.306583943311125\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:54<01:55,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  316.1436221052427\n","Test Loss:  72.19842156325467\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:57<01:52,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  307.1458438062109\n","Test Loss:  70.37880861014128\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:00<01:48,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  309.21718059200794\n","Test Loss:  65.87561820074916\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:04<01:44,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  306.6048127822578\n","Test Loss:  65.39757023379207\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:07<01:41,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  304.0263408208266\n","Test Loss:  61.40547221666202\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:11<01:38,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  305.83660233719274\n","Test Loss:  61.462176284752786\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:14<01:34,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  307.01187366992235\n","Test Loss:  68.13008373975754\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:17<01:31,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  301.6767142545432\n","Test Loss:  60.976551621221006\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:21<01:28,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  294.64034675993025\n","Test Loss:  60.907752581872046\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:24<01:25,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  298.7490007421002\n","Test Loss:  59.967859556898475\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:28<01:21,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  293.2513844203204\n","Test Loss:  59.4484141189605\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:31<01:19,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  293.9010233907029\n","Test Loss:  60.671599911060184\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:35<01:15,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  295.7125622620806\n","Test Loss:  66.28563748300076\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:38<01:12,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  295.0851466609165\n","Test Loss:  60.58096216246486\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:41<01:08,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  292.83559692744166\n","Test Loss:  59.94962664972991\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:45<01:10,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  287.1103828335181\n","Test Loss:  73.29714831896126\n","Early stopping with best_loss:  59.4484141189605 and test_loss for this epoch:  73.29714831896126 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","0.9611144975529161\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:49,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  585.0231514815241\n","Test Loss:  95.31706906482577\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:47,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  388.65138851851225\n","Test Loss:  87.1061814725399\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:43,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  359.81758724339306\n","Test Loss:  86.65257201716304\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:40,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  337.8552967235446\n","Test Loss:  85.14939117245376\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:36,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  329.5240801470354\n","Test Loss:  89.01033434644341\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:32,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  327.0755925439298\n","Test Loss:  79.20408609136939\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:29,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  323.23569671204314\n","Test Loss:  77.68067441694438\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:25,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  317.6903433771804\n","Test Loss:  80.10327699966729\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:22,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  319.8876894460991\n","Test Loss:  76.15144388307817\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:34<02:18,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  312.0286275432445\n","Test Loss:  76.05612758547068\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:38<02:14,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  305.4163683094084\n","Test Loss:  82.08332222141325\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:41<02:11,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  306.2365810535848\n","Test Loss:  75.60600283369422\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:45<02:07,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  304.367233694531\n","Test Loss:  84.3781680110842\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:48<02:04,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  301.47259915526956\n","Test Loss:  75.72996214870363\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:51<02:00,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  291.5355734946206\n","Test Loss:  76.81080642901361\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:55<01:57,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  295.6940624087583\n","Test Loss:  72.84766093501821\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:58<01:53,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  292.3111106599681\n","Test Loss:  72.29892300348729\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:02<01:50,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  295.694368917495\n","Test Loss:  72.64793653413653\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:05<01:46,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  293.3790655429475\n","Test Loss:  72.40353181399405\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:09<01:42,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  292.7191218934022\n","Test Loss:  90.34947043377906\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:12<01:39,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  292.81779324263334\n","Test Loss:  81.12582829594612\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:15<01:44,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  292.7503211023286\n","Test Loss:  78.96765043307096\n","Early stopping with best_loss:  72.29892300348729 and test_loss for this epoch:  78.96765043307096 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","1.0907803272374519\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:01,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  584.9478846006095\n","Test Loss:  90.59852086566389\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:58,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  409.15819266438484\n","Test Loss:  79.02528021717444\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:53,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  376.66979008726776\n","Test Loss:  73.57983925938606\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:49,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  362.63899591285735\n","Test Loss:  73.11380826029927\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:46,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  354.16568814078346\n","Test Loss:  74.84454200277105\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:43,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  345.2405463969335\n","Test Loss:  69.03514654748142\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:39,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  335.78468432836235\n","Test Loss:  80.51850425638258\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:29<02:35,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  326.99136647442356\n","Test Loss:  78.63729047961533\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:33<02:33,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  327.92567109223455\n","Test Loss:  73.7020956510678\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:37<02:29,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  322.7774068908766\n","Test Loss:  65.25588472932577\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:40<02:25,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  314.5560627593659\n","Test Loss:  69.69736193213612\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:44<02:22,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  322.37101469375193\n","Test Loss:  67.97922019194812\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:48<02:18,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  318.45289682783186\n","Test Loss:  65.16881697298959\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:52<02:15,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  314.48805470764637\n","Test Loss:  74.34407600574195\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:56<02:11,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  309.50796809699386\n","Test Loss:  70.01398457400501\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:59<02:07,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  307.74479031935334\n","Test Loss:  64.6859318735078\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:03<02:03,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  304.1048567527905\n","Test Loss:  69.46738753048703\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:07<01:59,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  305.81452478049323\n","Test Loss:  67.44167655240744\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:10<01:56,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  305.5754296206869\n","Test Loss:  60.64749645255506\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:14<01:53,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  302.46261926181614\n","Test Loss:  64.15848311316222\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:18<01:49,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  302.1490212958306\n","Test Loss:  60.496482802554965\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:22<01:45,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  304.0413139713928\n","Test Loss:  61.7442245837301\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:26<01:41,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  299.9495717603713\n","Test Loss:  60.3995596463792\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:29<01:37,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  298.9732249202207\n","Test Loss:  59.31271215877496\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:33<01:33,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  293.7536621373147\n","Test Loss:  69.59293643105775\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:37<01:29,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  298.3320650155656\n","Test Loss:  61.424225020222366\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:40<01:25,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  294.32679590955377\n","Test Loss:  67.81935771647841\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:44<01:21,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  290.83812199160457\n","Test Loss:  61.54070378607139\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:48<01:25,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  290.9904672540724\n","Test Loss:  60.492530941963196\n","Early stopping with best_loss:  59.31271215877496 and test_loss for this epoch:  60.492530941963196 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","0.9561949510570354\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:39,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  653.0944117866457\n","Test Loss:  99.26482683233917\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:36,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  417.8341035535559\n","Test Loss:  110.880046967417\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:33,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  376.0839096168056\n","Test Loss:  72.67260102927685\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:12<02:28,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  354.76213341671973\n","Test Loss:  75.96712825540453\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:24,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  336.9284710250795\n","Test Loss:  82.22647560806945\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:22,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  330.384785159491\n","Test Loss:  78.73652664851397\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:22<02:18,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  318.4737714258954\n","Test Loss:  68.56620766129345\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:25<02:15,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  320.26586786005646\n","Test Loss:  68.13308173045516\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:29<02:11,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  320.94068051408976\n","Test Loss:  68.262641383335\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:32<02:09,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  316.8054944071919\n","Test Loss:  68.92922667227685\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:35<02:06,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  313.59249903447926\n","Test Loss:  65.61442981660366\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:38<02:04,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  310.7431955584325\n","Test Loss:  72.89784330315888\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:42<02:01,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  301.82737808907405\n","Test Loss:  65.9678260628134\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:45<01:57,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  307.0348958428949\n","Test Loss:  69.92220997624099\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:48<01:52,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  306.2793946284801\n","Test Loss:  81.70402190461755\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:51<02:00,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  309.0424374532886\n","Test Loss:  68.17912516184151\n","Early stopping with best_loss:  65.61442981660366 and test_loss for this epoch:  68.17912516184151 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","1.0421279789981968\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:43,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  638.7933673039079\n","Test Loss:  98.46393118053675\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:39,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  435.09270154312253\n","Test Loss:  87.65212733112276\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:36,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  378.81571440957487\n","Test Loss:  77.9469957370311\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:32,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  348.4991997247562\n","Test Loss:  74.26397706288844\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:29,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  331.66764191072434\n","Test Loss:  74.05479075014591\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:26,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  335.284728044644\n","Test Loss:  72.60907086916268\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:22,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  333.468112885952\n","Test Loss:  74.20966508239508\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:20,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  310.92679470591247\n","Test Loss:  72.79631085880101\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:29<02:16,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.78345992974937\n","Test Loss:  69.14765394851565\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:13,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.6550136478618\n","Test Loss:  67.63754384266213\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:36<02:10,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  305.19040822423995\n","Test Loss:  75.03208087012172\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:07,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  306.3374794689007\n","Test Loss:  70.03726512053981\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:03,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  300.910860626027\n","Test Loss:  72.3403480798006\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:46<02:00,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  293.64797931164503\n","Test Loss:  68.90027053747326\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:50<01:57,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  289.41533397044986\n","Test Loss:  65.46091771591455\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:53<01:53,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  299.2790186214261\n","Test Loss:  68.37153886631131\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:56<01:50,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  286.8232471747324\n","Test Loss:  66.85499932989478\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:00<01:47,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  287.5604402059689\n","Test Loss:  68.81484001176432\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:03<01:43,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  280.06471220124513\n","Test Loss:  69.3060622275807\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:06<01:49,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  285.4137297673151\n","Test Loss:  71.71305062342435\n","Early stopping with best_loss:  65.46091771591455 and test_loss for this epoch:  71.71305062342435 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","1.041104284956402\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:44,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  569.48550709337\n","Test Loss:  95.23353209905326\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:41,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  389.48806296661496\n","Test Loss:  86.26640792191029\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:37,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  358.11426638951525\n","Test Loss:  85.57395127229393\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:34,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  339.0569336405024\n","Test Loss:  82.91327244788408\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:29,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  323.90822656219825\n","Test Loss:  84.76978242117912\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:27,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  317.3942851051688\n","Test Loss:  80.93534474819899\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:24,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  312.4856988787651\n","Test Loss:  75.94923306861892\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:20,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  308.90409633284435\n","Test Loss:  71.9506182121113\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:17,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  310.3172355853021\n","Test Loss:  78.89694345369935\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:14,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  294.5830519320443\n","Test Loss:  71.30990968691185\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:36<02:11,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  298.0360070494935\n","Test Loss:  71.31965416949242\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:08,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  296.69063356658444\n","Test Loss:  69.30157187953591\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:05,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  282.358914071694\n","Test Loss:  72.20979425637051\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:47<02:01,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  293.6325299204327\n","Test Loss:  75.90419405698776\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:50<01:57,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  287.8073945627548\n","Test Loss:  71.26661747135222\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:53<01:54,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  287.63876414857805\n","Test Loss:  69.03038223832846\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:57<01:51,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  281.1109989332035\n","Test Loss:  72.04741241736338\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:00<01:47,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  284.0276407864876\n","Test Loss:  77.73267604783177\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:03<01:44,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  277.9965229611844\n","Test Loss:  76.59251539083198\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:07<01:40,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  277.53770160721615\n","Test Loss:  67.38989942148328\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:10<01:38,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  282.8719595503062\n","Test Loss:  66.3108230328653\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:13<01:34,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  278.80391986388713\n","Test Loss:  69.43994787707925\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:17<01:31,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  274.3781344096642\n","Test Loss:  70.5502508468926\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:20<01:27,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  269.87285209959373\n","Test Loss:  63.823404799215496\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:24<01:24,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  273.57066157320514\n","Test Loss:  72.72305451938882\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:27<01:20,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  265.0288063529879\n","Test Loss:  66.04246719321236\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:30<01:17,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  267.8175469841808\n","Test Loss:  69.70346293132752\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:34<01:14,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  269.2031495575793\n","Test Loss:  69.11115211248398\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:37<01:16,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  264.6492406409234\n","Test Loss:  69.5793961789459\n","Early stopping with best_loss:  63.823404799215496 and test_loss for this epoch:  69.5793961789459 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","1.0316981257863111\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:01,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  565.4238765696064\n","Test Loss:  82.41277840547264\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:59,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  387.3421956785023\n","Test Loss:  89.87203280674294\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:55,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  355.2434516996145\n","Test Loss:  73.5546202417463\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:51,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  345.0750588439405\n","Test Loss:  77.23998569743708\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:49,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  332.4986438360065\n","Test Loss:  73.09027533140033\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:44,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  318.6584213702008\n","Test Loss:  75.77970132604241\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:40,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  309.1785134682432\n","Test Loss:  74.66762083768845\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:29<02:36,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  312.93358829803765\n","Test Loss:  81.04351997224148\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:33<02:33,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  310.0803486662917\n","Test Loss:  72.18708492582664\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:37<02:29,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  304.8438228322193\n","Test Loss:  77.88240764848888\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:41<02:26,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  299.806275382638\n","Test Loss:  78.47690672427416\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:44<02:22,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  299.7280333288945\n","Test Loss:  68.39043641835451\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:48<02:18,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  295.15146747324616\n","Test Loss:  68.65183880832046\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:52<02:13,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  292.86968743801117\n","Test Loss:  69.52543515199795\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:55<02:10,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  296.3491265811026\n","Test Loss:  66.64421457983553\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:59<02:06,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  292.6746983975172\n","Test Loss:  70.35352083668113\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:03<02:02,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  284.529042431619\n","Test Loss:  70.97486618347466\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:07<01:58,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  288.1947520431131\n","Test Loss:  63.819480126257986\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:10<01:54,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  285.18805008754134\n","Test Loss:  67.339536936488\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:14<01:50,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  286.5085815023631\n","Test Loss:  62.74206951120868\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:18<01:46,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  283.0913371099159\n","Test Loss:  65.86206265026703\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:21<01:43,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  285.80556903313845\n","Test Loss:  63.389737769030035\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:25<01:39,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  277.55743491090834\n","Test Loss:  68.30280114198104\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:29<01:36,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  274.6854429207742\n","Test Loss:  66.34077736735344\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:32<01:40,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  278.1522209555842\n","Test Loss:  69.5289705414325\n","Early stopping with best_loss:  62.74206951120868 and test_loss for this epoch:  69.5289705414325 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","1.0065092286913633\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:36,  3.20s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  692.5906900539994\n","Test Loss:  96.04676014184952\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:30,  3.14s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  454.72770240996033\n","Test Loss:  83.10176199860871\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:28,  3.15s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  398.5846424298361\n","Test Loss:  83.69194943690673\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:12<02:23,  3.12s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  358.6250113416463\n","Test Loss:  73.56933584902436\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:15<02:21,  3.13s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  350.59251637011766\n","Test Loss:  72.98326013982296\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:18<02:18,  3.15s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  325.29098147014156\n","Test Loss:  69.82128331670538\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:22<02:15,  3.16s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  332.09466562327\n","Test Loss:  68.64057653583586\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:25<02:12,  3.15s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  315.22301436401904\n","Test Loss:  80.4827655851841\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:28<02:09,  3.15s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  315.8805685611442\n","Test Loss:  68.84309561923146\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:31<02:06,  3.15s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  309.747451428324\n","Test Loss:  72.23955722339451\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:34<02:03,  3.16s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  316.68549661990255\n","Test Loss:  71.4860094729811\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:14,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  300.6493641445413\n","Test Loss:  82.23145460151136\n","Early stopping with best_loss:  68.64057653583586 and test_loss for this epoch:  82.23145460151136 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","1.10239725798997\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:37,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  603.7290655784309\n","Test Loss:  84.47611577715725\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:35,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  400.742848074995\n","Test Loss:  77.63617680221796\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:31,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  364.3632569005713\n","Test Loss:  73.60861056298018\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:12<02:28,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  336.37565239332616\n","Test Loss:  74.67238849867135\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:24,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  325.5765363564715\n","Test Loss:  69.24233192391694\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:21,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  323.6902021812275\n","Test Loss:  69.59801382757723\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:22<02:17,  3.20s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  303.85227854177356\n","Test Loss:  66.80387457180768\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:25<02:14,  3.19s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.7455913713202\n","Test Loss:  64.32943691220134\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:28<02:10,  3.19s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  301.1032206090167\n","Test Loss:  64.69180329889059\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:32<02:08,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  290.3784896945581\n","Test Loss:  67.58019564626738\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:35<02:05,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  296.21889282856137\n","Test Loss:  62.2233530941885\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:38<02:01,  3.20s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  286.0382914552465\n","Test Loss:  66.15368831716478\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:41<01:58,  3.20s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  288.28858945611864\n","Test Loss:  65.37965387897566\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:44<01:55,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  278.44918450014666\n","Test Loss:  66.05574001569767\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:48<01:52,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  280.29060080554336\n","Test Loss:  59.9151893844828\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:51<01:49,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  279.5128301172517\n","Test Loss:  61.08192336652428\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:54<01:46,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  274.26659604907036\n","Test Loss:  64.11019421834499\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [00:57<01:42,  3.20s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  273.90404370846227\n","Test Loss:  64.99407853418961\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:01<01:40,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  275.02148629631847\n","Test Loss:  62.147740243468434\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:04<01:36,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  272.25978395109996\n","Test Loss:  58.19583053980023\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:07<01:33,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  265.80523886345327\n","Test Loss:  58.654370456933975\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:10<01:30,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  264.9979760698043\n","Test Loss:  69.98317068815231\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:13<01:26,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  265.7575134737417\n","Test Loss:  57.86282782861963\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:17<01:23,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  263.0805890150368\n","Test Loss:  57.97527882177383\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:20<01:20,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  260.59505727607757\n","Test Loss:  56.804504440166056\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:23<01:17,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  257.51610731426626\n","Test Loss:  57.85097467061132\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:26<01:13,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  259.68777226563543\n","Test Loss:  60.49772664066404\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:30<01:10,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  258.2705898513086\n","Test Loss:  62.76697074994445\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:33<01:07,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  252.92713319323957\n","Test Loss:  57.573001326061785\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:36<01:04,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  251.7464957893826\n","Test Loss:  56.1929600937292\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:39<01:01,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  256.4088200898841\n","Test Loss:  56.96927000069991\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:42<00:57,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  247.16759726777673\n","Test Loss:  54.54933932935819\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:46<00:54,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  254.96250783931464\n","Test Loss:  55.80807582428679\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:49<00:51,  3.19s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  246.43534478358924\n","Test Loss:  59.99599619489163\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:52<00:48,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  250.8265675464645\n","Test Loss:  58.684851166792214\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [01:55<00:45,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  246.43359293043613\n","Test Loss:  61.49918639054522\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [01:58<00:46,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  242.63155398285016\n","Test Loss:  55.89099372457713\n","Early stopping with best_loss:  54.54933932935819 and test_loss for this epoch:  55.89099372457713 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","0.9490091118312021\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:40,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  575.8053490333259\n","Test Loss:  83.84566644951701\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:37,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  395.0201761480421\n","Test Loss:  68.78986231610179\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:33,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  356.1248090621084\n","Test Loss:  64.705900141038\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:29,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  328.4589687800035\n","Test Loss:  60.58517859503627\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:24,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  318.6779562733136\n","Test Loss:  80.41087657120079\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:22,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  305.69769743736833\n","Test Loss:  65.38158595375717\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:22<02:19,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  306.56389058055356\n","Test Loss:  58.467053649015725\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:25<02:15,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  307.0850542096887\n","Test Loss:  60.77522066421807\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:29<02:14,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  304.65232038870454\n","Test Loss:  61.6888153674081\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:32<02:12,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  292.08768778899685\n","Test Loss:  57.66665694024414\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:35<02:08,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  293.13855960126966\n","Test Loss:  57.67406212631613\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:39<02:05,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  295.08784285280854\n","Test Loss:  56.40836300048977\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:42<02:02,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  277.85155534092337\n","Test Loss:  55.270574897527695\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:45<01:59,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  282.1406189147383\n","Test Loss:  53.73516685748473\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:49<01:55,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  288.50488043017685\n","Test Loss:  57.40848934836686\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:52<01:52,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  280.92427600361407\n","Test Loss:  62.06699820421636\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:55<01:49,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  278.06177021190524\n","Test Loss:  57.99366755736992\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [00:59<01:46,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  278.04184844112024\n","Test Loss:  60.49168502911925\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:02<01:51,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  272.10067080007866\n","Test Loss:  58.467578107491136\n","Early stopping with best_loss:  53.73516685748473 and test_loss for this epoch:  58.467578107491136 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","0.931844764868428\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:01,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  523.8836473152041\n","Test Loss:  84.24094813596457\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:58,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  373.5257623703219\n","Test Loss:  89.06181511096656\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:54,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  348.78450572583824\n","Test Loss:  74.28143823612481\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:52,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  323.35333794681355\n","Test Loss:  67.4015159253031\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:47,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  317.83063313318416\n","Test Loss:  71.55927540268749\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:42,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  312.72085912385955\n","Test Loss:  64.94397227279842\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:38,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  306.36107452888973\n","Test Loss:  64.4277862245217\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:29<02:34,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  297.19730804814026\n","Test Loss:  63.14408555813134\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:33<02:31,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  296.37690618960187\n","Test Loss:  65.94873290043324\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:37<02:28,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  292.992286008317\n","Test Loss:  64.54632954020053\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:40<02:26,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  291.2748709479347\n","Test Loss:  62.56384793296456\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:44<02:22,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  286.9534108315129\n","Test Loss:  57.89137255307287\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:48<02:17,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  288.07851704768836\n","Test Loss:  68.13822630234063\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:51<02:12,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  281.09286423493177\n","Test Loss:  57.879127545282245\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:55<02:08,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  276.07747331354767\n","Test Loss:  71.84866855852306\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:59<02:03,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  276.6241882070899\n","Test Loss:  59.93133067432791\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:02<01:59,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  276.0376708076801\n","Test Loss:  57.67264630179852\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:06<01:55,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  272.09137415420264\n","Test Loss:  71.85509020276368\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:09<01:52,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  274.8470167124178\n","Test Loss:  60.765961684286594\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:13<01:47,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  265.0882023284212\n","Test Loss:  60.29373772535473\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:17<01:44,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  267.3521917592734\n","Test Loss:  59.29844210809097\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:20<01:51,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  269.63214556733146\n","Test Loss:  60.28961802320555\n","Early stopping with best_loss:  57.67264630179852 and test_loss for this epoch:  60.28961802320555 ...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mae_validation:\n","0.9753986214078826\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:45,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  980.9985275790095\n","Test Loss:  151.00656286999583\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:43,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  681.9290638081729\n","Test Loss:  123.87604995444417\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:39,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  574.6358907744288\n","Test Loss:  112.29672029800713\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:35,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  512.9663488809019\n","Test Loss:  107.59453881904483\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:31,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  494.11265489272773\n","Test Loss:  103.88883410580456\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:28,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  470.1484165471047\n","Test Loss:  99.05349676404148\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:25,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  439.0861808974296\n","Test Loss:  99.3557608728297\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:21,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  437.5771933752112\n","Test Loss:  95.59418013039976\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:17,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  421.57396553084254\n","Test Loss:  93.34139645146206\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:14,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  407.42562386160716\n","Test Loss:  89.92853350192308\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:11,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  404.88425817899406\n","Test Loss:  87.7369171641767\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:08,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  394.95121225435287\n","Test Loss:  88.88662377046421\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:05,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  388.552720121108\n","Test Loss:  88.02791544608772\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:47<02:02,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  381.0713237626478\n","Test Loss:  83.53521925024688\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:50<01:58,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  376.30665189400315\n","Test Loss:  82.85124483890831\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:54<01:55,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  370.75116529036313\n","Test Loss:  83.72167540248483\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:57<01:56,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  371.05861350893974\n","Test Loss:  82.76699008769356\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:01<01:51,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  366.3907699836418\n","Test Loss:  81.91305200755596\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:04<01:47,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  360.8071641162969\n","Test Loss:  82.3026459980756\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:08<01:44,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  359.4495657905936\n","Test Loss:  82.34010703652166\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:11<01:40,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  353.43778803991154\n","Test Loss:  78.18630890082568\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:15<01:36,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  355.18067144602537\n","Test Loss:  81.93885652069002\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:18<01:32,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  345.78339841309935\n","Test Loss:  78.50422500725836\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:21<01:29,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  350.0930630862713\n","Test Loss:  78.85544632561505\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:25<01:25,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  345.49876853683963\n","Test Loss:  77.25323691312224\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:28<01:23,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  340.9928285093047\n","Test Loss:  76.06120526604354\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:32<01:19,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  338.3825545338914\n","Test Loss:  75.8834094251506\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:35<01:16,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  334.57093141041696\n","Test Loss:  77.47343983710743\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:39<01:12,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  330.21562654944137\n","Test Loss:  75.53738598758355\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:42<01:08,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  332.2590155787766\n","Test Loss:  76.0789468260482\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:46<01:05,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  332.7714874432422\n","Test Loss:  76.29540781676769\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:49<01:01,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  322.73569497512653\n","Test Loss:  74.67836173437536\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:52<00:57,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  328.82605214603245\n","Test Loss:  76.08298731734976\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:56<00:53,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  329.13087011454627\n","Test Loss:  74.52278427686542\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:59<00:50,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  321.0551629480906\n","Test Loss:  73.82051911624148\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:02<00:47,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  322.37261152872816\n","Test Loss:  75.2645406210795\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:06<00:43,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  320.28131242934614\n","Test Loss:  74.35917065083049\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:09<00:40,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  321.02584896050394\n","Test Loss:  73.90470779268071\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:12<00:36,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  328.4685361222364\n","Test Loss:  73.92358888266608\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:16<00:34,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  320.4954352071509\n","Test Loss:  72.797966924496\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:20<00:31,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  322.9725921852514\n","Test Loss:  73.22820231039077\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:23<00:27,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  319.14727054024115\n","Test Loss:  74.66355148609728\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:27<00:24,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  313.9016847414896\n","Test Loss:  74.08501865295693\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:30<00:20,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  313.5106693659909\n","Test Loss:  73.27510284585878\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:34<00:17,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  312.8813423058018\n","Test Loss:  72.58494938816875\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 92%|█████████▏| 46/50 [02:37<00:13,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  311.9087475039996\n","Test Loss:  75.79959318367764\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 94%|█████████▍| 47/50 [02:40<00:10,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  311.4556403309107\n","Test Loss:  73.3032622965984\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 96%|█████████▌| 48/50 [02:44<00:06,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  314.4752576961182\n","Test Loss:  73.06333837192506\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 49/50 [02:48<00:03,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  307.1855223388411\n","Test Loss:  71.98501182161272\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 50/50 [02:51<00:00,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  305.6118586147204\n","Test Loss:  71.29291776637547\n","mae_validation:\n","1.0581441722263292\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:59,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  896.1921398714185\n","Test Loss:  140.09123646095395\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:56,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  594.8418581150472\n","Test Loss:  106.2762257438153\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:52,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  504.69454047456384\n","Test Loss:  96.78110200539231\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:47,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  467.28230004385114\n","Test Loss:  94.3590893521905\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:44,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  442.8773159859702\n","Test Loss:  90.36130417184904\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:40,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  421.59387027006596\n","Test Loss:  89.52990753669292\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:35,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  414.0944743603468\n","Test Loss:  84.85057628620416\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:29<02:31,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  407.1995271896012\n","Test Loss:  85.31781781092286\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:32<02:27,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  392.18183995224535\n","Test Loss:  81.28702566819265\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:36<02:24,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  384.082323551178\n","Test Loss:  80.71634238399565\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:39<02:19,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  378.35064425109886\n","Test Loss:  81.69328132364899\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:43<02:15,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  370.72712789010257\n","Test Loss:  76.87788545899093\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:46<02:12,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  360.3374716206454\n","Test Loss:  76.79805715568364\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:50<02:09,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  358.2586137210019\n","Test Loss:  74.5983154233545\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:54<02:05,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  357.0241296486929\n","Test Loss:  73.36508861556649\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:57<02:01,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  349.7080612669233\n","Test Loss:  72.11404898343608\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:01<01:58,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  345.35593509534374\n","Test Loss:  78.82268694601953\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:04<01:54,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  340.593528278172\n","Test Loss:  70.58098001265898\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:08<01:51,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  336.1204105014913\n","Test Loss:  78.15353026241064\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:12<01:47,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  339.78804302820936\n","Test Loss:  71.8718658098951\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:15<01:43,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  327.7054171352647\n","Test Loss:  76.19804296223447\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:19<01:39,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  330.32378719188273\n","Test Loss:  67.74360627960414\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:22<01:35,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  326.59607338672504\n","Test Loss:  68.12230137269944\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:26<01:31,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  325.3799439817667\n","Test Loss:  76.83279704954475\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:29<01:28,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  319.9509178856388\n","Test Loss:  68.58171657519415\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:33<01:24,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  326.7425342691131\n","Test Loss:  69.31206606281921\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:36<01:21,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  325.6309775210684\n","Test Loss:  67.71880606329069\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:40<01:17,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  323.10303566837683\n","Test Loss:  66.09491247497499\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:43<01:14,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  318.47950764512643\n","Test Loss:  66.10899222665466\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:47<01:10,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  320.62533722724766\n","Test Loss:  66.00787872448564\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:50<01:07,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  314.5518819289282\n","Test Loss:  68.14281402342021\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:54<01:04,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  312.9901533597149\n","Test Loss:  68.57602981943637\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:58<01:00,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  314.0579975822475\n","Test Loss:  70.1439009513706\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [02:01<00:57,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  308.616505633574\n","Test Loss:  66.07190634962171\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:05<00:53,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.3585633211769\n","Test Loss:  65.86212343722582\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:08<00:50,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  307.6918938367162\n","Test Loss:  68.82978961709887\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:12<00:46,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.43691285699606\n","Test Loss:  65.00684567447752\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:16<00:43,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  310.6267177914269\n","Test Loss:  65.7797343567945\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:19<00:39,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  307.97438983479515\n","Test Loss:  64.72093555983156\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:23<00:35,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  307.33896851632744\n","Test Loss:  64.19499660469592\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:26<00:32,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  308.4944911072962\n","Test Loss:  63.73022469691932\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:30<00:28,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  310.23714695638046\n","Test Loss:  64.10778101533651\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:34<00:25,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  307.01900433935225\n","Test Loss:  63.16613386338577\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:37<00:21,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  306.87264793366194\n","Test Loss:  63.9786710543558\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:41<00:17,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  299.65313177206554\n","Test Loss:  62.9922906239517\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 92%|█████████▏| 46/50 [02:44<00:14,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  295.8585194083862\n","Test Loss:  62.704409717582166\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 94%|█████████▍| 47/50 [02:48<00:10,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  302.52671148627996\n","Test Loss:  62.13223902415484\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 96%|█████████▌| 48/50 [02:51<00:07,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  299.5149392266758\n","Test Loss:  63.18777875602245\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 49/50 [02:55<00:03,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  298.46238421602175\n","Test Loss:  70.58985752193257\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 50/50 [02:58<00:00,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  297.7729825293645\n","Test Loss:  62.67953111091629\n","mae_validation:\n","0.9576770838856902\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:53,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  808.2366770207882\n","Test Loss:  125.44367723166943\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:48,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  548.7369950208813\n","Test Loss:  99.89839231967926\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:45,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  479.48168239183724\n","Test Loss:  89.88185941334814\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:42,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  452.57754201162606\n","Test Loss:  87.99570449418388\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:38,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  428.5202414467931\n","Test Loss:  83.47488249372691\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:37,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  417.78779960796237\n","Test Loss:  80.09843059908599\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:32,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  390.07104321662337\n","Test Loss:  80.2733591645956\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:29,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  388.5122865908779\n","Test Loss:  76.42531456053257\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:32<02:27,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  376.0499988673255\n","Test Loss:  75.7642630096525\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:35<02:23,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  364.5315957781859\n","Test Loss:  74.49625972472131\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:39<02:21,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  355.0560881718993\n","Test Loss:  70.55245215725154\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:43<02:18,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  359.6092131738551\n","Test Loss:  69.7570471582003\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:46<02:16,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  353.75206968933344\n","Test Loss:  71.6960471663624\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:50<02:12,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  335.96721610054374\n","Test Loss:  69.85115677956492\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:54<02:08,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  337.5796061810106\n","Test Loss:  68.14090653927997\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:57<02:04,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  332.7887882203795\n","Test Loss:  71.79907735437155\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:01<02:00,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  333.5612829073798\n","Test Loss:  67.29433191940188\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:04<01:55,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  323.67074694857\n","Test Loss:  68.41070501040667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:08<01:52,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  326.33408294571564\n","Test Loss:  68.15235149487853\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:12<01:48,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  319.2776677226648\n","Test Loss:  68.00483351945877\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:15<01:45,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  322.6166683968622\n","Test Loss:  71.798877495341\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:19<01:49,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  326.6543952189386\n","Test Loss:  68.72458571474999\n","Early stopping with best_loss:  67.29433191940188 and test_loss for this epoch:  68.72458571474999 ...\n","mae_validation:\n","1.0116523985595407\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:07,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  752.1892310269177\n","Test Loss:  138.790231321007\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:04,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  503.5194971803576\n","Test Loss:  110.72711897268891\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:59,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  450.6694003678858\n","Test Loss:  113.537481248728\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<02:57,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  422.2619314584881\n","Test Loss:  119.61246499046683\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:19<02:55,  3.89s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  393.6494368314743\n","Test Loss:  94.42557906266302\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:23<02:52,  3.93s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  373.4658189462498\n","Test Loss:  88.20300333155319\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:27<02:49,  3.94s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  363.3282439224422\n","Test Loss:  86.3603521073237\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:31<02:46,  3.96s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  352.5953059401363\n","Test Loss:  84.59648240171373\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:35<02:41,  3.95s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  347.201979477657\n","Test Loss:  83.127313723322\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:39<02:38,  3.96s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  340.2405684236437\n","Test Loss:  80.41646923217922\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:43<02:35,  3.98s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  334.26498691691086\n","Test Loss:  80.98050898034126\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:47<02:30,  3.95s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  333.4541644542478\n","Test Loss:  79.98842430626974\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:51<02:27,  4.00s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  331.3770884075202\n","Test Loss:  79.76189119368792\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:55<02:23,  3.97s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  315.2984635112807\n","Test Loss:  82.28050554729998\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:59<02:18,  3.95s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  318.88252335973084\n","Test Loss:  77.79552036244422\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [01:02<02:14,  3.95s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  317.33578148204833\n","Test Loss:  78.65804089070298\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:06<02:09,  3.94s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.6340316524729\n","Test Loss:  77.17638754937798\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:10<02:04,  3.90s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  309.4455016166903\n","Test Loss:  80.2234419118613\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:14<02:00,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  306.99276902829297\n","Test Loss:  76.53724262584001\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:18<01:55,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  301.7370673860423\n","Test Loss:  78.46956057287753\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:22<01:51,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  304.1103186267428\n","Test Loss:  77.45065324846655\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:26<01:47,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  302.64581394032575\n","Test Loss:  75.50415671244264\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:29<01:44,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  306.5043970984407\n","Test Loss:  76.50525649776682\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:33<01:39,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  303.982619301416\n","Test Loss:  80.74212968163192\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:37<01:35,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  300.9110562889837\n","Test Loss:  72.58542417595163\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:41<01:31,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  299.0652699912898\n","Test Loss:  79.84342406969517\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:45<01:27,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  297.3462663125247\n","Test Loss:  73.55578884016722\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:48<01:23,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  292.6301259510219\n","Test Loss:  73.02237647306174\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:52<01:19,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  288.9396193763241\n","Test Loss:  82.69686842896044\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:56<01:24,  4.02s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  290.0481648091227\n","Test Loss:  73.92292991187423\n","Early stopping with best_loss:  72.58542417595163 and test_loss for this epoch:  73.92292991187423 ...\n","mae_validation:\n","1.0651203172945511\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:46,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  969.2839003130794\n","Test Loss:  159.66516526974738\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:43,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  670.2810288108885\n","Test Loss:  127.40848126634955\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:38,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  540.5921780653298\n","Test Loss:  110.50345646589994\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:34,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  484.57448451034725\n","Test Loss:  109.16044469922781\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:29,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  468.663653139025\n","Test Loss:  107.57000852562487\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:26,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  446.40590381389484\n","Test Loss:  95.80032515246421\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:22,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  431.59622897207737\n","Test Loss:  89.99133590050042\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:19,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  411.84991033840925\n","Test Loss:  88.67360691446811\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:29<02:15,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  408.48516041971743\n","Test Loss:  84.02404120797291\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:12,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  397.9177342224866\n","Test Loss:  83.46375240385532\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:36<02:08,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  388.4486593757756\n","Test Loss:  78.24463864695281\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:39<02:05,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  376.7294230610132\n","Test Loss:  84.8660373929888\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:02,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  366.80742007121444\n","Test Loss:  77.65875459462404\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:46<01:58,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  362.79537493688986\n","Test Loss:  75.11290257447399\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:49<01:55,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  356.5508869802579\n","Test Loss:  73.19864452071488\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:53<01:52,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  358.0076110791415\n","Test Loss:  73.7220061160624\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:56<01:49,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  347.2099574781023\n","Test Loss:  70.52489822311327\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [00:59<01:45,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  348.0931695718318\n","Test Loss:  69.10478675970808\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:02<01:42,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  336.52376459911466\n","Test Loss:  68.71500771073624\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:06<01:39,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  333.429781812476\n","Test Loss:  67.68450776394457\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:09<01:35,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  332.0270347301848\n","Test Loss:  66.8325137142092\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:12<01:32,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  323.9192699654959\n","Test Loss:  71.70373987639323\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:16<01:29,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  328.0755880130455\n","Test Loss:  65.49434099439532\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:19<01:26,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  330.68336416920647\n","Test Loss:  65.87244547903538\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:22<01:22,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  322.64456465560943\n","Test Loss:  72.43069529300556\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:26<01:18,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  321.9701822614297\n","Test Loss:  64.5082897040993\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:29<01:15,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  319.5501724369824\n","Test Loss:  64.60055604763329\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:32<01:12,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  320.6193918278441\n","Test Loss:  65.24429952539504\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:35<01:09,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  314.2990044027101\n","Test Loss:  64.71382775902748\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:39<01:05,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  313.21674604760483\n","Test Loss:  64.17381291277707\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:42<01:02,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  314.8793886024505\n","Test Loss:  63.3389547476545\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:45<00:59,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  320.22762912511826\n","Test Loss:  63.38017955515534\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:49<00:56,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  315.81256797444075\n","Test Loss:  63.3357087476179\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:52<00:52,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.40136570855975\n","Test Loss:  62.5994401848875\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:55<00:49,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  313.1623353695031\n","Test Loss:  64.2957130796276\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [01:59<00:46,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  306.7030287878588\n","Test Loss:  62.41291320556775\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:02<00:43,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  309.96913659689017\n","Test Loss:  63.72883996600285\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:05<00:39,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  303.08645152207464\n","Test Loss:  62.139013457577676\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:09<00:36,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  306.27467632247135\n","Test Loss:  62.28016857523471\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:12<00:33,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  307.0412842477672\n","Test Loss:  65.88484240649268\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:15<00:30,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  299.12365431850776\n","Test Loss:  61.500739487819374\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:19<00:26,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  306.1904360661283\n","Test Loss:  65.48032361455262\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:22<00:23,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  297.71067063347436\n","Test Loss:  68.25855275988579\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:25<00:19,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  301.9965774714947\n","Test Loss:  60.57647785660811\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:29<00:16,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  303.1537159807049\n","Test Loss:  61.48819449963048\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 92%|█████████▏| 46/50 [02:32<00:13,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  300.43352240510285\n","Test Loss:  63.498501408845186\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 94%|█████████▍| 47/50 [02:35<00:09,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  301.36471300339326\n","Test Loss:  62.96234940481372\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 96%|█████████▌| 48/50 [02:39<00:06,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  296.4949761934113\n","Test Loss:  60.4273686055094\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 49/50 [02:42<00:03,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  298.18797251489013\n","Test Loss:  60.51754496525973\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 50/50 [02:45<00:00,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  298.7334140385501\n","Test Loss:  61.79407657403499\n","mae_validation:\n","0.9611291560030569\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:46,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  882.2323587983847\n","Test Loss:  148.6101588346064\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:44,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  616.565552694723\n","Test Loss:  107.56281547341496\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:41,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  512.5584239177406\n","Test Loss:  98.62602131813765\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:36,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  463.97120777890086\n","Test Loss:  91.94784737750888\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:32,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  444.2869997974485\n","Test Loss:  88.90774459764361\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:29,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  424.60946580953896\n","Test Loss:  83.80424833251163\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:26,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  402.41331387590617\n","Test Loss:  80.55342430528253\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:22,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  387.9290579846129\n","Test Loss:  91.71937102358788\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:19,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  370.25539838243276\n","Test Loss:  78.22866302542388\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:34<02:16,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  366.24734331388026\n","Test Loss:  78.92549670301378\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:12,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  365.05150289740413\n","Test Loss:  92.50441840849817\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:09,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  348.06963034998626\n","Test Loss:  74.07429712917656\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:44<02:05,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  352.7709846473299\n","Test Loss:  74.6015781769529\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:47<02:03,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  338.15039631165564\n","Test Loss:  72.39703646674752\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:51<01:59,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  340.83434663154185\n","Test Loss:  71.94073052285239\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:54<01:55,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  334.5560372788459\n","Test Loss:  69.9779088627547\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:57<01:51,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  335.6251738611609\n","Test Loss:  71.46296585071832\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:01<01:48,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  325.04278876818717\n","Test Loss:  68.8683686319273\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:04<01:45,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  327.1619049748406\n","Test Loss:  68.8141780118458\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:08<01:42,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  324.8583995627705\n","Test Loss:  67.20706874877214\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:11<01:39,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  317.3741811248474\n","Test Loss:  71.81507222936489\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:14<01:35,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  317.04767387453467\n","Test Loss:  73.4011939271586\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:18<01:31,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  314.62812846992165\n","Test Loss:  66.14042478124611\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:21<01:28,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  311.60330816265196\n","Test Loss:  69.09179428592324\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:25<01:24,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  312.670090847183\n","Test Loss:  67.05315628368407\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:28<01:21,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  306.81131322076544\n","Test Loss:  67.73579584527761\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:31<01:17,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  310.02922624442726\n","Test Loss:  67.36602911213413\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:35<01:14,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  304.8244702415541\n","Test Loss:  65.55673474352807\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:38<01:10,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  304.6296404157765\n","Test Loss:  65.11962756095454\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:41<01:07,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  303.56458710413426\n","Test Loss:  64.57439407054335\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:45<01:03,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  299.53022415656596\n","Test Loss:  67.48309499071911\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:48<01:00,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  303.7656239774078\n","Test Loss:  68.51910245558247\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:52<00:57,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  296.0700960299\n","Test Loss:  68.92958757001907\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:55<00:54,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  303.7992358771153\n","Test Loss:  65.5529041448608\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:58<00:51,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  301.4692489532754\n","Test Loss:  64.54834839515388\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:02<00:47,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  290.4016706724651\n","Test Loss:  72.90891368780285\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:05<00:44,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  298.71796501870267\n","Test Loss:  65.74238716857508\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:09<00:40,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  297.0156615576707\n","Test Loss:  62.983627539128065\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:12<00:37,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  296.7396746901795\n","Test Loss:  69.35462347604334\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:15<00:33,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  285.97466915892437\n","Test Loss:  64.77666494296864\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:19<00:30,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  292.70463805180043\n","Test Loss:  65.77119420329109\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:22<00:27,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  294.1097277114168\n","Test Loss:  63.77672309149057\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:26<00:27,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  298.58624482061714\n","Test Loss:  63.05504491645843\n","Early stopping with best_loss:  62.983627539128065 and test_loss for this epoch:  63.05504491645843 ...\n","mae_validation:\n","0.9859354638107215\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:49,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  790.7839197590947\n","Test Loss:  121.29446094855666\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:45,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  534.422646005638\n","Test Loss:  107.12568227574229\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:42,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  469.114453708753\n","Test Loss:  92.21758414246142\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:38,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  427.8303349679336\n","Test Loss:  83.70908950269222\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:34,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  402.8442203877494\n","Test Loss:  81.83425076678395\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:32,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  389.82439014455304\n","Test Loss:  87.42969856318086\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:28,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  376.98640093114227\n","Test Loss:  77.59047714155167\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:24,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  360.5705743818544\n","Test Loss:  74.12100365618244\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:27,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  354.01154925394803\n","Test Loss:  73.68229548633099\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:35<02:23,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  352.1142165665515\n","Test Loss:  85.4667372033\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:38<02:19,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  341.062196791172\n","Test Loss:  73.94181518629193\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:42<02:16,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  342.7223453130573\n","Test Loss:  71.16543046478182\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:45<02:12,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  333.28387631010264\n","Test Loss:  71.96141403866932\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:49<02:08,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  326.98804757744074\n","Test Loss:  69.62677758745849\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:52<02:04,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  327.21315170731395\n","Test Loss:  75.59800981823355\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:56<02:00,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  317.4495896464214\n","Test Loss:  73.88328024186194\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:00<01:58,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  319.95679919654503\n","Test Loss:  75.73527190182358\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:03<01:55,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  314.1649291180074\n","Test Loss:  70.65469869691879\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:07<01:59,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  313.13483641669154\n","Test Loss:  73.02444968093187\n","Early stopping with best_loss:  69.62677758745849 and test_loss for this epoch:  73.02444968093187 ...\n","mae_validation:\n","1.057032284273707\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:13,  3.95s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  739.9313726015389\n","Test Loss:  116.24436021689326\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:06,  3.88s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  495.0552690234035\n","Test Loss:  102.07207396347076\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<03:02,  3.89s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  437.79165165126324\n","Test Loss:  93.2268584528938\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<03:00,  3.92s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  398.24715607380494\n","Test Loss:  85.3160867774859\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:19<02:54,  3.88s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  380.4514969503507\n","Test Loss:  83.14735287800431\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:23<02:50,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  360.19304480124265\n","Test Loss:  78.11006962042302\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:27<02:45,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  356.0806899163872\n","Test Loss:  78.78348541911691\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:30<02:39,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  344.35751865198836\n","Test Loss:  77.90103358682245\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:34<02:35,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  332.8944335882552\n","Test Loss:  82.25812539551407\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:38<02:31,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  331.2813662784174\n","Test Loss:  75.50585477380082\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:42<02:26,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  323.83595360582694\n","Test Loss:  72.16982915159315\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:45<02:22,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  324.16479032021016\n","Test Loss:  71.47186914552003\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:49<02:19,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  307.7305198633112\n","Test Loss:  71.4180023027584\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:53<02:16,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  319.6402932717465\n","Test Loss:  72.14789624791592\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:57<02:12,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.91265863901936\n","Test Loss:  71.22011740645394\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [01:00<02:08,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.36391717894\n","Test Loss:  68.14481399022043\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:04<02:04,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  308.72110026166774\n","Test Loss:  67.1716513345018\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:08<02:00,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  299.07228163443506\n","Test Loss:  68.91622913163155\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:12<01:56,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  301.734905640129\n","Test Loss:  67.18102552788332\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:16<01:53,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  300.6820626524277\n","Test Loss:  65.52000945387408\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:19<01:48,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  297.0749664111063\n","Test Loss:  66.36895981431007\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:23<01:44,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  291.5716938832775\n","Test Loss:  65.95069096190855\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:27<01:40,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  289.80248709907755\n","Test Loss:  67.0493868868798\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:30<01:36,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  293.99347272887826\n","Test Loss:  67.89952062163502\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:34<01:42,  3.94s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  286.829984406475\n","Test Loss:  65.77111750934273\n","Early stopping with best_loss:  65.52000945387408 and test_loss for this epoch:  65.77111750934273 ...\n","mae_validation:\n","1.010498002629632\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:39,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  928.6076265498996\n","Test Loss:  138.34760620817542\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:36,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  631.8901161700487\n","Test Loss:  108.41180451400578\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:31,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  517.365274772048\n","Test Loss:  104.59212505351752\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:12<02:28,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  462.1254575345665\n","Test Loss:  95.00911175645888\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:26,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  442.0770620536059\n","Test Loss:  96.2584937531501\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:22,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  420.1595574077219\n","Test Loss:  95.56631904188544\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:22<02:19,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  404.7249107118696\n","Test Loss:  85.82310558715835\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:25<02:16,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  389.2946187183261\n","Test Loss:  84.25295853428543\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:29<02:13,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  385.713899907656\n","Test Loss:  87.24346480704844\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:32<02:11,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  374.595976036042\n","Test Loss:  81.16619620565325\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:35<02:08,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  362.9713057875633\n","Test Loss:  78.88787545077503\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:39<02:06,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  358.7824257770553\n","Test Loss:  77.72556600812823\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:42<02:03,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  351.56210830435157\n","Test Loss:  76.73295928398147\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:45<01:59,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  351.70185485389084\n","Test Loss:  74.9894561143592\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:49<01:55,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  343.10841786675155\n","Test Loss:  73.68952911393717\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:52<01:52,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  339.26457960437983\n","Test Loss:  75.83341557253152\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:55<01:48,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  336.0652551688254\n","Test Loss:  71.41892200009897\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [00:59<01:44,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  337.55077429674566\n","Test Loss:  71.63664450170472\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:02<01:41,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  336.80953027214855\n","Test Loss:  74.95768769644201\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:05<01:38,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  321.65203689644113\n","Test Loss:  68.91395298903808\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:08<01:34,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  322.62538687698543\n","Test Loss:  68.56898079905659\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:12<01:31,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  308.3951849322766\n","Test Loss:  68.19871711172163\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:15<01:28,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  315.7542175122071\n","Test Loss:  68.7317260815762\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:18<01:24,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  313.18780062999576\n","Test Loss:  67.67581656528637\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:21<01:21,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  312.8984192190692\n","Test Loss:  67.2072454597801\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:25<01:18,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  313.31798897869885\n","Test Loss:  71.48322726599872\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:28<01:15,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.9855201859027\n","Test Loss:  66.15849913470447\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:31<01:12,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  310.68982461467385\n","Test Loss:  65.84178121760488\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:35<01:09,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  301.7122848308645\n","Test Loss:  65.0153972459957\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:38<01:05,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  308.6747325276956\n","Test Loss:  64.45449200971052\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:41<01:03,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  303.68873046245426\n","Test Loss:  65.71127717848867\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:45<01:00,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  301.441708446946\n","Test Loss:  65.80782237602398\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:48<00:57,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  298.66053358558565\n","Test Loss:  63.95843700412661\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:52<00:54,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  299.8183909673244\n","Test Loss:  73.75315953278914\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:55<00:51,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  298.0754269948229\n","Test Loss:  66.18012454267591\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [01:59<00:48,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  297.6002522171475\n","Test Loss:  63.13844733638689\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:02<00:44,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  298.08930643927306\n","Test Loss:  66.58553259214386\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:05<00:40,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  293.19784317165613\n","Test Loss:  65.22851792909205\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:09<00:36,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  297.4806813406758\n","Test Loss:  63.97288359422237\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:12<00:33,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  298.36673954315484\n","Test Loss:  63.652490611188114\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:15<00:33,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  295.645653586369\n","Test Loss:  64.57176159135997\n","Early stopping with best_loss:  63.13844733638689 and test_loss for this epoch:  64.57176159135997 ...\n","mae_validation:\n","1.006360983666789\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:47,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  824.9271826483309\n","Test Loss:  143.67387571558356\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:42,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  554.1588000208139\n","Test Loss:  113.77903755754232\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:39,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  478.7486340813339\n","Test Loss:  101.83089121989906\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:38,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  438.0563524775207\n","Test Loss:  106.77007429394871\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:34,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  418.61300546117127\n","Test Loss:  91.54500626213849\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:32,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  388.19784006197006\n","Test Loss:  90.19830522872508\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:29,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  370.22637189226225\n","Test Loss:  86.01839880086482\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:25,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  360.4520354247652\n","Test Loss:  82.00655423942953\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:20,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  353.1396786784753\n","Test Loss:  85.8769766613841\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:34<02:17,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  353.52570553217083\n","Test Loss:  78.62134257284924\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:13,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  334.7309395018965\n","Test Loss:  76.11874561710283\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:41<02:10,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  338.28986630262807\n","Test Loss:  75.43403033958748\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:44<02:07,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  327.5788721065037\n","Test Loss:  74.48318311292678\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:48<02:03,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  320.56149387313053\n","Test Loss:  73.44427610840648\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:51<01:59,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  320.7576439441182\n","Test Loss:  73.61300953943282\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:54<01:54,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  319.3299440029077\n","Test Loss:  73.9183334549889\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:58<01:51,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  317.37910457421094\n","Test Loss:  77.01484058145434\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:01<01:48,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.7396660139784\n","Test Loss:  69.87212109705433\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:04<01:44,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  300.758589270059\n","Test Loss:  79.93153375387192\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:08<01:41,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  303.20796861033887\n","Test Loss:  70.60265644546598\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:11<01:37,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  299.33453208301216\n","Test Loss:  71.642848033458\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:15<01:34,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  303.75893246475607\n","Test Loss:  68.29975463170558\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:18<01:30,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  296.19658477790654\n","Test Loss:  74.89020389597863\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:21<01:27,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  297.35491785686463\n","Test Loss:  67.52560098608956\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:25<01:24,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  294.0129073862918\n","Test Loss:  75.40012306440622\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:28<01:20,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  296.2094666804187\n","Test Loss:  68.77404464036226\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:31<01:17,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  290.0679466710426\n","Test Loss:  65.4781539849937\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:35<01:13,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  291.45494986418635\n","Test Loss:  67.05145852640271\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:38<01:10,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  290.73147984268144\n","Test Loss:  68.86471377592534\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:41<01:07,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  286.290890368633\n","Test Loss:  68.7122105769813\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:45<01:04,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  285.4976286625024\n","Test Loss:  65.74790289439261\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:48<01:01,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  287.37372260540724\n","Test Loss:  65.38268798962235\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:52<00:57,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  287.2177775450982\n","Test Loss:  68.43109068693593\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:55<00:53,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  288.80942789744586\n","Test Loss:  65.55710308020934\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:58<00:50,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  283.1580092916265\n","Test Loss:  65.7660836041905\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:02<00:47,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  278.0690575055778\n","Test Loss:  69.77934191469103\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:05<00:44,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  281.07925573317334\n","Test Loss:  64.8899577120319\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:09<00:40,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  277.04964861646295\n","Test Loss:  68.59017992950976\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:12<00:37,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  279.9553412317764\n","Test Loss:  67.88840170297772\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:15<00:34,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  276.875566074159\n","Test Loss:  63.56224266765639\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:19<00:30,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  276.1130403780844\n","Test Loss:  70.39974966878071\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:22<00:27,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  276.07384900562465\n","Test Loss:  64.54711233451962\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:26<00:23,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  269.1464966619387\n","Test Loss:  65.45327527821064\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:29<00:20,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  273.2742711068131\n","Test Loss:  64.19955212948844\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:32<00:20,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  275.86620391579345\n","Test Loss:  65.88271673023701\n","Early stopping with best_loss:  63.56224266765639 and test_loss for this epoch:  65.88271673023701 ...\n","mae_validation:\n","1.0160057369624798\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:53,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  723.7924319654703\n","Test Loss:  118.19804001040757\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:48,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  496.5063536949456\n","Test Loss:  105.82648460939527\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:42,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  441.36015403456986\n","Test Loss:  93.43096461240202\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:38,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  402.7153267040849\n","Test Loss:  83.94146160269156\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:34,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  377.2461530622095\n","Test Loss:  80.42676090635359\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:30,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  370.57481001503766\n","Test Loss:  77.72380789415911\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:26,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  359.11766961123794\n","Test Loss:  76.11904301028699\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:22,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  345.4171393318102\n","Test Loss:  78.27096457127482\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:20,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  339.2278967620805\n","Test Loss:  73.36023567989469\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:34<02:16,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  327.54358033137396\n","Test Loss:  73.36681342730299\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:12,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  319.73332076426595\n","Test Loss:  74.03289554780349\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:41<02:09,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  316.33514087693766\n","Test Loss:  70.42248553503305\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:44<02:06,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  316.8548801820725\n","Test Loss:  69.84203423652798\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:47<02:02,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  315.28149741794914\n","Test Loss:  70.47314004669897\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:51<01:58,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  303.2784749311395\n","Test Loss:  72.39468137919903\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:54<01:55,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  300.785858056508\n","Test Loss:  69.89547271281481\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:58<01:52,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  300.0793582391925\n","Test Loss:  70.39676801906899\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:01<01:48,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  296.14792128885165\n","Test Loss:  68.21889333496802\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:04<01:44,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  294.356664378196\n","Test Loss:  70.8442067578435\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:08<01:42,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  292.8312578741461\n","Test Loss:  69.08772739954293\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:11<01:38,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  290.08075084816664\n","Test Loss:  67.28317986149341\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:15<01:34,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  287.37029775534756\n","Test Loss:  66.89723745547235\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:18<01:31,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  284.6078350218013\n","Test Loss:  67.06677862815559\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:21<01:28,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  279.5357415471226\n","Test Loss:  67.72728334832937\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:25<01:25,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  278.1990658636205\n","Test Loss:  64.78006274159998\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:28<01:21,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  282.3885429929942\n","Test Loss:  64.93372934823856\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:32<01:18,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  275.97704840963706\n","Test Loss:  68.89757723174989\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:35<01:14,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  279.02717201737687\n","Test Loss:  65.13379054516554\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:38<01:10,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  273.5504281502217\n","Test Loss:  64.22367645706981\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:42<01:07,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  270.36755505250767\n","Test Loss:  63.95940698310733\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:45<01:04,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  266.4778311410919\n","Test Loss:  66.2541927183047\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:48<01:00,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  273.10195543896407\n","Test Loss:  65.17042869329453\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:52<00:57,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  263.27780923992395\n","Test Loss:  63.03604163508862\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:55<00:53,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  270.82275068061426\n","Test Loss:  65.98640796449035\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:59<00:50,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  265.4018995105289\n","Test Loss:  65.22474393295124\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:02<00:47,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  267.7168687558733\n","Test Loss:  69.88281872170046\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:05<00:43,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  267.46975048584864\n","Test Loss:  62.915213893167675\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:09<00:40,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  260.47807188821025\n","Test Loss:  65.92969463672489\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:12<00:37,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  260.6402000486851\n","Test Loss:  63.66984294494614\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:16<00:34,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  258.67759457603097\n","Test Loss:  63.89727228973061\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:19<00:30,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  261.95741120167077\n","Test Loss:  65.1528527259361\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:22<00:27,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  262.7606613957323\n","Test Loss:  62.24704268574715\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:26<00:23,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  257.86120542604476\n","Test Loss:  62.04846432060003\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:29<00:20,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  261.3437694432214\n","Test Loss:  61.89252444403246\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:33<00:17,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  255.6631489410065\n","Test Loss:  60.989649969153106\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 92%|█████████▏| 46/50 [02:36<00:13,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  253.35924196941778\n","Test Loss:  61.55403984291479\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 94%|█████████▍| 47/50 [02:39<00:10,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  255.40278589259833\n","Test Loss:  62.29383779224008\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 96%|█████████▌| 48/50 [02:43<00:06,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  255.59481041878462\n","Test Loss:  63.16656866017729\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 49/50 [02:46<00:03,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  250.5109547157772\n","Test Loss:  61.58003923622891\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 49/50 [02:50<00:03,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  253.47221129806712\n","Test Loss:  62.00564411468804\n","Early stopping with best_loss:  60.989649969153106 and test_loss for this epoch:  62.00564411468804 ...\n","mae_validation:\n","0.9861527251897784\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:03,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  717.8538661822677\n","Test Loss:  111.60935887601227\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:01,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  477.4121666830033\n","Test Loss:  96.84617176745087\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:56,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  429.3990132929757\n","Test Loss:  91.85550957545638\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:52,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  388.50774595257826\n","Test Loss:  79.41960186860524\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:49,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  379.4068343550898\n","Test Loss:  77.86726362537593\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:44,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  354.95345131866634\n","Test Loss:  74.25542526785284\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:40,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  350.07316711498424\n","Test Loss:  73.64582138275728\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:29<02:37,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  338.3840519892983\n","Test Loss:  73.62336445506662\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:33<02:33,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  328.2231974164024\n","Test Loss:  72.31891937879845\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:37<02:28,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  327.5906545254402\n","Test Loss:  69.64103651884943\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:41<02:24,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  325.0132369832136\n","Test Loss:  68.34827770572156\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:44<02:20,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.2545587942004\n","Test Loss:  67.38217833545059\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:48<02:16,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  304.1978756338358\n","Test Loss:  73.38135174056515\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:52<02:12,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  307.2180797099136\n","Test Loss:  68.43037580419332\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:55<02:09,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  300.84787512756884\n","Test Loss:  65.49929459113628\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:59<02:05,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  296.70451824273914\n","Test Loss:  71.56966390274465\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:03<02:01,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  292.3925071917474\n","Test Loss:  64.50331445224583\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:06<01:58,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  285.60187693173066\n","Test Loss:  64.60518934391439\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:10<01:54,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  289.9030100735836\n","Test Loss:  68.36594687215984\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:14<01:50,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  282.2897985817399\n","Test Loss:  63.484896844252944\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:17<01:47,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  280.10942190350033\n","Test Loss:  64.41356061585248\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:21<01:43,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  280.8838729066774\n","Test Loss:  62.49342499487102\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:25<01:39,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  284.243156385608\n","Test Loss:  63.75624205172062\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:29<01:36,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  276.35859118402004\n","Test Loss:  62.455884056165814\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:32<01:32,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  276.8953385204077\n","Test Loss:  61.74842832190916\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:36<01:29,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  271.45544775621966\n","Test Loss:  61.50403071986511\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:40<01:25,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  267.6477597602643\n","Test Loss:  61.06277356296778\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:44<01:21,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  269.4033786901273\n","Test Loss:  60.88315203366801\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:47<01:18,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  266.0842392386403\n","Test Loss:  59.98487394396216\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:51<01:14,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  262.09945616452023\n","Test Loss:  60.42986702825874\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:55<01:10,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  265.5477258078754\n","Test Loss:  61.95752980560064\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:58<01:07,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  266.0075952094048\n","Test Loss:  65.56588321737945\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [02:02<01:03,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  259.2068572503049\n","Test Loss:  59.95494716754183\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [02:06<00:59,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  261.0452424287796\n","Test Loss:  58.74171037785709\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:10<00:56,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  253.53178855823353\n","Test Loss:  60.851443253224716\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:13<00:52,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  258.11375241493806\n","Test Loss:  59.04576292820275\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:17<00:48,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  256.47815666068345\n","Test Loss:  60.47860226687044\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:21<00:44,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  251.97327087959275\n","Test Loss:  63.084906646749005\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:25<00:45,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  256.28247740026563\n","Test Loss:  59.2284342488274\n","Early stopping with best_loss:  58.74171037785709 and test_loss for this epoch:  59.2284342488274 ...\n","mae_validation:\n","0.9589426142779554\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:34,  3.15s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  908.3621280565858\n","Test Loss:  147.76527131162584\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:32,  3.18s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  656.0956076346338\n","Test Loss:  120.04416966065764\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:29,  3.19s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  522.9375861547887\n","Test Loss:  100.41002477705479\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:12<02:26,  3.18s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  457.9286833656952\n","Test Loss:  93.79199955053627\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:15<02:22,  3.18s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  430.99759426061064\n","Test Loss:  90.40266317129135\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:20,  3.19s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  416.5433603171259\n","Test Loss:  92.03409278765321\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:22<02:17,  3.19s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  395.74470244022086\n","Test Loss:  85.59753718972206\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:25<02:13,  3.17s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  379.6700907566119\n","Test Loss:  84.61720317602158\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:28<02:09,  3.17s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  372.18703906424344\n","Test Loss:  83.66476519079879\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:31<02:08,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  368.60070950211957\n","Test Loss:  82.89038392715156\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:35<02:06,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  358.26271106116474\n","Test Loss:  80.03736799955368\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:38<02:03,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  352.45424183271825\n","Test Loss:  78.10073455469683\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:41<01:59,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  346.8789917009417\n","Test Loss:  78.82965715928003\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:44<01:55,  3.20s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  341.0820664111525\n","Test Loss:  76.72931785741821\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:47<01:51,  3.20s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  340.29490460269153\n","Test Loss:  79.18041541799903\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:51<01:47,  3.18s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  331.44470722274855\n","Test Loss:  79.03161904809531\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:54<01:44,  3.17s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  327.8590576443821\n","Test Loss:  74.70985374972224\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [00:57<01:41,  3.17s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  325.80575075373054\n","Test Loss:  73.50075405463576\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:00<01:38,  3.18s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  320.84904427966103\n","Test Loss:  73.95062437700108\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:03<01:34,  3.16s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  314.33709584362805\n","Test Loss:  72.12450779229403\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:06<01:31,  3.16s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  310.1787011511624\n","Test Loss:  70.69048744812608\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:10<01:28,  3.16s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  312.8058447865769\n","Test Loss:  73.27263195998967\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:13<01:25,  3.16s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  307.2730749649927\n","Test Loss:  69.99301213119179\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:16<01:21,  3.15s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  305.57890025526285\n","Test Loss:  70.41624659392983\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:19<01:19,  3.17s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.0620520852972\n","Test Loss:  69.69336307700723\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:22<01:15,  3.17s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  296.0652436190285\n","Test Loss:  69.82804365549237\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:25<01:12,  3.17s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  294.9612450785935\n","Test Loss:  71.0333750396967\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:29<01:09,  3.15s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  295.6092168632895\n","Test Loss:  68.11361566372216\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:32<01:06,  3.16s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  290.3319212333299\n","Test Loss:  70.28213969804347\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:35<01:02,  3.14s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  298.2337170736864\n","Test Loss:  68.67964085843414\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:38<00:59,  3.14s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  295.07035756111145\n","Test Loss:  68.46887484146282\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:41<00:56,  3.14s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  290.15677482262254\n","Test Loss:  68.66438655275851\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:44<00:58,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  290.75564853381366\n","Test Loss:  69.04774300660938\n","Early stopping with best_loss:  68.11361566372216 and test_loss for this epoch:  69.04774300660938 ...\n","mae_validation:\n","1.0750558957172123\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:38,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  812.5780343487859\n","Test Loss:  128.64846383966506\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:34,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  567.7713360637426\n","Test Loss:  115.2954738214612\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:32,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  482.30111071281135\n","Test Loss:  94.9761252310127\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:12<02:29,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  438.79952420294285\n","Test Loss:  91.31183955818415\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:26,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  415.4200110323727\n","Test Loss:  89.40217469446361\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:23,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  393.36056577507406\n","Test Loss:  86.70902935229242\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:22<02:20,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  385.6949889957905\n","Test Loss:  83.99010609695688\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:17,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  371.8153264988214\n","Test Loss:  79.06860551633872\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:29<02:13,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  353.87174811260775\n","Test Loss:  80.99982536863536\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:32<02:10,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  349.24388896441087\n","Test Loss:  76.23520016344264\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:35<02:07,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  340.4116919771768\n","Test Loss:  75.8598674684763\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:39<02:04,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  339.12703828001395\n","Test Loss:  72.96986924670637\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:42<02:00,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  326.1537490012124\n","Test Loss:  71.91380273271352\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:45<01:57,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  319.2958791712299\n","Test Loss:  73.08153737895191\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:48<01:53,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  318.35574229154736\n","Test Loss:  71.1332891676575\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:52<01:50,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  316.87620449857786\n","Test Loss:  71.97119616437703\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:55<01:48,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  312.08567940071225\n","Test Loss:  75.02512863371521\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [00:58<01:45,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.7026688414626\n","Test Loss:  69.98794747155625\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:01<01:41,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  297.3543759472668\n","Test Loss:  69.35967374918982\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:05<01:38,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  302.36408110521734\n","Test Loss:  72.74218264152296\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:08<01:34,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  293.43271840550005\n","Test Loss:  67.96306055458263\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:11<01:31,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  291.5248569403775\n","Test Loss:  74.31849813647568\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:14<01:27,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  291.4576876698993\n","Test Loss:  65.89883188111708\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:18<01:24,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  287.16992415254936\n","Test Loss:  67.62253857497126\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:21<01:21,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  293.14937877096236\n","Test Loss:  74.96011258289218\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:24<01:17,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  285.72663698904216\n","Test Loss:  66.10497535765171\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:27<01:14,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  281.7873813547194\n","Test Loss:  66.79955159965903\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:31<01:11,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  276.65461409604177\n","Test Loss:  64.93410391360521\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:34<01:08,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  287.30119490390643\n","Test Loss:  64.44149738084525\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:37<01:05,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  285.1562742097303\n","Test Loss:  64.12743972055614\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:40<01:01,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  278.1087156380527\n","Test Loss:  64.81242694985121\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:44<00:58,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  276.53089287132025\n","Test Loss:  64.7662986498326\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:47<00:55,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  273.3140398352407\n","Test Loss:  64.54255658667535\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:50<00:51,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  275.0191671815701\n","Test Loss:  63.59154324000701\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:53<00:48,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  275.48102164268494\n","Test Loss:  62.842492101714015\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [01:57<00:45,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  271.30211864225566\n","Test Loss:  63.76884014811367\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:00<00:42,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  269.90698302024975\n","Test Loss:  61.94927109731361\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:03<00:38,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  271.03750092489645\n","Test Loss:  64.6798529876396\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:06<00:35,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  268.99929454852827\n","Test Loss:  63.41606522537768\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:10<00:32,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  271.82402291195467\n","Test Loss:  63.56878975825384\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:13<00:29,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  269.1360132014379\n","Test Loss:  63.44191615679301\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:16<00:25,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  265.64864598959684\n","Test Loss:  59.510274137952365\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:19<00:22,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  263.92449218872935\n","Test Loss:  68.072090273723\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:23<00:19,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  266.44002744276077\n","Test Loss:  60.944141254760325\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:26<00:16,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  264.58000691700727\n","Test Loss:  62.26232308987528\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 92%|█████████▏| 46/50 [02:29<00:12,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  263.59624282550067\n","Test Loss:  61.06365868309513\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 92%|█████████▏| 46/50 [02:32<00:13,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  261.72235615644604\n","Test Loss:  61.39532236289233\n","Early stopping with best_loss:  59.510274137952365 and test_loss for this epoch:  61.39532236289233 ...\n","mae_validation:\n","0.9829598573721161\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:41,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  727.1030246913433\n","Test Loss:  124.79655949957669\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:36,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  498.9395724199712\n","Test Loss:  106.74483504425734\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:33,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  439.35259855538607\n","Test Loss:  94.41726614627987\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:31,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  401.5989230009727\n","Test Loss:  88.14410852082074\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:27,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  380.09892074018717\n","Test Loss:  103.7647039340809\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:23,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  372.43934663338587\n","Test Loss:  98.35097550600767\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:22<02:20,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  358.4605038301088\n","Test Loss:  82.74533934798092\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:17,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  335.7314101830125\n","Test Loss:  76.28197474265471\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:29<02:14,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  328.57737645320594\n","Test Loss:  75.05313845537603\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:32<02:11,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  325.9617617633194\n","Test Loss:  78.36139260232449\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:36<02:09,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  314.25533030182123\n","Test Loss:  73.38249677140266\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:39<02:05,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  303.3898882595822\n","Test Loss:  70.2435371009633\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:42<02:02,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  298.5431586401537\n","Test Loss:  69.91621345933527\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:45<01:58,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  301.47214584890753\n","Test Loss:  72.85277179069817\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:49<01:55,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  297.1225847331807\n","Test Loss:  67.45286295749247\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:52<01:52,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  291.4230427038856\n","Test Loss:  68.37045298190787\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:55<01:48,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  287.8813550500199\n","Test Loss:  68.02941305935383\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [00:59<01:44,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  284.8829583735205\n","Test Loss:  67.23111259751022\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:02<01:42,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  285.5483948616311\n","Test Loss:  65.8296879902482\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:05<01:38,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  275.3839775393717\n","Test Loss:  70.75848902855068\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:08<01:34,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  269.7128441943787\n","Test Loss:  64.12217754893936\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:12<01:32,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  278.26295077218674\n","Test Loss:  65.53968477249146\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:15<01:29,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  276.5322995397728\n","Test Loss:  63.95571327954531\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:19<01:26,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  272.0324660623446\n","Test Loss:  61.54513075761497\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:22<01:22,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  268.5194262890145\n","Test Loss:  63.37069409713149\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:25<01:19,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  261.59522472834215\n","Test Loss:  61.22975838277489\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:28<01:16,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  263.74880911037326\n","Test Loss:  64.28010357962921\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:32<01:12,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  258.8334031016566\n","Test Loss:  61.68322204705328\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:35<01:09,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  259.8217215947807\n","Test Loss:  62.23242549877614\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:38<01:06,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  259.5884749898687\n","Test Loss:  63.86560500692576\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:42<01:08,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  259.426711788401\n","Test Loss:  61.894522288115695\n","Early stopping with best_loss:  61.22975838277489 and test_loss for this epoch:  61.894522288115695 ...\n","mae_validation:\n","1.0036163661139512\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:59,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  691.914463727735\n","Test Loss:  118.183503174223\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:56,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  456.84515654854476\n","Test Loss:  103.69682883005589\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:54,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  405.8466048424598\n","Test Loss:  94.65817220881581\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:50,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  372.1518282308243\n","Test Loss:  86.23861710447818\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:45,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  351.6823159568012\n","Test Loss:  82.50480720680207\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:42,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  343.19024596363306\n","Test Loss:  82.62453209073283\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:38,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  331.0664984071627\n","Test Loss:  80.79538258071989\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:29<02:33,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  322.13852415792644\n","Test Loss:  82.84578755311668\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:33<02:30,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  307.51119889272377\n","Test Loss:  79.3863149615936\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:36<02:26,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  307.26179829379544\n","Test Loss:  73.14451190363616\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:40<02:22,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  303.63286652602255\n","Test Loss:  71.5878884345293\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:44<02:18,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  294.393852628069\n","Test Loss:  75.94239726988599\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:47<02:15,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  283.19404475204647\n","Test Loss:  70.82144262222573\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:51<02:11,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  288.73648817278445\n","Test Loss:  69.32541047502309\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:55<02:08,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  285.7766386345029\n","Test Loss:  78.01058115623891\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:58<02:04,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  269.60732724238187\n","Test Loss:  80.08739250991493\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:02<02:00,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  274.69290216919035\n","Test Loss:  72.03771863644943\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:05<01:56,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  270.5155699676834\n","Test Loss:  66.07212963234633\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:09<01:53,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  269.53023849613965\n","Test Loss:  72.22940502129495\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:13<01:49,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  259.7614687369205\n","Test Loss:  69.58275526505895\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:16<01:45,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  256.735359236598\n","Test Loss:  68.2338509275578\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:20<01:42,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  262.2689763326198\n","Test Loss:  65.00796069344506\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:24<01:38,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  258.7826627062168\n","Test Loss:  64.74128906521946\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:27<01:35,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  253.81010071467608\n","Test Loss:  68.80954478518106\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:31<01:31,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  253.09418121632189\n","Test Loss:  61.96986271091737\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:35<01:28,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  250.43802315741777\n","Test Loss:  63.47620480880141\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:38<01:24,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  248.04730236623436\n","Test Loss:  61.794278618879616\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:42<01:20,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  245.59384581586346\n","Test Loss:  63.17317864764482\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:46<01:17,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  243.74462512508035\n","Test Loss:  66.81555810570717\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:50<01:13,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  235.68197482195683\n","Test Loss:  66.2914052712731\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:53<01:10,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  242.79545142664574\n","Test Loss:  60.73041806742549\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:57<01:06,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  237.62795192282647\n","Test Loss:  62.95458808913827\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [02:01<01:02,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  238.6361972638406\n","Test Loss:  63.962703791446984\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [02:04<00:58,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  238.86099873762578\n","Test Loss:  61.67540901643224\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:08<00:54,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  238.0911258093547\n","Test Loss:  60.861829379573464\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:12<00:51,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  233.88969614589587\n","Test Loss:  60.55333056091331\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:15<00:47,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  231.3295149123296\n","Test Loss:  60.8489076141268\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:19<00:43,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  233.49858299526386\n","Test Loss:  65.83593941945583\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:22<00:40,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  231.91168670728803\n","Test Loss:  60.46222210954875\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:26<00:36,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  231.90116544580087\n","Test Loss:  60.45293585117906\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:30<00:32,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  222.65465062577277\n","Test Loss:  60.81497455807403\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:33<00:29,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  232.37604623101652\n","Test Loss:  63.26959694351535\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:37<00:25,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  225.00538368243724\n","Test Loss:  60.264462682418525\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:41<00:21,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  229.9825615300797\n","Test Loss:  60.59877936216071\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:44<00:18,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  226.7907347176224\n","Test Loss:  57.16369049670175\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 92%|█████████▏| 46/50 [02:48<00:14,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  224.5092834411189\n","Test Loss:  61.69518276583403\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 94%|█████████▍| 47/50 [02:52<00:10,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  224.46191329788417\n","Test Loss:  57.59377149492502\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 96%|█████████▌| 48/50 [02:55<00:07,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  217.34181843372062\n","Test Loss:  57.13675567694008\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 49/50 [02:59<00:03,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  224.3759250123985\n","Test Loss:  60.179861353710294\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 50/50 [03:03<00:00,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  217.48939034342766\n","Test Loss:  60.61744306050241\n","mae_validation:\n","0.9579337373471932\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:50,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  750.5938412640244\n","Test Loss:  99.34519696049392\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:50,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  501.66992960497737\n","Test Loss:  86.29550658259541\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:43,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  460.1761118345894\n","Test Loss:  82.81733970716596\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:38,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  410.30001102387905\n","Test Loss:  79.49578479118645\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:35,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  400.29919058596715\n","Test Loss:  79.04881308693439\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:32,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  379.2320556268096\n","Test Loss:  74.22805596329272\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:28,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  374.92039904836565\n","Test Loss:  73.87333494052291\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:25,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  359.66764063900337\n","Test Loss:  70.90688635781407\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:22,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  359.9486104249954\n","Test Loss:  76.94107837136835\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:34<02:19,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  346.99086974188685\n","Test Loss:  71.63436910090968\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:38<02:15,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  354.52644685003906\n","Test Loss:  70.67310278629884\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:41<02:12,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  345.698231340386\n","Test Loss:  70.32595071848482\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:45<02:08,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  336.3199827973731\n","Test Loss:  70.14082412980497\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:48<02:04,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  335.5373573405668\n","Test Loss:  68.46847507171333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:52<02:01,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  331.8074982492253\n","Test Loss:  77.16978929191828\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:55<01:59,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  335.4339371593669\n","Test Loss:  73.72261963412166\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:59<01:56,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  327.8696423433721\n","Test Loss:  69.83708419185132\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:02<01:53,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  329.3429853860289\n","Test Loss:  77.34303282108158\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:06<01:58,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  334.99371992843226\n","Test Loss:  68.7617900678888\n","Early stopping with best_loss:  68.46847507171333 and test_loss for this epoch:  68.7617900678888 ...\n","mae_validation:\n","1.032630417598338\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:58,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  674.6031910898164\n","Test Loss:  97.73155958019197\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:57,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  481.49274182971567\n","Test Loss:  85.9595043649897\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:51,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  415.8253158815205\n","Test Loss:  83.26123366691172\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:47,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  401.8556245099753\n","Test Loss:  80.87332766829059\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:41,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  379.17911390261725\n","Test Loss:  85.46385980583727\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:37,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  370.71129963733256\n","Test Loss:  77.6155994720757\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:34,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  362.7100805211812\n","Test Loss:  79.08090170565993\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:29,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  352.4883393163327\n","Test Loss:  76.98689402732998\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:32<02:25,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  347.9237113003619\n","Test Loss:  73.3030043784529\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:35<02:23,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  348.28554549627006\n","Test Loss:  72.60621999111027\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:39<02:19,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  342.81349839735776\n","Test Loss:  73.27180363610387\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:43<02:16,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  341.5021164622158\n","Test Loss:  78.27300003450364\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:46<02:12,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  347.661044341512\n","Test Loss:  70.62982926983386\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:50<02:08,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  338.90971944015473\n","Test Loss:  67.62424137303606\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:53<02:05,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  328.83524176478386\n","Test Loss:  68.85136207938194\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:57<02:01,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  329.4796097036451\n","Test Loss:  71.3537909053266\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:00<01:58,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  324.1067650562618\n","Test Loss:  73.99011625861749\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:04<01:54,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  322.3090694514103\n","Test Loss:  76.32370868884027\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:08<02:00,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  327.46560149476863\n","Test Loss:  69.9706762637943\n","Early stopping with best_loss:  67.62424137303606 and test_loss for this epoch:  69.9706762637943 ...\n","mae_validation:\n","1.0196351498706557\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:54,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  608.5939060891978\n","Test Loss:  101.6391089251265\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:52,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  446.75039709638804\n","Test Loss:  92.12799602001905\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:48,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  401.4679459920153\n","Test Loss:  86.79630150645971\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:45,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  378.81111351121217\n","Test Loss:  80.55106974719092\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:42,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  365.5210432866588\n","Test Loss:  102.64355482719839\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:37,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  356.08643893618137\n","Test Loss:  78.61760787153617\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:32,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  347.17322108801454\n","Test Loss:  76.6500162165612\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:29,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  343.490314766299\n","Test Loss:  90.62037709541619\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:32<02:26,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  333.3503541704267\n","Test Loss:  75.30099361389875\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:35<02:22,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  336.6161631643772\n","Test Loss:  77.60953513160348\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:39<02:18,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  331.37623820081353\n","Test Loss:  74.00172209087759\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:42<02:14,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  332.8276698626578\n","Test Loss:  75.33922603726387\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:46<02:11,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  329.87261105980724\n","Test Loss:  79.3673583152704\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:49<02:08,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  329.3888591332361\n","Test Loss:  72.42999564716592\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:53<02:04,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  320.1330951936543\n","Test Loss:  72.33231901563704\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:57<02:00,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  324.0608077580109\n","Test Loss:  83.00334054790437\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:00<01:56,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  320.427835714072\n","Test Loss:  73.04715303145349\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:04<01:53,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.31104448996484\n","Test Loss:  70.8615480652079\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:07<01:50,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  317.96760745649226\n","Test Loss:  71.40827409736812\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:11<01:47,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  319.4056062521413\n","Test Loss:  83.59811956435442\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:14<01:43,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  311.0819248603657\n","Test Loss:  78.37366878055036\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:18<01:39,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  309.36393680237234\n","Test Loss:  75.96049060858786\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:21<01:36,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  308.18451728951186\n","Test Loss:  68.99716944340616\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:25<01:33,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  312.14104048954323\n","Test Loss:  72.02506573311985\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:29<01:30,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  297.76474912650883\n","Test Loss:  75.36001394223422\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:32<01:26,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  309.54115670360625\n","Test Loss:  72.81608895212412\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:36<01:22,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  301.4835149401333\n","Test Loss:  70.469657195732\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:40<01:25,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  299.81139795761555\n","Test Loss:  70.58522624755278\n","Early stopping with best_loss:  68.99716944340616 and test_loss for this epoch:  70.58522624755278 ...\n","mae_validation:\n","1.0310576861701508\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:12,  3.93s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  602.6277581509203\n","Test Loss:  97.1644974425435\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:06,  3.88s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  432.53050581365824\n","Test Loss:  86.24542298028246\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<03:00,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  395.9177869837731\n","Test Loss:  85.38187078014016\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<02:55,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  384.8029716955498\n","Test Loss:  81.4205867126584\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:19<02:50,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  358.3126542037353\n","Test Loss:  86.13927442487329\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:46,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  355.22654767520726\n","Test Loss:  82.66271270625293\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:43,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  353.9583172565326\n","Test Loss:  78.16838968871161\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:30<02:39,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  344.1999744698405\n","Test Loss:  88.25689031369984\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:34<02:34,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  338.7326767919585\n","Test Loss:  77.99527757707983\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:38<02:31,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  342.4102542633191\n","Test Loss:  86.96126422099769\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:41<02:27,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  335.3241413026117\n","Test Loss:  77.34255907032639\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:45<02:23,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  335.34632941987365\n","Test Loss:  72.88403309043497\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:49<02:19,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  325.6876376606524\n","Test Loss:  77.37867551948875\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:53<02:15,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  323.8322164574638\n","Test Loss:  73.44213427836075\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:56<02:11,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  323.6583043737337\n","Test Loss:  71.43176467902958\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [01:00<02:08,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  326.82577867712826\n","Test Loss:  78.17219318076968\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:04<02:04,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  317.94662699010223\n","Test Loss:  77.70845787040889\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:08<02:00,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  322.84570991201326\n","Test Loss:  71.9647361394018\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:11<01:56,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  321.7651505069807\n","Test Loss:  73.75681112054735\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:15<01:53,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.98612918378785\n","Test Loss:  70.46377206360921\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:19<01:49,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  316.9449524828233\n","Test Loss:  72.24641780834645\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:23<01:45,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  315.96843397896737\n","Test Loss:  73.67502414714545\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:27<01:42,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  317.0762322023511\n","Test Loss:  70.1279987427406\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:30<01:38,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  310.24059703107923\n","Test Loss:  69.42355085443705\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:34<01:34,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  308.53312063682824\n","Test Loss:  69.68095440417528\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:38<01:30,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.4478495782241\n","Test Loss:  67.95655953651294\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:42<01:26,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  310.44260243233293\n","Test Loss:  68.02524403389543\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:45<01:22,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  307.1162007385865\n","Test Loss:  68.26074439939111\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:49<01:19,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  301.2866152655333\n","Test Loss:  67.61262854095548\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:53<01:15,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  301.39913085475564\n","Test Loss:  73.85617764666677\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:57<01:11,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.25637707673013\n","Test Loss:  65.23166341427714\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [02:01<01:08,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  303.5018127821386\n","Test Loss:  67.82665505842306\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [02:04<01:04,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  299.0070410296321\n","Test Loss:  68.97892696969211\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [02:08<01:00,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  299.7950105322525\n","Test Loss:  69.07360144145787\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:12<00:56,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  297.2123386412859\n","Test Loss:  65.44029582850635\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:16<00:58,  3.89s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  300.7071584938094\n","Test Loss:  67.77110786642879\n","Early stopping with best_loss:  65.23166341427714 and test_loss for this epoch:  67.77110786642879 ...\n","mae_validation:\n","0.9840434242242623\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:40,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  732.8167637102306\n","Test Loss:  122.28294595889747\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:39,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  469.1552377101034\n","Test Loss:  85.69301484059542\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:39,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  426.7923435261473\n","Test Loss:  75.763687197119\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:37,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  380.63314251229167\n","Test Loss:  74.21280660107732\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:34,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  373.45398840121925\n","Test Loss:  68.67350506875664\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:30,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  358.453484156169\n","Test Loss:  70.1363069973886\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:26,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  356.6151207042858\n","Test Loss:  69.91260352078825\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:21,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  347.6085232151672\n","Test Loss:  71.66643447987735\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:18,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  347.5766878062859\n","Test Loss:  72.10961918858811\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:33<02:34,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  344.9365922990255\n","Test Loss:  69.02567013073713\n","Early stopping with best_loss:  68.67350506875664 and test_loss for this epoch:  69.02567013073713 ...\n","mae_validation:\n","1.0520411176351974\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:50,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  674.367875631433\n","Test Loss:  114.4988855868578\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:44,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  465.0581470588222\n","Test Loss:  87.90813723672181\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:40,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  408.68707574065775\n","Test Loss:  90.48589518293738\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:38,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  385.4598810216412\n","Test Loss:  78.23782341671176\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:36,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  364.9618996675126\n","Test Loss:  75.10715802479535\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:32,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  350.217065946199\n","Test Loss:  71.70574221294373\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:29,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  341.0322366808541\n","Test Loss:  71.77105420548469\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:26,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  334.5118071241304\n","Test Loss:  71.42865889519453\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:22,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  336.5643438170664\n","Test Loss:  81.32156918384135\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:34<02:19,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  334.39377181231976\n","Test Loss:  72.65204311069101\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:38<02:16,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  332.57732344418764\n","Test Loss:  78.17038240656257\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:41<02:12,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  328.19764657272026\n","Test Loss:  71.87357216887176\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:45<02:22,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  319.90798227116466\n","Test Loss:  74.61153989564627\n","Early stopping with best_loss:  71.42865889519453 and test_loss for this epoch:  74.61153989564627 ...\n","mae_validation:\n","1.0812925126655306\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:53,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  623.190955426544\n","Test Loss:  103.85801188461483\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:47,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  438.9664683714509\n","Test Loss:  91.4211801821366\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:44,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  386.32718260306865\n","Test Loss:  83.13981279078871\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:40,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  375.5375550216995\n","Test Loss:  74.35216841287911\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:38,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  356.50931361177936\n","Test Loss:  71.52914565242827\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:34,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  357.423457428813\n","Test Loss:  73.83328507374972\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:30,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  335.12212223466486\n","Test Loss:  82.59551069233567\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:27,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  332.2608175948262\n","Test Loss:  79.34888615086675\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:24,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  332.91865643300116\n","Test Loss:  70.05417594406754\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:35<02:19,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  329.3128144107759\n","Test Loss:  72.18817391060293\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:38<02:15,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  316.2678221273236\n","Test Loss:  70.2870219796896\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:42<02:12,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  315.78961877617985\n","Test Loss:  66.5980750983581\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:45<02:09,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  316.4519930598326\n","Test Loss:  69.21506320405751\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:49<02:06,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  319.14190034382045\n","Test Loss:  69.83900199597701\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:52<02:03,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  314.45431876182556\n","Test Loss:  68.68355903215706\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:56<01:59,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.36464018467814\n","Test Loss:  65.45977067761123\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:59<01:56,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  311.07357633067295\n","Test Loss:  71.42397024389356\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:03<01:53,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  307.84548802580684\n","Test Loss:  66.73945713695139\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:06<01:49,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  300.2478136457503\n","Test Loss:  64.71270701382309\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:10<01:46,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  301.3023595060222\n","Test Loss:  65.4858466386795\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:13<01:42,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  296.69472905015573\n","Test Loss:  65.16610724572092\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:17<01:38,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  301.5002201953903\n","Test Loss:  64.9094514939934\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:20<01:34,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  296.0755483210087\n","Test Loss:  64.13795540994033\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:24<01:31,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  296.74880962539464\n","Test Loss:  63.493825094774365\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:27<01:28,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  293.97817383985966\n","Test Loss:  66.63037383928895\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:31<01:25,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  295.39168252423406\n","Test Loss:  67.17845516372472\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:35<01:22,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  297.31870298134163\n","Test Loss:  62.91177839366719\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:38<01:19,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  289.4432042995468\n","Test Loss:  63.34274616651237\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:42<01:16,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  287.5615716157481\n","Test Loss:  67.22030088445172\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:46<01:12,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  278.3641626024619\n","Test Loss:  63.83183504175395\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:49<01:07,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  283.65923865744844\n","Test Loss:  63.49543166067451\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:53<01:03,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  275.94846597732976\n","Test Loss:  61.13502742629498\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:56<00:59,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  282.26437507756054\n","Test Loss:  66.32971900142729\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:59<00:55,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  284.94972484745085\n","Test Loss:  63.03640268370509\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:03<00:52,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  283.22263722959906\n","Test Loss:  60.81258109305054\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:06<00:48,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  276.7862668808084\n","Test Loss:  61.456500611500815\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:10<00:45,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  276.3017706410028\n","Test Loss:  63.855740945087746\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:13<00:41,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  276.7638274433557\n","Test Loss:  66.55536725278944\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:17<00:38,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  279.3808930143714\n","Test Loss:  62.95152276917361\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:20<00:34,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  270.2015673108399\n","Test Loss:  60.289161757566035\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:24<00:30,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  275.54958430770785\n","Test Loss:  75.31609515473247\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:27<00:27,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  273.97778859268874\n","Test Loss:  60.820418970193714\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:30<00:24,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  271.1818612674251\n","Test Loss:  60.15142520330846\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:34<00:20,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  270.57040075119585\n","Test Loss:  62.25094759836793\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:37<00:17,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  272.1100578275509\n","Test Loss:  65.84435682650656\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 92%|█████████▏| 46/50 [02:41<00:13,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  275.9019960798323\n","Test Loss:  71.28581285849214\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 94%|█████████▍| 47/50 [02:44<00:10,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  269.78764822147787\n","Test Loss:  60.41297191940248\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 96%|█████████▌| 48/50 [02:48<00:06,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  274.5062874741852\n","Test Loss:  59.6473281532526\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 49/50 [02:51<00:03,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  272.03715022746474\n","Test Loss:  65.12217297963798\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 50/50 [02:54<00:00,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  268.3567944755778\n","Test Loss:  61.626188112422824\n","mae_validation:\n","0.9573019516937089\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:01,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  573.4689089749008\n","Test Loss:  95.07359071262181\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:59,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  422.1791601544246\n","Test Loss:  87.6575342323631\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:56,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  381.6463734507561\n","Test Loss:  79.6919185500592\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:51,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  360.83757787104696\n","Test Loss:  76.84664576221257\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:47,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  351.1218814216554\n","Test Loss:  78.0399672575295\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:45,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  347.72456053318456\n","Test Loss:  75.45466500334442\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:41,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  334.0200925373938\n","Test Loss:  73.74398401007056\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:29<02:36,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  333.5654885470867\n","Test Loss:  81.07997443433851\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:33<02:33,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  327.979663209524\n","Test Loss:  74.04166734684259\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:37<02:30,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  327.0309550142847\n","Test Loss:  71.294059664011\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:41<02:26,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  323.26818813290447\n","Test Loss:  70.74850735673681\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:44<02:22,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  323.1824143626727\n","Test Loss:  73.6165502872318\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:48<02:19,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  315.5481071770191\n","Test Loss:  68.93854939937592\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:52<02:14,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  315.84977383259684\n","Test Loss:  67.79716406576335\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:56<02:10,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  305.28708689729683\n","Test Loss:  66.78146789316088\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:59<02:07,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  306.0933320997283\n","Test Loss:  71.73081331700087\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:03<02:02,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  307.19403561903164\n","Test Loss:  69.30840224213898\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:07<01:59,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  303.5485225971788\n","Test Loss:  71.86812440864742\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:11<01:56,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  300.809116166085\n","Test Loss:  77.71846506372094\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:14<02:02,  3.94s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  299.7522287396714\n","Test Loss:  67.38932049041614\n","Early stopping with best_loss:  66.78146789316088 and test_loss for this epoch:  67.38932049041614 ...\n","mae_validation:\n","1.0318971152468588\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:42,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  663.2411287967116\n","Test Loss:  106.31858176365495\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:41,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  452.9121183352545\n","Test Loss:  119.21747982595116\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:43,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  401.50051884073764\n","Test Loss:  87.52313634031452\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:37,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  365.6826146445237\n","Test Loss:  86.09586038766429\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:31,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  352.7234129952267\n","Test Loss:  83.21287002321333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:27,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  345.6943424907513\n","Test Loss:  84.67800248600543\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:23,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  336.4328710148111\n","Test Loss:  80.16892279218882\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:19,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  329.8190397294238\n","Test Loss:  99.1041743112728\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:15,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  315.46328378096223\n","Test Loss:  83.9190722424537\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:11,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  314.94175851065665\n","Test Loss:  80.27216993644834\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:36<02:07,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  322.7181770913303\n","Test Loss:  81.14194716326892\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:39<02:03,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  307.97133591677994\n","Test Loss:  75.88703448744491\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:00,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  312.99712116550654\n","Test Loss:  75.78032613219693\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:46<01:58,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  308.3991837785579\n","Test Loss:  75.16004312224686\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:49<01:55,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  304.28860653052106\n","Test Loss:  71.34123235149309\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:53<01:51,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  307.7766935620457\n","Test Loss:  77.11741394456476\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:56<01:48,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  302.2537195337936\n","Test Loss:  93.24437926709652\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [00:59<01:45,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  306.33014656137675\n","Test Loss:  74.80674581415951\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:02<01:41,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  296.0490124486387\n","Test Loss:  71.9826394484844\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:06<01:47,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  294.48460124013945\n","Test Loss:  77.44357801787555\n","Early stopping with best_loss:  71.34123235149309 and test_loss for this epoch:  77.44357801787555 ...\n","mae_validation:\n","1.0916603100805313\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:51,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  629.1659219115973\n","Test Loss:  95.10429392382503\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:45,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  434.899395596236\n","Test Loss:  82.87905480898917\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:39,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  379.0825486155227\n","Test Loss:  96.1711424747482\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:35,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  359.60662335250527\n","Test Loss:  75.24184652790427\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:31,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  345.1082405121997\n","Test Loss:  70.76183324027807\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:28,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  346.7486980156973\n","Test Loss:  76.16900028847158\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:25,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  331.8080297964625\n","Test Loss:  75.98713036067784\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:21,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  325.690932857804\n","Test Loss:  70.25498828664422\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:16,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  319.27241269964725\n","Test Loss:  67.61737965326756\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:13,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  312.3058713329956\n","Test Loss:  71.69047464150935\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:10,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  317.5125893484801\n","Test Loss:  72.15019871015102\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:08,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  312.8283539307304\n","Test Loss:  64.64302381128073\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:04,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  302.8991277338937\n","Test Loss:  66.73203850071877\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:47<02:00,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  307.5883644875139\n","Test Loss:  64.54121137270704\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:50<01:58,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  301.39812005124986\n","Test Loss:  71.77163577871397\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:54<01:56,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  297.1680982634425\n","Test Loss:  65.82467992696911\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:57<01:53,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  293.7724242117256\n","Test Loss:  70.93497086456046\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:01<01:49,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  292.48224486224353\n","Test Loss:  68.97717971215025\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:04<01:46,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  284.0895162075758\n","Test Loss:  60.87862641829997\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:07<01:42,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  291.9374936730601\n","Test Loss:  61.3769460413605\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:11<01:38,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  292.75097861047834\n","Test Loss:  63.474018449895084\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:14<01:35,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  291.1916415495798\n","Test Loss:  60.83730329107493\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:17<01:31,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  286.5683873128146\n","Test Loss:  60.731102644465864\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:21<01:28,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  285.6745716347359\n","Test Loss:  59.37507436238229\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:24<01:24,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  276.5566831231117\n","Test Loss:  61.584707600064576\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:28<01:21,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  278.74858199153095\n","Test Loss:  65.76573187019676\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:31<01:18,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  279.69017352722585\n","Test Loss:  64.14285711292177\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:34<01:14,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  281.31824333546683\n","Test Loss:  59.96656395774335\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:38<01:11,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  275.8042683005333\n","Test Loss:  57.470014441758394\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:41<01:07,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  273.7973933129106\n","Test Loss:  61.74691032851115\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:45<01:04,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  268.67632338264957\n","Test Loss:  59.411202360875905\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:48<01:01,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  266.196953532286\n","Test Loss:  60.122261399403214\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:52<00:58,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  271.3067409088835\n","Test Loss:  57.28824344649911\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:55<00:54,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  270.0866915346123\n","Test Loss:  61.380597522482276\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:58<00:51,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  262.726043431554\n","Test Loss:  58.668436331208795\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:02<00:47,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  266.5837775673717\n","Test Loss:  58.531356510706246\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:05<00:43,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  263.11760117392987\n","Test Loss:  57.75574078690261\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:08<00:45,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  256.8397667496465\n","Test Loss:  60.975861640647054\n","Early stopping with best_loss:  57.28824344649911 and test_loss for this epoch:  60.975861640647054 ...\n","mae_validation:\n","0.9449400279450428\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:44,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  602.4630269873887\n","Test Loss:  93.89561408013105\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:42,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  428.36810771748424\n","Test Loss:  85.96252603456378\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:39,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  380.5232287533581\n","Test Loss:  88.40341889485717\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:36,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  349.0620843428187\n","Test Loss:  74.0555525533855\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:32,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  340.5596319530159\n","Test Loss:  79.49397507635877\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:28,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  337.09875622438267\n","Test Loss:  72.0130947753787\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:25,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  321.0040144296363\n","Test Loss:  77.81082948297262\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:22,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  319.1139745460823\n","Test Loss:  69.92087199399248\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:19,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  318.474593098741\n","Test Loss:  76.38729936629534\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:15,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  310.52666803775355\n","Test Loss:  67.9171901633963\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:11,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  308.0516417576\n","Test Loss:  69.36417919956148\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:08,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  305.458497219719\n","Test Loss:  68.42660516686738\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:04,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  299.4113071002066\n","Test Loss:  71.29201336670667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:47<02:01,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  302.5772233470343\n","Test Loss:  67.05446711555123\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:50<01:58,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  297.43986839801073\n","Test Loss:  71.30151265021414\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:54<01:55,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  289.9078927570954\n","Test Loss:  67.87909198179841\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:57<01:51,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  292.5078961441759\n","Test Loss:  69.90267188474536\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:00<01:47,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  294.9347303668037\n","Test Loss:  65.90226155566052\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:04<01:44,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  290.6105692414567\n","Test Loss:  69.47774062678218\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:07<01:42,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  286.0779972858727\n","Test Loss:  62.269057027529925\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:11<01:38,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  279.35056642629206\n","Test Loss:  63.46836580336094\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:14<01:35,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  284.3042371608317\n","Test Loss:  64.48468229314312\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:17<01:31,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  280.50227182917297\n","Test Loss:  66.27609730046242\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:21<01:28,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  280.0698060807772\n","Test Loss:  67.51841860078275\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:24<01:31,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  275.2330490127206\n","Test Loss:  68.99152322392911\n","Early stopping with best_loss:  62.269057027529925 and test_loss for this epoch:  68.99152322392911 ...\n","mae_validation:\n","1.0004040715161362\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:07,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  556.2575048226863\n","Test Loss:  81.72888381965458\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:05,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  411.67506009712815\n","Test Loss:  74.75896386010572\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:58,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  374.09239862672985\n","Test Loss:  66.72722356021404\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<02:52,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  359.48744728043675\n","Test Loss:  80.6206406801939\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:48,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  343.0921260425821\n","Test Loss:  65.74857225152664\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:45,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  339.6940841060132\n","Test Loss:  78.72645192407072\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:40,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  332.37215300416574\n","Test Loss:  66.13555518351495\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:30<02:36,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  326.069587726146\n","Test Loss:  66.83695937599987\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:33<02:32,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  326.65437523275614\n","Test Loss:  68.35705571062863\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:37<02:28,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  319.32248790282756\n","Test Loss:  64.28908375231549\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:41<02:25,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  317.9597009648569\n","Test Loss:  60.8662702026777\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:44<02:22,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  317.805337282829\n","Test Loss:  58.455431405687705\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:48<02:17,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  307.64380270661786\n","Test Loss:  71.67375395121053\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:52<02:13,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  308.62241098098457\n","Test Loss:  56.979582010768354\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:56<02:10,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  308.22871606005356\n","Test Loss:  57.61344009498134\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:59<02:06,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  296.0136514962651\n","Test Loss:  62.50460838060826\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:03<02:04,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  295.4222814356908\n","Test Loss:  55.49926656624302\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:07<02:01,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  300.2307102503255\n","Test Loss:  59.23908744286746\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:11<01:58,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  298.9197457237169\n","Test Loss:  58.103510685265064\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:15<01:53,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  301.1163502810523\n","Test Loss:  64.83702342025936\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:18<01:49,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  295.84347945358604\n","Test Loss:  60.60818378534168\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:22<01:54,  3.93s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  292.60287475120276\n","Test Loss:  67.306904328987\n","Early stopping with best_loss:  55.49926656624302 and test_loss for this epoch:  67.306904328987 ...\n","mae_validation:\n","0.9377698104378182\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:36,  3.19s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  636.0105932727456\n","Test Loss:  87.45823717676103\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:31,  3.15s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  432.2852071849629\n","Test Loss:  89.47872689086944\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:28,  3.15s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  397.7129412405193\n","Test Loss:  80.16093695908785\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:12<02:25,  3.16s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  376.56164792831987\n","Test Loss:  71.83227393217385\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:15<02:22,  3.16s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  363.24297704105265\n","Test Loss:  70.20074988761917\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:18<02:19,  3.17s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  343.56134513253346\n","Test Loss:  65.04257683432661\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:22<02:16,  3.18s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  337.70018711173907\n","Test Loss:  74.47698784992099\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:25<02:12,  3.16s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  333.3625362785533\n","Test Loss:  69.56114084785804\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:28<02:09,  3.16s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  335.7689543347806\n","Test Loss:  63.69327416270971\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:31<02:06,  3.15s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  321.2280848585069\n","Test Loss:  64.87188066076487\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:34<02:03,  3.16s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  323.08074535336345\n","Test Loss:  64.4206690536812\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:37<02:00,  3.16s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.57926255464554\n","Test Loss:  60.69760084198788\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:41<01:56,  3.15s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  312.21891095582396\n","Test Loss:  68.68005265109241\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:44<01:54,  3.17s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  313.00237296056\n","Test Loss:  64.0254559989553\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:47<01:50,  3.15s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  307.03162811324\n","Test Loss:  62.46681101433933\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:50<01:46,  3.14s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  304.79904725402594\n","Test Loss:  67.35416501481086\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:53<01:54,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  299.0923732062802\n","Test Loss:  65.46816215012223\n","Early stopping with best_loss:  60.69760084198788 and test_loss for this epoch:  65.46816215012223 ...\n","mae_validation:\n","1.010061577459976\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:40,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  636.6233310997486\n","Test Loss:  88.4559907540679\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:34,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  448.55520968604833\n","Test Loss:  84.75916783884168\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:32,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  390.1918983189389\n","Test Loss:  78.92045059520751\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:12<02:29,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  372.65399008989334\n","Test Loss:  70.35259244497865\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:26,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  349.12690977100283\n","Test Loss:  70.71747051365674\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:22,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  332.86863548262045\n","Test Loss:  73.7965607251972\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:22<02:19,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  326.6770476978272\n","Test Loss:  67.4703300986439\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:25<02:16,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  327.8958239853382\n","Test Loss:  63.34394291136414\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:29<02:12,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.31484949728474\n","Test Loss:  60.80433060415089\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:32<02:09,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  313.1351311611943\n","Test Loss:  68.89524405868724\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:35<02:06,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  302.20146258268505\n","Test Loss:  66.66518081352115\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:39<02:05,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  295.928655940108\n","Test Loss:  59.67594331223518\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:42<02:02,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  287.7819363651797\n","Test Loss:  60.6559023540467\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:45<01:58,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  289.8363724402152\n","Test Loss:  57.79361779522151\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:48<01:55,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  286.8033228851855\n","Test Loss:  67.86943352082744\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:52<01:51,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  289.1911840867251\n","Test Loss:  63.15077570267022\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:55<01:49,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  284.8473494788632\n","Test Loss:  65.36790626542643\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [00:59<01:46,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  285.7113247709349\n","Test Loss:  63.94828876387328\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:02<01:50,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  280.99987732060254\n","Test Loss:  64.09735813271254\n","Early stopping with best_loss:  57.79361779522151 and test_loss for this epoch:  64.09735813271254 ...\n","mae_validation:\n","0.971255933658545\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:38,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  598.8154558083043\n","Test Loss:  82.33108390774578\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:37,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  413.79387190937996\n","Test Loss:  75.71182790957391\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:34,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  371.8507271707058\n","Test Loss:  69.83551440387964\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:31,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  353.6904619289562\n","Test Loss:  84.03232999658212\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:28,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  343.63830517744645\n","Test Loss:  68.43193481862545\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:24,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  333.58617403078824\n","Test Loss:  58.89997605280951\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:22<02:20,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  317.4077261420898\n","Test Loss:  65.85878257080913\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:17,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  312.292757647112\n","Test Loss:  60.11270076781511\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:29<02:15,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  309.96300883125514\n","Test Loss:  64.30036998121068\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:15,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  301.5987500734627\n","Test Loss:  59.42389752436429\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:36<02:12,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  295.2587688593194\n","Test Loss:  56.591723922407255\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:08,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  303.5560002233833\n","Test Loss:  59.00083123287186\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:04,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  296.5426031695679\n","Test Loss:  60.27260172739625\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:46<02:00,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  298.3321793419309\n","Test Loss:  64.51607679389417\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:49<01:56,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  292.6165455272421\n","Test Loss:  69.54238923452795\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:53<02:04,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  285.99794510472566\n","Test Loss:  58.86233622953296\n","Early stopping with best_loss:  56.591723922407255 and test_loss for this epoch:  58.86233622953296 ...\n","mae_validation:\n","0.9533782065192657\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:59,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  530.467250905931\n","Test Loss:  85.69255361054093\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:56,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  382.1397113930434\n","Test Loss:  80.40663541387767\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:52,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  355.25977587653324\n","Test Loss:  90.13125553075224\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:50,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  341.304256381467\n","Test Loss:  79.18817435204983\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:47,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  330.09368955902755\n","Test Loss:  81.49836127646267\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:44,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  312.48885020427406\n","Test Loss:  70.65511683886871\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:40,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  310.00842231698334\n","Test Loss:  68.03285146225244\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:29<02:35,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  305.87275461200625\n","Test Loss:  72.27049655094743\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:33<02:31,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  298.1260934062302\n","Test Loss:  68.4696568766376\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:36<02:27,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  298.35741280578077\n","Test Loss:  69.42333173239604\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:40<02:24,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  296.49843199411407\n","Test Loss:  78.50053958874196\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:44<02:19,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  289.1925171976909\n","Test Loss:  62.96842273231596\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:48<02:15,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  283.6450631804764\n","Test Loss:  64.4649500688538\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:51<02:11,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  286.1886391621083\n","Test Loss:  80.045962177217\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:55<02:08,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  288.6790631217882\n","Test Loss:  67.46254398394376\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:58<02:04,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  283.0542894778773\n","Test Loss:  66.03452637046576\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [01:02<02:12,  3.91s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  279.1133340978995\n","Test Loss:  65.32093675713986\n","Early stopping with best_loss:  62.96842273231596 and test_loss for this epoch:  65.32093675713986 ...\n","mae_validation:\n","1.0251007079673904\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:47,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  1019.6110954582691\n","Test Loss:  152.1800799369812\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:44,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  700.4212508834898\n","Test Loss:  122.28116693720222\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:41,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  571.6262252815068\n","Test Loss:  108.26051063649356\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:37,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  529.9397942256182\n","Test Loss:  100.41623032465577\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:34,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  485.2859446378425\n","Test Loss:  93.66246569342911\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:30,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  468.3248607739806\n","Test Loss:  97.88307857140899\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:26,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  447.4823217028752\n","Test Loss:  85.66309340950102\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:23,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  434.9733817791566\n","Test Loss:  85.37615531822667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:20,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  424.83147356752306\n","Test Loss:  83.194583972916\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:34<02:16,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  415.0807914887555\n","Test Loss:  84.61808892246336\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:12,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  409.35562904458493\n","Test Loss:  80.77573913056403\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:41<02:10,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  397.2584203230217\n","Test Loss:  80.00547099439427\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:44<02:06,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  392.89182811416686\n","Test Loss:  78.53894954174757\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:47<02:02,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  384.83276815200225\n","Test Loss:  75.81028796592727\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:51<01:59,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  380.5184088796377\n","Test Loss:  76.19127260893583\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:54<01:55,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  369.53567649330944\n","Test Loss:  77.06775245629251\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:58<01:52,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  372.21170945465565\n","Test Loss:  81.36441681906581\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:01<01:49,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  368.91397229675204\n","Test Loss:  73.76988694630563\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:05<01:46,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  360.6080279648304\n","Test Loss:  72.62290503503755\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:08<01:42,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  357.804112249054\n","Test Loss:  72.86451158113778\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:11<01:39,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  358.2763365106657\n","Test Loss:  73.15139253437519\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:15<01:35,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  348.6083547351882\n","Test Loss:  73.41881230380386\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:18<01:33,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  350.17218554299325\n","Test Loss:  70.58053807914257\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:22<01:30,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  347.6881561698392\n","Test Loss:  73.33462668955326\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:25<01:26,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  342.39908922370523\n","Test Loss:  70.34536960395053\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:29<01:25,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  335.2281100549735\n","Test Loss:  69.83661022502929\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:32<01:21,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  341.0929920626804\n","Test Loss:  77.27359535172582\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:36<01:16,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  343.04540948104113\n","Test Loss:  69.90691539738327\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:39<01:13,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  342.4784038839862\n","Test Loss:  70.07932535652071\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:43<01:09,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  335.9806393808685\n","Test Loss:  77.70616106223315\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:46<01:05,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  338.8343256302178\n","Test Loss:  68.59503689780831\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:50<01:02,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  337.956741202157\n","Test Loss:  73.37129693618044\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:53<00:58,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  334.13553069904447\n","Test Loss:  72.64140914659947\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:57<00:55,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  332.53053304646164\n","Test Loss:  68.29094535810873\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:00<00:51,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  328.9605027101934\n","Test Loss:  68.74685624614358\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:04<00:48,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  329.83156634960324\n","Test Loss:  68.23131401184946\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:07<00:45,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  324.1188204870559\n","Test Loss:  70.7022889922373\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:11<00:41,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  326.370234252885\n","Test Loss:  68.38067282456905\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:14<00:38,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  326.65209815930575\n","Test Loss:  70.92511680908501\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:17<00:34,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  321.7876151241362\n","Test Loss:  72.69021857436746\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:21<00:35,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  320.66009873803705\n","Test Loss:  68.47323013469577\n","Early stopping with best_loss:  68.23131401184946 and test_loss for this epoch:  68.47323013469577 ...\n","mae_validation:\n","1.0173460827664884\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:53,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  924.4972376972437\n","Test Loss:  147.24076215922832\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:49,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  636.1955210752785\n","Test Loss:  112.66532311215997\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:46,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  523.810303049162\n","Test Loss:  105.96154924854636\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:42,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  488.09702821448445\n","Test Loss:  103.05968997813761\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:38,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  460.80047878436744\n","Test Loss:  92.48079936485738\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:37,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  440.63271775469184\n","Test Loss:  89.55505989212543\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:33,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  425.718477351591\n","Test Loss:  90.10572243109345\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:29,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  417.4182187952101\n","Test Loss:  83.45503777684644\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:26,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  412.8352846838534\n","Test Loss:  81.23981876764446\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:35<02:23,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  396.8644168362953\n","Test Loss:  81.12323934352025\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:39<02:19,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  385.45946114277467\n","Test Loss:  80.64780501462519\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:42<02:16,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  380.57566311210394\n","Test Loss:  77.85139627568424\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:46<02:12,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  369.72187007125467\n","Test Loss:  77.43050017394125\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:49<02:08,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  364.1816688431427\n","Test Loss:  75.6645450713113\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:53<02:04,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  359.07260120101273\n","Test Loss:  72.74221693910658\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:57<02:01,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  359.0783073604107\n","Test Loss:  72.4575276945252\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:00<01:58,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  352.7862357618287\n","Test Loss:  72.75902980286628\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:04<01:54,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  350.50848084362224\n","Test Loss:  72.57856017164886\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:07<01:49,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  346.7881807857193\n","Test Loss:  77.12139144260436\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:11<01:46,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  340.7504807035439\n","Test Loss:  72.35214552469552\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:14<01:43,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  337.23754604067653\n","Test Loss:  75.44525069277734\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:18<01:39,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  328.4228557595052\n","Test Loss:  73.19782693544403\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:21<01:36,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  338.91755487723276\n","Test Loss:  70.12140498030931\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:25<01:32,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  335.5980613438878\n","Test Loss:  71.90832214872353\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:28<01:28,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  330.34545481950045\n","Test Loss:  70.25858858693391\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:32<01:25,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  326.82290981709957\n","Test Loss:  71.77885680831969\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:36<01:21,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  324.81469533522613\n","Test Loss:  71.07524486165494\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:39<01:24,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  331.8977791694924\n","Test Loss:  78.4266976355575\n","Early stopping with best_loss:  70.12140498030931 and test_loss for this epoch:  78.4266976355575 ...\n","mae_validation:\n","1.0407916861330673\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:52,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  806.9889794401824\n","Test Loss:  124.92241847701371\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:52,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  548.4962953496724\n","Test Loss:  104.23088294267654\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:51,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  489.48735374026\n","Test Loss:  101.5937078204006\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:46,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  458.4091510567814\n","Test Loss:  99.12031098734587\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:46,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  438.80620118416846\n","Test Loss:  91.23663836810738\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:41,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  425.43394906865433\n","Test Loss:  88.07466917298734\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:35,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  408.6512835966423\n","Test Loss:  87.13605497125536\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:29<02:32,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  393.0205451436341\n","Test Loss:  83.7550371629186\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:32<02:30,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  371.861425912939\n","Test Loss:  78.33163657039404\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:36<02:25,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  376.4321367931552\n","Test Loss:  82.26589296991006\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:40<02:22,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  363.110826196149\n","Test Loss:  82.32076615002006\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:43<02:19,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  352.32286574924365\n","Test Loss:  77.81604376062751\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:47<02:15,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  350.78132429672405\n","Test Loss:  72.31787467841059\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:51<02:11,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  344.9255513306707\n","Test Loss:  71.89559006341733\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:54<02:08,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  341.3867575349286\n","Test Loss:  77.2113858954981\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:58<02:03,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  341.32597229164094\n","Test Loss:  75.46156515646726\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:01<01:59,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  331.79239806113765\n","Test Loss:  72.88819018751383\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:05<01:55,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  332.54193204594776\n","Test Loss:  71.808295276016\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:09<01:52,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  322.87733274977654\n","Test Loss:  69.70985247008502\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:12<01:49,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  326.85785753745586\n","Test Loss:  74.57196551654488\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:16<01:45,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  322.82828484941274\n","Test Loss:  87.82484484743327\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:19<01:40,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  321.8160563246347\n","Test Loss:  70.49690721230581\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:23<01:36,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  324.39322300767526\n","Test Loss:  67.93033026810735\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:27<01:32,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  317.3185591911897\n","Test Loss:  68.75145144574344\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:30<01:29,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  320.09958812082186\n","Test Loss:  73.61170001421124\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:34<01:25,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  315.43006820464507\n","Test Loss:  66.97958580916747\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:37<01:22,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  313.84067631792277\n","Test Loss:  70.33878012467176\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:41<01:18,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  311.3073733015917\n","Test Loss:  67.12974218628369\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:44<01:14,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  310.28360123641323\n","Test Loss:  68.2721704216674\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:48<01:11,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  311.0124274333939\n","Test Loss:  69.33942104969174\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:51<01:07,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  300.0493366215378\n","Test Loss:  66.421053907834\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:55<01:04,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  306.00149372732267\n","Test Loss:  68.37768695084378\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:59<01:00,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  313.1345195416361\n","Test Loss:  70.26412490010262\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [02:02<00:56,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  303.5917563959956\n","Test Loss:  66.48178427387029\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:06<00:53,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  302.5790634546429\n","Test Loss:  67.66972832521424\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:09<00:50,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  298.01743819611147\n","Test Loss:  65.17607371741906\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:13<00:46,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  297.6711375089362\n","Test Loss:  65.77129638427868\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:16<00:42,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  301.0458175782114\n","Test Loss:  66.4324094275944\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:20<00:39,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  302.39281721273437\n","Test Loss:  66.90894198371097\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:24<00:35,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  302.75877606868744\n","Test Loss:  67.4766351301223\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:27<00:32,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  299.7326311264187\n","Test Loss:  64.80090300925076\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:31<00:28,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  295.95945871062577\n","Test Loss:  65.30226823408157\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:34<00:24,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  292.92883860878646\n","Test Loss:  65.50877214688808\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:38<00:21,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  291.3301105136052\n","Test Loss:  69.94804737903178\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:41<00:17,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  292.6858663652092\n","Test Loss:  65.40656199958175\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:45<00:18,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  290.654222690966\n","Test Loss:  68.12998951645568\n","Early stopping with best_loss:  64.80090300925076 and test_loss for this epoch:  68.12998951645568 ...\n","mae_validation:\n","0.9779057799975382\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:08,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  746.8762652724981\n","Test Loss:  116.88641064986587\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:02,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  520.352179473266\n","Test Loss:  117.70037963613868\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:59,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  471.10837094299495\n","Test Loss:  93.47449713014066\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<02:56,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  437.5262839589268\n","Test Loss:  97.8554417528212\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:19<02:53,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  419.5393562167883\n","Test Loss:  84.71476980019361\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:23<02:51,  3.90s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  394.0291511635296\n","Test Loss:  78.6441938742064\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:46,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  379.00122565962374\n","Test Loss:  76.5101935332641\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:30<02:41,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  370.44158431049436\n","Test Loss:  78.5067578330636\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:34<02:37,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  358.15586143080145\n","Test Loss:  73.84385077748448\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:38<02:33,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  354.9982940647751\n","Test Loss:  76.12225676514208\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:42<02:29,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  349.533011126332\n","Test Loss:  78.17755988612771\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:46<02:25,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  339.4251119089313\n","Test Loss:  72.42534665670246\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:49<02:20,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  341.5464256047271\n","Test Loss:  69.54306497145444\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:53<02:17,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  334.87381016276777\n","Test Loss:  68.22906138002872\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:57<02:14,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  329.99294044356793\n","Test Loss:  76.9688715217635\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [01:01<02:09,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  326.1867466993863\n","Test Loss:  70.15877641178668\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:05<02:05,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  325.0526626287028\n","Test Loss:  67.28291314886883\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:08<02:01,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  323.6321591483429\n","Test Loss:  71.49778066948056\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:12<01:58,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  320.0935266241431\n","Test Loss:  66.83450544532388\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:16<01:54,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  318.57154542044736\n","Test Loss:  68.72144203912467\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:20<01:51,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  313.09645963739604\n","Test Loss:  68.41347964759916\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:24<01:48,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  318.9278315389529\n","Test Loss:  67.76694277813658\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:28<01:43,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  312.44376555876806\n","Test Loss:  65.18728306470439\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:31<01:39,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  303.73175775725394\n","Test Loss:  75.25911059044302\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:35<01:36,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  310.76929107354954\n","Test Loss:  65.4932151613757\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:39<01:31,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.17085589375347\n","Test Loss:  63.94359913561493\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:43<01:27,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  302.9713083389215\n","Test Loss:  74.61527088563889\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:47<01:24,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  302.1510147191584\n","Test Loss:  63.33670406322926\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:51<01:20,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  303.95386043796316\n","Test Loss:  64.356275790371\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:54<01:16,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  295.8320218655281\n","Test Loss:  62.09146412555128\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:58<01:12,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  294.2107503032312\n","Test Loss:  62.95458776084706\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [02:02<01:08,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  308.1516893967055\n","Test Loss:  63.48268168326467\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [02:06<01:05,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  298.02766086068004\n","Test Loss:  61.84140839613974\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [02:10<01:01,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  299.5535353287123\n","Test Loss:  61.92620909679681\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:14<00:57,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  296.8462932133116\n","Test Loss:  66.01647791173309\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:17<00:52,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  297.0460508512333\n","Test Loss:  60.58170349244028\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:21<00:49,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  287.97146835224703\n","Test Loss:  61.65843812422827\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:25<00:45,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  293.98729140125215\n","Test Loss:  62.241596014704555\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:29<00:41,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  288.1531603659969\n","Test Loss:  63.809817917179316\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:32<00:37,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  284.4092951428611\n","Test Loss:  61.23962201643735\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:36<00:39,  3.92s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  287.4660689912271\n","Test Loss:  63.83780965209007\n","Early stopping with best_loss:  60.58170349244028 and test_loss for this epoch:  63.83780965209007 ...\n","mae_validation:\n","0.9459960289321212\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:41,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  1007.6848552152514\n","Test Loss:  156.40995061211288\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:39,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  743.6593506410718\n","Test Loss:  125.6248749345541\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:38,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  589.8780673146248\n","Test Loss:  107.47890982031822\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:33,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  517.2356813661754\n","Test Loss:  101.49050495424308\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:29,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  468.33132569491863\n","Test Loss:  95.77815105207264\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:26,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  451.35915536480024\n","Test Loss:  93.77269898168743\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:23,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  444.8797790049575\n","Test Loss:  91.52399788983166\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:21,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  426.10659351572394\n","Test Loss:  89.61099509638734\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:17,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  417.1075620446354\n","Test Loss:  90.40267010079697\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:14,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  406.1467558182776\n","Test Loss:  85.31441475776955\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:36<02:12,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  392.18684390047565\n","Test Loss:  84.51609902316704\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:10,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  382.1182605577633\n","Test Loss:  86.98641168605536\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:07,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  367.3218458723277\n","Test Loss:  81.59875562263187\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:47<02:05,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  368.4979619309306\n","Test Loss:  79.5047413026914\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:50<02:01,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  354.84109269920737\n","Test Loss:  78.1984799746424\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:54<01:57,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  350.732766543515\n","Test Loss:  78.07619051763322\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:57<01:54,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  346.04948924388736\n","Test Loss:  76.24707868043333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:01<01:50,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  340.94649431249127\n","Test Loss:  76.50948841916397\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:04<01:46,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  338.4573689205572\n","Test Loss:  74.60361422877759\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:08<01:43,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  326.5969422007911\n","Test Loss:  75.943991352804\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:11<01:39,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  327.2741324910894\n","Test Loss:  76.11528250295669\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:14<01:35,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  330.8826250345446\n","Test Loss:  74.29641304630786\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:18<01:31,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  325.002782468684\n","Test Loss:  82.18966880394146\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:21<01:27,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  316.84288881812245\n","Test Loss:  74.6049275607802\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:24<01:24,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  314.8785936895292\n","Test Loss:  71.661858309526\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:28<01:20,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  312.2057400532067\n","Test Loss:  72.75718932319432\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:31<01:17,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  314.83296882384457\n","Test Loss:  75.56187476962805\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:35<01:13,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  311.3282576650381\n","Test Loss:  72.67610471323133\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:38<01:10,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  313.2826144020073\n","Test Loss:  71.20908323302865\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:41<01:07,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  308.4909519762732\n","Test Loss:  74.16111868526787\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:45<01:03,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  309.9049186981283\n","Test Loss:  75.76267568394542\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:48<01:01,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  314.36364053888246\n","Test Loss:  70.83480576658621\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:51<00:57,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  310.3599698524922\n","Test Loss:  71.78118294663727\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:55<00:54,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  306.6369191114791\n","Test Loss:  74.2826495077461\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:58<00:50,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  305.02779401047155\n","Test Loss:  69.75691367127001\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:02<00:47,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  306.9756896481849\n","Test Loss:  70.4332413575612\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:05<00:43,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  299.9242208870128\n","Test Loss:  70.15615311358124\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:08<00:40,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  307.69073044136167\n","Test Loss:  72.35211204434745\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:12<00:37,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  297.1150182443671\n","Test Loss:  71.14324507815763\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:15<00:38,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  298.2421273207292\n","Test Loss:  71.77725494792685\n","Early stopping with best_loss:  69.75691367127001 and test_loss for this epoch:  71.77725494792685 ...\n","mae_validation:\n","1.0478760201728081\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:47,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  902.3186354413629\n","Test Loss:  147.54312970675528\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:48,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  606.0074487235397\n","Test Loss:  108.62387138418853\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:44,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  525.1027226205915\n","Test Loss:  97.53389548510313\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:39,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  477.98659909982234\n","Test Loss:  95.58622810430825\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:36,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  447.58279003435746\n","Test Loss:  86.74790346762165\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:32,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  421.13151338044554\n","Test Loss:  85.43026474118233\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:30,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  410.82290551345795\n","Test Loss:  85.12911695893854\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:27,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  407.12860330892727\n","Test Loss:  79.86053379205987\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:23,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  381.51186054432765\n","Test Loss:  81.54658581223339\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:34<02:19,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  373.2756337937899\n","Test Loss:  73.42270596837625\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:38<02:14,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  361.8779743928462\n","Test Loss:  93.72937775123864\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:41<02:11,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  362.5056281434372\n","Test Loss:  72.59479885082692\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:45<02:08,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  350.0136617817916\n","Test Loss:  70.84121595695615\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:48<02:04,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  350.53592557646334\n","Test Loss:  73.54225549474359\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:52<02:00,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  345.5331302615814\n","Test Loss:  75.19601106108166\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:55<01:57,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  336.6075916457921\n","Test Loss:  71.89335585013032\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:59<01:54,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  335.7450283220969\n","Test Loss:  73.01970032695681\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:02<01:51,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  328.1802018554881\n","Test Loss:  67.62175292149186\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:06<01:48,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  329.77784712193534\n","Test Loss:  66.01008427201305\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:09<01:44,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  329.25785747822374\n","Test Loss:  67.6120928907767\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:13<01:41,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  320.50946265505627\n","Test Loss:  68.77566242590547\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:16<01:37,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  319.80536293797195\n","Test Loss:  70.70008467882872\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:20<01:34,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  321.57260010764\n","Test Loss:  66.23998812376522\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:23<01:38,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  317.2492406903766\n","Test Loss:  69.2061713244766\n","Early stopping with best_loss:  66.01008427201305 and test_loss for this epoch:  69.2061713244766 ...\n","mae_validation:\n","1.009738827507979\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:48,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  806.0924213193357\n","Test Loss:  134.3958617048338\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:46,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  540.3216303251684\n","Test Loss:  111.52059913985431\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:43,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  478.31222089752555\n","Test Loss:  100.67087934771553\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:39,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  432.48712182510644\n","Test Loss:  96.38843941409141\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:37,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  404.79650886170566\n","Test Loss:  89.01558497943915\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:34,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  381.1713557736948\n","Test Loss:  105.64182985015213\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:29,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  366.58158174622804\n","Test Loss:  83.31112938094884\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:27,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  354.63918721908703\n","Test Loss:  82.67097307275981\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:24,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  354.59120305720717\n","Test Loss:  86.78453873191029\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:34<02:20,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  337.50021810480393\n","Test Loss:  81.26953720208257\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:38<02:16,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  341.935827891808\n","Test Loss:  79.26762599684298\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:41<02:13,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  332.8477036454715\n","Test Loss:  81.41882409527898\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:45<02:09,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  325.4643805939704\n","Test Loss:  80.46532631246373\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:48<02:05,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  322.86086288653314\n","Test Loss:  77.75505436398089\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:52<02:02,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  317.047869422473\n","Test Loss:  76.28533839341253\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:55<01:58,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  318.0397544587031\n","Test Loss:  76.00507824029773\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:59<01:54,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  303.781955675222\n","Test Loss:  74.37734930543229\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:02<01:51,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  312.70332349603996\n","Test Loss:  77.01996219810098\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:06<01:47,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  302.40555300051346\n","Test Loss:  74.63854974508286\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:09<01:44,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  303.4079193715006\n","Test Loss:  74.43087235931307\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:13<01:41,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  300.69726848602295\n","Test Loss:  75.09431779012084\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:16<01:37,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  300.1379291857593\n","Test Loss:  72.80524584744126\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:20<01:34,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  301.52245551743545\n","Test Loss:  73.75471279770136\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:23<01:30,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  301.9314235849306\n","Test Loss:  74.14738562027924\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:27<01:26,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  295.0533717623912\n","Test Loss:  78.46831609075889\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:30<01:23,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  291.4947888921015\n","Test Loss:  72.42517325328663\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:34<01:21,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  290.7640248257667\n","Test Loss:  73.3495018845424\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:37<01:17,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  291.11770384944975\n","Test Loss:  72.35534493252635\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:41<01:13,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  288.11817248770967\n","Test Loss:  74.20187884988263\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:44<01:10,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  285.82437443640083\n","Test Loss:  70.70643229130656\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:48<01:06,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  286.76500412169844\n","Test Loss:  75.20378165412694\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:51<01:02,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  286.41901882970706\n","Test Loss:  70.21108564920723\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:55<00:59,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  284.3321868511848\n","Test Loss:  78.41710042627528\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:58<00:55,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  283.1292967181653\n","Test Loss:  68.86742316419259\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:01<00:51,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  279.234906272497\n","Test Loss:  69.21998649602756\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:05<00:48,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  276.76999722677283\n","Test Loss:  69.08064662583638\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:08<00:44,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  284.0765997618437\n","Test Loss:  70.11111075989902\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:12<00:41,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  279.4490944975987\n","Test Loss:  69.01040429668501\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:15<00:42,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  273.89448976516724\n","Test Loss:  70.64668496884406\n","Early stopping with best_loss:  68.86742316419259 and test_loss for this epoch:  70.64668496884406 ...\n","mae_validation:\n","1.0277403064856585\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:08,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  759.7669657096267\n","Test Loss:  112.11470169760287\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:02,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  514.4149166028947\n","Test Loss:  107.03221784345806\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:59,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  459.0037541715428\n","Test Loss:  100.1373844370246\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<02:56,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  419.1274983501062\n","Test Loss:  85.37750706402585\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:19<02:54,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  386.67491000145674\n","Test Loss:  84.38312525767833\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:23<02:50,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  374.81775899697095\n","Test Loss:  77.54854951333255\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:45,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  366.4010516218841\n","Test Loss:  76.36056821327657\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:30<02:41,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  347.5463476665318\n","Test Loss:  75.79424961004406\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:34<02:36,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  350.7106978148222\n","Test Loss:  82.15957251563668\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:38<02:32,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  339.60781751805916\n","Test Loss:  78.30304892268032\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:42<02:30,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  332.87477382365614\n","Test Loss:  79.00330906221643\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:46<02:26,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  321.8736395603046\n","Test Loss:  71.73316343687475\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:49<02:22,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  317.97551353648305\n","Test Loss:  70.97970213135704\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:53<02:19,  3.88s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  318.26516055036336\n","Test Loss:  70.77974241273478\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:57<02:15,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  309.868078189902\n","Test Loss:  76.72779747843742\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [01:01<02:11,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.49546301411465\n","Test Loss:  69.95535143371671\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:05<02:07,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  311.3044446213171\n","Test Loss:  70.48039392195642\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:09<02:02,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  299.3962182078976\n","Test Loss:  71.59602393675596\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:12<01:57,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  300.38251253357157\n","Test Loss:  69.44501721300185\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:16<01:53,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  297.1172940704273\n","Test Loss:  70.09476139722392\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:20<01:49,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  296.05588879575953\n","Test Loss:  69.81277023861185\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:24<01:46,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  290.195477551315\n","Test Loss:  68.84680035337806\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:28<01:42,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  294.2423271816224\n","Test Loss:  78.09081045631319\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:31<01:38,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  295.47626030794345\n","Test Loss:  72.76346024218947\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:35<01:34,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  286.4290355592966\n","Test Loss:  68.95432103145868\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:39<01:30,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  288.77369934739545\n","Test Loss:  70.25742503069341\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:43<01:26,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  278.5642743278295\n","Test Loss:  67.80837790109217\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:46<01:23,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  285.47242370597087\n","Test Loss:  68.76561422646046\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:50<01:19,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  282.22636986197904\n","Test Loss:  68.12644523289055\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:54<01:15,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  279.407688642852\n","Test Loss:  67.87706662528217\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:58<01:11,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  278.11532723065466\n","Test Loss:  66.0224436444696\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [02:02<01:08,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  279.26945907692425\n","Test Loss:  68.27792221307755\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [02:05<01:04,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  279.7156515675597\n","Test Loss:  69.76406070496887\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [02:09<01:00,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  275.6546773305163\n","Test Loss:  66.27953502675518\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:13<00:57,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  275.1239771847613\n","Test Loss:  65.80007209442556\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:17<00:53,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  274.1951846033335\n","Test Loss:  65.67831049673259\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:21<00:49,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  270.7073989165947\n","Test Loss:  65.9566752279643\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:24<00:45,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  275.72432440984994\n","Test Loss:  71.40948104579002\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:28<00:41,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  270.9655110654421\n","Test Loss:  70.27881872095168\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:32<00:38,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  267.22659542784095\n","Test Loss:  65.13198139891028\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:36<00:34,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  267.06704626977444\n","Test Loss:  66.87000771681778\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:40<00:30,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  268.35881194705144\n","Test Loss:  66.5253752572462\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:43<00:26,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  265.730569224339\n","Test Loss:  70.32269209530205\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:47<00:22,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  263.2780049778521\n","Test Loss:  64.40183416334912\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:51<00:18,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  266.5690307293553\n","Test Loss:  65.48704758752137\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 92%|█████████▏| 46/50 [02:55<00:15,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  267.46748544322327\n","Test Loss:  64.02125927899033\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 94%|█████████▍| 47/50 [02:58<00:11,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  265.88899994129315\n","Test Loss:  64.44053459819406\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 96%|█████████▌| 48/50 [03:02<00:07,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  261.117975525558\n","Test Loss:  69.23970106616616\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 49/50 [03:06<00:03,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  260.1237871325575\n","Test Loss:  64.38748157862574\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 50/50 [03:10<00:00,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  262.51428782986477\n","Test Loss:  63.884021933190525\n","mae_validation:\n","0.9954948843343183\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:44,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  933.4672151654959\n","Test Loss:  144.92118440568447\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:40,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  660.1453299671412\n","Test Loss:  117.74306763708591\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:34,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  520.8297993000597\n","Test Loss:  104.61531371809542\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:31,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  462.80615179799497\n","Test Loss:  95.18716584332287\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:27,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  435.40351328253746\n","Test Loss:  92.6300306096673\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:23,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  413.32838405575603\n","Test Loss:  92.06423997180536\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:22<02:20,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  397.4565166872926\n","Test Loss:  85.27249443111941\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:17,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  383.82857493078336\n","Test Loss:  85.22621176205575\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:29<02:13,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  379.1407162025571\n","Test Loss:  81.84832115378231\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:32<02:10,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  361.8121169982478\n","Test Loss:  83.90370695106685\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:35<02:06,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  356.049433096312\n","Test Loss:  83.02433118782938\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:39<02:03,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  352.78327089129016\n","Test Loss:  76.98953250981867\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:42<02:00,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  352.3016909407452\n","Test Loss:  75.6782820140943\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:45<01:57,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  340.10755839711055\n","Test Loss:  77.53881021589041\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:48<01:53,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  336.13972556823865\n","Test Loss:  73.45060164271854\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:52<01:50,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  332.61499160621315\n","Test Loss:  77.08259358908981\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:55<01:47,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  322.4616878218949\n","Test Loss:  72.55808995570987\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [00:58<01:44,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  330.9321958613582\n","Test Loss:  69.83274240791798\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:02<01:40,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  317.9425717112608\n","Test Loss:  71.94533505570143\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:05<01:37,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  316.2655523037538\n","Test Loss:  70.72862914204597\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:08<01:34,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  311.39978148043156\n","Test Loss:  74.79194983839989\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:11<01:31,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  307.99014252610505\n","Test Loss:  70.47709877602756\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:15<01:27,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  304.5045660263859\n","Test Loss:  69.12989494437352\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:18<01:24,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  306.9913946967572\n","Test Loss:  67.73234105296433\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:21<01:21,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  303.8889412311837\n","Test Loss:  69.36003324948251\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:24<01:17,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  299.72476443462074\n","Test Loss:  68.29070565849543\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:27<01:14,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  301.17981916619465\n","Test Loss:  66.61451267637312\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:31<01:11,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  299.5850029331632\n","Test Loss:  69.53437431529164\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:34<01:08,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  298.4648773991503\n","Test Loss:  67.95900585339405\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:37<01:05,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  298.61022628238425\n","Test Loss:  65.91786389518529\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:41<01:01,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  295.5063155977987\n","Test Loss:  65.51795237697661\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:44<00:58,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  291.8830693811178\n","Test Loss:  66.76890196325257\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:47<00:55,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  295.67067342530936\n","Test Loss:  65.20255633303896\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:50<00:52,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  294.50675758789293\n","Test Loss:  66.25447788531892\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:54<00:49,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  290.368202297017\n","Test Loss:  67.53483670670539\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [01:57<00:46,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  288.8810473596677\n","Test Loss:  69.27887349762022\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:00<00:43,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  284.70623437920585\n","Test Loss:  68.81863078707829\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:04<00:40,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  285.0611585229635\n","Test Loss:  64.15445061214268\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:07<00:36,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  290.1041893903166\n","Test Loss:  65.01398917706683\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:10<00:33,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  282.47479446418583\n","Test Loss:  65.72013808507472\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:14<00:29,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  285.73886421415955\n","Test Loss:  64.35397090110928\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:17<00:26,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  287.3757275319658\n","Test Loss:  65.86152430390939\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:21<00:26,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  286.42403024295345\n","Test Loss:  64.20902140904218\n","Early stopping with best_loss:  64.15445061214268 and test_loss for this epoch:  64.20902140904218 ...\n","mae_validation:\n","1.0093618827982782\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:49,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  866.762083940208\n","Test Loss:  137.02688945829868\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:43,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  585.7544904127717\n","Test Loss:  103.3544742166996\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:41,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  485.1082935370505\n","Test Loss:  93.27695991285145\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:37,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  441.50389403197914\n","Test Loss:  86.61224508285522\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:33,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  415.8298005256802\n","Test Loss:  95.39397929422557\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:32,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  401.7292143073864\n","Test Loss:  79.00281305145472\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:30,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  381.5939542152919\n","Test Loss:  82.52539378870279\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:26,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  377.72126008057967\n","Test Loss:  73.41076863277704\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:22,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  361.46032897010446\n","Test Loss:  71.73026689025573\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:34<02:19,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  361.67655555787496\n","Test Loss:  72.5301257930696\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:38<02:15,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  342.47116829734296\n","Test Loss:  85.32199816964567\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:41<02:12,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  345.3516983445734\n","Test Loss:  70.01906702527776\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:45<02:08,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  340.790522408206\n","Test Loss:  69.84021722897887\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:48<02:04,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  335.45053531159647\n","Test Loss:  74.5506009021774\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:51<02:00,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  325.52515975385904\n","Test Loss:  68.3685675226152\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:55<01:56,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  324.0089112473652\n","Test Loss:  68.99728864617646\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:58<01:52,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  320.70463752467185\n","Test Loss:  69.46370676811785\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:02<01:50,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  316.1610904149711\n","Test Loss:  65.92181249894202\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:05<01:46,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  309.2618712398689\n","Test Loss:  69.53922116244212\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:08<01:42,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  314.8475846336223\n","Test Loss:  65.67970564123243\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:12<01:39,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  310.37044497951865\n","Test Loss:  65.00141845736653\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:15<01:35,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  307.2600393202156\n","Test Loss:  64.88857423793525\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:19<01:31,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  297.35269157681614\n","Test Loss:  70.33732818067074\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:22<01:28,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  301.515272132121\n","Test Loss:  65.85004484606907\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:26<01:25,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  297.76963108964264\n","Test Loss:  65.42749603977427\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:29<01:21,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  300.6936447583139\n","Test Loss:  64.1116550178267\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:32<01:17,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  298.4829491665587\n","Test Loss:  64.77747332770377\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:36<01:14,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  292.5537082357332\n","Test Loss:  67.00599456951022\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:39<01:11,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  290.6337672001682\n","Test Loss:  65.13003442110494\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:42<01:07,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  294.3885646471754\n","Test Loss:  65.34529194701463\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:46<01:10,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  285.731685992796\n","Test Loss:  71.13592790160328\n","Early stopping with best_loss:  64.1116550178267 and test_loss for this epoch:  71.13592790160328 ...\n","mae_validation:\n","1.0112468445463902\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:46,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  767.0130192264915\n","Test Loss:  126.78140933439136\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:41,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  519.8747068252414\n","Test Loss:  105.19875871576369\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:38,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  449.08735649473965\n","Test Loss:  97.24654576927423\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:35,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  429.39274765830487\n","Test Loss:  92.61832964420319\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:31,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  399.00570456217974\n","Test Loss:  87.21787086315453\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:28,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  378.657099254895\n","Test Loss:  83.67170170135796\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:25,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  356.225689413026\n","Test Loss:  87.12496828194708\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:23,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  349.6714465050027\n","Test Loss:  80.03736436227337\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:18,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  338.40362340118736\n","Test Loss:  78.34903704561293\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:14,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  327.43567605502903\n","Test Loss:  80.06122039631009\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:11,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  328.68864175025374\n","Test Loss:  75.67129038088024\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:07,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  317.79613579576835\n","Test Loss:  78.59696174226701\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:04,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.43164811469615\n","Test Loss:  75.54055004566908\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:47<02:01,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  310.34648830024526\n","Test Loss:  73.71088508656248\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:50<01:59,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  304.46218431787565\n","Test Loss:  72.1269475929439\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:54<01:56,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  312.0053748120554\n","Test Loss:  72.19097371771932\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:57<01:52,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  303.0162095823325\n","Test Loss:  73.1949192835018\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:01<01:49,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  296.89851124957204\n","Test Loss:  71.90038541145623\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:04<01:46,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  288.4990751473233\n","Test Loss:  73.00878454837948\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:07<01:42,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  294.37328180484474\n","Test Loss:  70.81685356516391\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:11<01:39,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  287.2459528106265\n","Test Loss:  70.11840377794579\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:14<01:35,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  283.7234547305852\n","Test Loss:  69.74985991232097\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:18<01:32,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  284.552090940997\n","Test Loss:  72.02431218000129\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:21<01:28,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  279.85646366584115\n","Test Loss:  69.37735664658248\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:24<01:24,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  273.81135845091194\n","Test Loss:  71.25710518565029\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:28<01:22,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  280.868914762279\n","Test Loss:  69.3360241400078\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:31<01:18,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  277.75539168482646\n","Test Loss:  69.71520005818456\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:35<01:15,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  276.4396352618933\n","Test Loss:  68.35523349442519\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:38<01:12,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  274.4419427588582\n","Test Loss:  68.78549044486135\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:42<01:08,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  272.41526926867664\n","Test Loss:  70.464311176911\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:45<01:04,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  269.2515964936465\n","Test Loss:  68.04753775685094\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:48<01:01,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  272.95939866267145\n","Test Loss:  67.55802344274707\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:52<00:58,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  270.0262887952849\n","Test Loss:  67.8590502680745\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:55<00:54,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  267.1237496419344\n","Test Loss:  67.26919311471283\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:59<00:50,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  267.2323689330369\n","Test Loss:  69.2573790512979\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:02<00:47,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  262.112518240232\n","Test Loss:  70.35190044809133\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:05<00:44,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  263.9200422782451\n","Test Loss:  68.96578350337222\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:09<00:40,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  263.8809077483602\n","Test Loss:  68.08625073730946\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:13<00:38,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  263.46387854171917\n","Test Loss:  66.37909227190539\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:16<00:35,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  260.6848950609565\n","Test Loss:  65.5526674692519\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:20<00:31,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  259.0859202367719\n","Test Loss:  68.53857849957421\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:23<00:28,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  259.89734237268567\n","Test Loss:  67.46148312743753\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:27<00:24,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  257.3390008257702\n","Test Loss:  66.1258841259405\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:30<00:20,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  255.7197829252109\n","Test Loss:  66.30590718518943\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:34<00:21,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  257.1880875499919\n","Test Loss:  66.47700529778376\n","Early stopping with best_loss:  65.5526674692519 and test_loss for this epoch:  66.47700529778376 ...\n","mae_validation:\n","1.017699689038018\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:12,  3.92s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  722.0925247184932\n","Test Loss:  124.29040479985997\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:07,  3.91s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  504.86613068170846\n","Test Loss:  94.10112944431603\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<03:06,  3.97s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  431.7547118961811\n","Test Loss:  115.49075054563582\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<03:02,  3.98s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  408.39663339499384\n","Test Loss:  84.38724498776719\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:19<02:57,  3.94s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  377.94413480814546\n","Test Loss:  81.95487439911813\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:23<02:52,  3.92s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  359.09565898263827\n","Test Loss:  78.40723644709215\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:27<02:48,  3.92s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  352.17235567606986\n","Test Loss:  73.83209093846381\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:31<02:44,  3.91s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  340.6980011211708\n","Test Loss:  71.99394108727574\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:35<02:40,  3.91s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  328.48745026625693\n","Test Loss:  79.59084860375151\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:39<02:35,  3.88s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  326.03597932495177\n","Test Loss:  69.59128820430487\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:42<02:29,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  319.5528759416193\n","Test Loss:  90.06150799896568\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:46<02:25,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  313.42999425390735\n","Test Loss:  68.4522813980002\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:50<02:21,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.79012305336073\n","Test Loss:  67.19292954262346\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:54<02:17,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.4218032332137\n","Test Loss:  66.71205693110824\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:58<02:12,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  302.45257546938956\n","Test Loss:  66.43761738203466\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [01:01<02:08,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  301.67420206591487\n","Test Loss:  65.05335729429498\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:05<02:04,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  297.6054833307862\n","Test Loss:  65.52394640701823\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:09<02:00,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  303.104492419865\n","Test Loss:  63.87080143485218\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:13<01:56,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  292.519264862407\n","Test Loss:  70.59638569224626\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:16<01:53,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  288.31472037546337\n","Test Loss:  68.38539665006101\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:20<01:49,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  284.53753921203315\n","Test Loss:  64.2161773215048\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:24<01:45,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  285.84556176979095\n","Test Loss:  62.06122357631102\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:28<01:41,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  282.01191120408475\n","Test Loss:  63.98805791279301\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:31<01:38,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  276.6557234460488\n","Test Loss:  61.38663740083575\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:35<01:34,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  275.8532997695729\n","Test Loss:  62.01200480712578\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:39<01:30,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  277.81088899611495\n","Test Loss:  60.61961003229953\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:43<01:27,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  279.0433336091228\n","Test Loss:  59.86906088795513\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:47<01:22,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  274.9261765333358\n","Test Loss:  60.89806016907096\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:50<01:18,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  272.6990172788501\n","Test Loss:  61.551219389773905\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:54<01:16,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  272.1735810379614\n","Test Loss:  60.77626214630436\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:58<01:12,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  270.67524656420574\n","Test Loss:  61.77702043764293\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [02:02<01:08,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  267.18240201612934\n","Test Loss:  58.60685136914253\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [02:06<01:05,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  265.03576944069937\n","Test Loss:  59.343016752041876\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [02:09<01:01,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  262.4872375894338\n","Test Loss:  58.07398522365838\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:13<00:56,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  266.2208022600971\n","Test Loss:  58.101943811867386\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:17<00:52,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  263.85585834737867\n","Test Loss:  58.002264803275466\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:21<00:49,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  260.5661042924039\n","Test Loss:  58.17238310398534\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:24<00:45,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  265.3830635664053\n","Test Loss:  57.394911223091185\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:28<00:41,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  263.1339502222836\n","Test Loss:  56.70654678437859\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:32<00:37,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  256.9557368089445\n","Test Loss:  57.34226744901389\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:36<00:33,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  257.5612914292142\n","Test Loss:  57.665955684147775\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:39<00:29,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  257.7008299822919\n","Test Loss:  56.741660023108125\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:43<00:26,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  253.2279897565022\n","Test Loss:  57.203083576168865\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:47<00:27,  3.90s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  252.0452257837169\n","Test Loss:  57.26296945940703\n","Early stopping with best_loss:  56.70654678437859 and test_loss for this epoch:  57.26296945940703 ...\n","mae_validation:\n","0.9301306874415025\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:35,  3.18s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  972.048556599766\n","Test Loss:  145.72105959057808\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:35,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  690.2963616400957\n","Test Loss:  114.92884211987257\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:31,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  556.9992723278701\n","Test Loss:  100.76778619736433\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:12<02:28,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  493.6209336714819\n","Test Loss:  90.88741813972592\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:23,  3.20s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  447.5377591662109\n","Test Loss:  87.86867152433842\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:22,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  426.8500994644128\n","Test Loss:  86.69743788614869\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:22<02:18,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  410.04598810570315\n","Test Loss:  80.52742758020759\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:25<02:15,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  390.3766427449882\n","Test Loss:  81.43763281730935\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:28<02:11,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  389.46949493838474\n","Test Loss:  80.69125134591013\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:32<02:08,  3.20s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  379.2126354537904\n","Test Loss:  75.78934955131263\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:35<02:04,  3.19s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  368.6748293945566\n","Test Loss:  78.04656942095608\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:38<02:01,  3.20s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  359.9057513875887\n","Test Loss:  72.92701370920986\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:41<01:59,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  348.71686422184575\n","Test Loss:  72.63571317726746\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:45<01:56,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  345.9204113818705\n","Test Loss:  71.78427363839\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:48<01:53,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  339.5139113981277\n","Test Loss:  74.03821623371914\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:51<01:50,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  336.4564985120669\n","Test Loss:  69.560451653786\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:54<01:47,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  329.1078604068607\n","Test Loss:  70.71455929242074\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [00:58<01:43,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  323.08325061015785\n","Test Loss:  66.78724535834044\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:01<01:39,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  330.15068954322487\n","Test Loss:  66.892353172414\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:04<01:36,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  323.4579978720285\n","Test Loss:  67.49021413456649\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:07<01:33,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  317.8597668893635\n","Test Loss:  65.13282135431655\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:10<01:30,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  312.8796619079076\n","Test Loss:  64.32079872582108\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:14<01:27,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  314.01364011131227\n","Test Loss:  63.827503063715994\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:17<01:24,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.15498231723905\n","Test Loss:  63.064768637064844\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:20<01:20,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  304.2467410610989\n","Test Loss:  63.41365128569305\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:23<01:16,  3.20s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  305.030157137895\n","Test Loss:  62.827887600287795\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:26<01:13,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  300.71997365169227\n","Test Loss:  62.694275226444006\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:30<01:11,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  295.11412490997463\n","Test Loss:  61.75026177382097\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:33<01:08,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  298.48646464711055\n","Test Loss:  65.20478663098766\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:36<01:06,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  297.89017249038443\n","Test Loss:  60.922306690365076\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:40<01:02,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  294.36366623220965\n","Test Loss:  59.81995395454578\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:43<00:58,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  294.33059001807123\n","Test Loss:  61.26585395215079\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:46<00:55,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  290.35150203248486\n","Test Loss:  61.728005500510335\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:49<00:51,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  293.14758486300707\n","Test Loss:  61.2892370861955\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:53<00:48,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  286.3203897131607\n","Test Loss:  60.916122938506305\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:56<00:49,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  280.96026444016024\n","Test Loss:  60.34126769029535\n","Early stopping with best_loss:  59.81995395454578 and test_loss for this epoch:  60.34126769029535 ...\n","mae_validation:\n","0.9880418477260643\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:44,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  844.8798258528113\n","Test Loss:  137.48418020829558\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:41,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  577.8354184255004\n","Test Loss:  111.96112363785505\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:37,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  485.3591413144022\n","Test Loss:  99.55360548291355\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:35,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  451.90142135415226\n","Test Loss:  90.18221139535308\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:30,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  417.1826213793829\n","Test Loss:  86.92560760071501\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:25,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  399.0372897647321\n","Test Loss:  89.93697549775243\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:22,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  382.75195813551545\n","Test Loss:  80.83482662215829\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:18,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  376.6400542380288\n","Test Loss:  78.36248494684696\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:29<02:14,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  366.5283603086136\n","Test Loss:  77.91806446854025\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:11,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  349.14364271843806\n","Test Loss:  78.32306489162147\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:36<02:08,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  349.5005945703015\n","Test Loss:  74.92949034646153\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:39<02:04,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  338.77397994697094\n","Test Loss:  73.1970690828748\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:42<02:00,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  332.01643006969243\n","Test Loss:  72.86480237264186\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:46<01:58,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  328.41657612100244\n","Test Loss:  75.94095597416162\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:49<01:55,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  324.60508390143514\n","Test Loss:  70.29957366595045\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:52<01:51,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  314.4220671374351\n","Test Loss:  71.499228682369\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:56<01:48,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.4251953572966\n","Test Loss:  68.74124149698764\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [00:59<01:45,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.70719940122217\n","Test Loss:  67.76081935316324\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:02<01:41,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  304.48110812110826\n","Test Loss:  66.38827306684107\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:05<01:37,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  299.89788678241894\n","Test Loss:  67.75207957625389\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:09<01:34,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  298.684000691399\n","Test Loss:  64.7208036435768\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:12<01:31,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  297.04751992668025\n","Test Loss:  69.54612849582918\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:15<01:28,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  298.1517723891884\n","Test Loss:  74.88313374528661\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:19<01:25,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  291.8829781888053\n","Test Loss:  65.98593029566109\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:22<01:22,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  295.5515227681026\n","Test Loss:  64.29079613741487\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:25<01:19,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  287.69849807349965\n","Test Loss:  62.332471263362095\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:29<01:16,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  290.9466257588938\n","Test Loss:  61.52316478523426\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:32<01:13,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  281.32937283813953\n","Test Loss:  61.719010803382844\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:35<01:10,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  281.9525841008872\n","Test Loss:  62.14645208977163\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:39<01:07,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  279.25480506010354\n","Test Loss:  61.924182892777026\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:42<01:04,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  286.63405509106815\n","Test Loss:  61.104903659783304\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:45<01:00,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  272.81914054090157\n","Test Loss:  62.403064475394785\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:49<00:56,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  274.7587066190317\n","Test Loss:  64.35900063952431\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:52<00:53,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  273.9519291818142\n","Test Loss:  63.72604146040976\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:55<00:50,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  274.6204160032794\n","Test Loss:  59.54650233872235\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [01:59<00:47,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  269.475036226213\n","Test Loss:  59.24066054262221\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:02<00:44,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  267.9108971208334\n","Test Loss:  58.914400461595505\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:06<00:40,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  269.0502861486748\n","Test Loss:  58.696519249118865\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:09<00:37,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  266.54596253158525\n","Test Loss:  58.59861285914667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:12<00:33,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  269.32572330348194\n","Test Loss:  58.77416210155934\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:16<00:30,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  261.2991303373128\n","Test Loss:  58.32756430702284\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:19<00:26,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  262.1557113514282\n","Test Loss:  59.7449241951108\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:22<00:23,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  261.8762989207171\n","Test Loss:  62.516383172012866\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:26<00:19,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  260.0724604232237\n","Test Loss:  58.35194811783731\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:29<00:16,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  257.28304960625246\n","Test Loss:  57.745617194101214\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 92%|█████████▏| 46/50 [02:32<00:13,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  262.5508713228628\n","Test Loss:  59.11186425574124\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 94%|█████████▍| 47/50 [02:36<00:09,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  260.6047708140686\n","Test Loss:  56.81596721988171\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 96%|█████████▌| 48/50 [02:39<00:06,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  257.97501071728766\n","Test Loss:  58.5867740730755\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 49/50 [02:42<00:03,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  255.84953549760394\n","Test Loss:  56.49278906499967\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 50/50 [02:46<00:00,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  257.2670217221603\n","Test Loss:  56.33782035717741\n","mae_validation:\n","0.9398677853988949\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:48,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  755.1881938166916\n","Test Loss:  126.38647830113769\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:42,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  518.5515451170504\n","Test Loss:  104.51978807570413\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:38,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  444.837637539953\n","Test Loss:  103.63372583314776\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:35,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  423.1133745238185\n","Test Loss:  88.12876925803721\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:32,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  392.3994228867814\n","Test Loss:  81.98614932596684\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:31,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  372.04131614184007\n","Test Loss:  79.00882534589618\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:28,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  358.7932134475559\n","Test Loss:  79.57836028886959\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:25,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  347.9927739677951\n","Test Loss:  72.77826304081827\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:20,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  340.4425277323462\n","Test Loss:  74.27586908917874\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:34<02:16,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  331.2744261776097\n","Test Loss:  69.33513315673918\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:11,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  327.42788220290095\n","Test Loss:  74.6220329720527\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:07,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  322.01207346562296\n","Test Loss:  66.51743189245462\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:44<02:03,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.6695232046768\n","Test Loss:  63.92159245442599\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:47<02:00,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  317.7149784346111\n","Test Loss:  72.00957752019167\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:50<01:57,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  304.86506948340684\n","Test Loss:  63.607028170488775\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:54<01:52,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  307.0283850925043\n","Test Loss:  65.65208541648462\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:57<01:49,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  299.81275699846447\n","Test Loss:  61.219262479338795\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:00<01:46,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  297.77386143244803\n","Test Loss:  61.10658971220255\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:04<01:43,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  286.5299794408493\n","Test Loss:  61.029989102855325\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:07<01:39,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  290.00038172723725\n","Test Loss:  59.64649486215785\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:10<01:37,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  286.4366859649308\n","Test Loss:  59.59342128504068\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:14<01:34,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  288.920866727829\n","Test Loss:  59.23787663318217\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:17<01:30,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  283.54050639574416\n","Test Loss:  58.56609139125794\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:20<01:26,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  278.06368599133566\n","Test Loss:  58.41132509801537\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:24<01:23,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  280.26836506230757\n","Test Loss:  60.89063116069883\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:27<01:20,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  282.9176775980741\n","Test Loss:  55.54623785801232\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:30<01:16,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  272.7012310670689\n","Test Loss:  58.35570300742984\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:34<01:13,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  267.67786487331614\n","Test Loss:  56.161496033892035\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:37<01:10,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  273.3752844808623\n","Test Loss:  56.24511258583516\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:40<01:07,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  271.36976238898933\n","Test Loss:  55.73621181678027\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:44<01:03,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  264.84087272174656\n","Test Loss:  54.18024024320766\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:47<01:00,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  265.42374293203466\n","Test Loss:  54.18228456331417\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:50<00:56,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  258.885977889644\n","Test Loss:  53.9121320778504\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:54<00:53,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  259.6012708786875\n","Test Loss:  53.384036945179105\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:57<00:50,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  261.40795052738395\n","Test Loss:  54.10813383292407\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:01<00:46,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  260.09381178207695\n","Test Loss:  54.911076817661524\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:04<00:43,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  254.66649224841967\n","Test Loss:  52.11432207515463\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:07<00:40,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  254.8524326523766\n","Test Loss:  58.365944187156856\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:11<00:36,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  257.1906171636656\n","Test Loss:  53.70963881770149\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:14<00:33,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  254.43566839722916\n","Test Loss:  52.767137474380434\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:17<00:30,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  253.26619906467386\n","Test Loss:  53.075719554908574\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:21<00:26,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  248.8949750396423\n","Test Loss:  51.118708819150925\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:24<00:23,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  246.20935337804258\n","Test Loss:  52.07708966266364\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:27<00:19,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  246.8466755556874\n","Test Loss:  51.67494008503854\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:31<00:16,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  242.2180094947107\n","Test Loss:  53.031591463368386\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 92%|█████████▏| 46/50 [02:34<00:13,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  244.9765597623773\n","Test Loss:  51.17772594606504\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 94%|█████████▍| 47/50 [02:37<00:09,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  249.31379025336355\n","Test Loss:  50.616432849434204\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 96%|█████████▌| 48/50 [02:40<00:06,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  239.75265009514987\n","Test Loss:  52.71242230408825\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 49/50 [02:44<00:03,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  242.8086357442662\n","Test Loss:  50.50833737431094\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 50/50 [02:47<00:00,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  243.19292688881978\n","Test Loss:  50.91065082605928\n","mae_validation:\n","0.8807582607794938\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:06,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  702.4565197695047\n","Test Loss:  135.48458080366254\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:58,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  496.3488549320027\n","Test Loss:  96.42762173619121\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:54,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  415.4530526502058\n","Test Loss:  90.74714520294219\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:50,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  399.4453340889886\n","Test Loss:  81.94584825471975\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:47,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  371.123989444226\n","Test Loss:  78.0943605452776\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:43,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  361.3956048809923\n","Test Loss:  74.57191902585328\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:40,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  349.78734644223005\n","Test Loss:  79.11745571903884\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:29<02:38,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  339.94370401930064\n","Test Loss:  69.56884554633871\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:33<02:33,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  332.25671534985304\n","Test Loss:  68.30194100830704\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:37<02:30,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.8844423899427\n","Test Loss:  68.13011911977082\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:41<02:28,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  308.4929512855597\n","Test Loss:  66.71756696794182\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:45<02:24,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  307.33816801197827\n","Test Loss:  65.23961039539427\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:48<02:19,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  294.0542182261124\n","Test Loss:  68.71094646211714\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:52<02:14,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  293.9307735771872\n","Test Loss:  66.17524678446352\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:56<02:10,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  288.800764137879\n","Test Loss:  68.60127795208246\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:59<02:06,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  286.7059316379018\n","Test Loss:  64.45132321398705\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:03<02:02,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  283.5869602719322\n","Test Loss:  65.3839369820198\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:07<01:58,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  281.17237395467237\n","Test Loss:  60.60277795884758\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:10<01:55,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  273.96382967662066\n","Test Loss:  63.7365804631263\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:14<01:51,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  270.67396641289815\n","Test Loss:  60.12369310017675\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:18<01:47,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  270.27007123082876\n","Test Loss:  61.207754637580365\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:22<01:43,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  264.61028291936964\n","Test Loss:  62.35623312462121\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:25<01:40,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  262.45463238842785\n","Test Loss:  61.3410452157259\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:29<01:36,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  263.8716224976815\n","Test Loss:  58.67088817525655\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:33<01:32,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  263.707220593933\n","Test Loss:  58.774153254926205\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:36<01:29,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  258.93200240517035\n","Test Loss:  61.59046096075326\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:40<01:25,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  254.40354903507978\n","Test Loss:  61.95563895814121\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:44<01:21,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  255.9257214271929\n","Test Loss:  59.16992893256247\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:48<01:24,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  250.96479026321322\n","Test Loss:  59.1616488317959\n","Early stopping with best_loss:  58.67088817525655 and test_loss for this epoch:  59.1616488317959 ...\n","mae_validation:\n","0.9647185383050058\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:52,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  741.5225094798952\n","Test Loss:  120.32178455963731\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:48,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  505.9424423230812\n","Test Loss:  98.84677983028814\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:44,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  445.8286195155233\n","Test Loss:  88.53070821799338\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:40,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  417.9110044594854\n","Test Loss:  82.7817050293088\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:35,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  387.3504266459495\n","Test Loss:  83.47136073419824\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:31,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  374.0360843143426\n","Test Loss:  78.86339249880984\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:27,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  367.1723944582045\n","Test Loss:  83.64369137352332\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:25,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  360.2116297679022\n","Test Loss:  75.98332613473758\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:22,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  359.9349478408694\n","Test Loss:  77.23452411498874\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:34<02:19,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  355.9964666762389\n","Test Loss:  78.9054149677977\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:38<02:15,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  348.39336738642305\n","Test Loss:  75.29985009226948\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:41<02:11,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  344.85312651563436\n","Test Loss:  76.85202748561278\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:45<02:08,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  345.3497422207147\n","Test Loss:  81.63835145323537\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:48<02:05,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  348.45572854205966\n","Test Loss:  72.63322520721704\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:52<02:02,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  340.3497337512672\n","Test Loss:  79.52581107616425\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:55<01:58,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  328.6219966998324\n","Test Loss:  79.48691639583558\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:59<01:55,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  332.94221719913185\n","Test Loss:  71.80666622426361\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:02<01:52,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  340.99623923562467\n","Test Loss:  70.46873822901398\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:06<01:49,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  329.99334846483544\n","Test Loss:  73.33502862881869\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:09<01:45,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  325.0810873704031\n","Test Loss:  72.34963096817955\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:13<01:41,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  320.7391479415819\n","Test Loss:  74.38233278598636\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:16<01:38,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  327.665006140247\n","Test Loss:  72.00348335877061\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:20<01:42,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  324.5478501068428\n","Test Loss:  76.42322357092053\n","Early stopping with best_loss:  70.46873822901398 and test_loss for this epoch:  76.42322357092053 ...\n","mae_validation:\n","1.0398514899895457\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:52,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  690.7214029282331\n","Test Loss:  124.71358310803771\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:49,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  484.3457049187273\n","Test Loss:  104.12015171721578\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:46,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  430.88329952792265\n","Test Loss:  88.58109753578901\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:44,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  389.23856244795024\n","Test Loss:  91.09270052891225\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:42,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  375.3576488690451\n","Test Loss:  86.36862328834832\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:40,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  361.87404492567293\n","Test Loss:  83.7929147016257\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:36,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  353.285220050253\n","Test Loss:  94.10272609023377\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:29<02:34,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  348.5919187832624\n","Test Loss:  77.52695212420076\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:32<02:29,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  344.33252429030836\n","Test Loss:  83.29113895911723\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:36<02:25,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  342.7615709071979\n","Test Loss:  76.19116853550076\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:39<02:20,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  338.94823115319014\n","Test Loss:  77.96700344327837\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:43<02:16,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  331.5356814619154\n","Test Loss:  75.18268538359553\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:46<02:12,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  336.3882659068331\n","Test Loss:  75.24085586890578\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:50<02:08,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  328.5456976778805\n","Test Loss:  78.83690219651908\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:53<02:04,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  324.1964251855388\n","Test Loss:  77.19625896029174\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:57<02:00,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  329.19602292217314\n","Test Loss:  72.95790100283921\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:00<01:56,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  326.032667754218\n","Test Loss:  83.03008495364338\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:04<01:52,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  326.0866098101251\n","Test Loss:  79.05574023677036\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:07<01:49,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  321.5869191940874\n","Test Loss:  76.88490734109655\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:11<01:46,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  324.49708229862154\n","Test Loss:  84.42711492814124\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:15<01:52,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  318.18053411506116\n","Test Loss:  75.84211039915681\n","Early stopping with best_loss:  72.95790100283921 and test_loss for this epoch:  75.84211039915681 ...\n","mae_validation:\n","1.0716645375761509\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:53,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  638.0713813407347\n","Test Loss:  96.14588617905974\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:52,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  459.7134431991726\n","Test Loss:  91.1441641561687\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:48,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  417.68498618435115\n","Test Loss:  81.35191554576159\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:44,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  387.5218123286031\n","Test Loss:  98.75375800766051\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:42,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  368.6129473121837\n","Test Loss:  75.74101149663329\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:38,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  363.8219215236604\n","Test Loss:  69.86449679872021\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:34,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  361.68503648974\n","Test Loss:  91.37807200010866\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:30,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  354.5424067527056\n","Test Loss:  68.68643450108357\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:32<02:27,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  350.3545846492052\n","Test Loss:  83.56959597021341\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:35<02:24,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  345.6644032364711\n","Test Loss:  89.51719480892643\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:39<02:20,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  347.71201417874545\n","Test Loss:  70.29928687773645\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:43<02:18,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  342.23198053240776\n","Test Loss:  69.11881457082927\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:46<02:28,  3.91s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  335.63239647774026\n","Test Loss:  70.60656123282388\n","Early stopping with best_loss:  68.68643450108357 and test_loss for this epoch:  70.60656123282388 ...\n","mae_validation:\n","1.0214469138776225\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:04,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  620.8070455305278\n","Test Loss:  86.34771363437176\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:03,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  439.23719889391214\n","Test Loss:  91.50071280170232\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:59,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  412.5462040049024\n","Test Loss:  78.18741698004305\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<02:55,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  398.1261553890072\n","Test Loss:  73.82103978935629\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:19<02:51,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  371.1453790087253\n","Test Loss:  81.28051944449544\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:47,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  364.2692396398634\n","Test Loss:  81.26476876530796\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:42,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  365.8424390698783\n","Test Loss:  73.90778208244592\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:30<02:39,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  351.4680688432418\n","Test Loss:  73.66609409544617\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:34<02:36,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  352.80323726683855\n","Test Loss:  85.03141612932086\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:38<02:33,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  345.18273871811107\n","Test Loss:  75.7033981056884\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:42<02:30,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  341.3855011276901\n","Test Loss:  69.29484306741506\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:45<02:25,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  341.9780093161389\n","Test Loss:  85.56841061543673\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:49<02:21,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  340.18263300321996\n","Test Loss:  69.83593791536987\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:53<02:17,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  333.7661857912317\n","Test Loss:  67.97444765921682\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:57<02:14,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  328.4076895345934\n","Test Loss:  67.13602042011917\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [01:01<02:10,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  331.3148035136983\n","Test Loss:  74.08746001683176\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:04<02:06,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  329.2564499787986\n","Test Loss:  66.0159314358607\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:08<02:03,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  328.40875573363155\n","Test Loss:  65.9305003657937\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:12<02:00,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  322.5085963876918\n","Test Loss:  69.8986762072891\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:16<01:56,  3.89s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  321.1892730621621\n","Test Loss:  66.1647287439555\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:20<01:53,  3.92s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  317.3711157203652\n","Test Loss:  69.27135346876457\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:24<01:49,  3.93s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  315.4688241467811\n","Test Loss:  65.94827964622527\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:28<01:45,  3.92s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  315.95348133705556\n","Test Loss:  63.85548828821629\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:32<01:41,  3.89s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  312.3302688691765\n","Test Loss:  67.00472594983876\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:36<01:36,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  317.0833911653608\n","Test Loss:  64.16204780712724\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:39<01:32,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  314.1223134100437\n","Test Loss:  72.14801447186619\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:43<01:28,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  310.377587325871\n","Test Loss:  64.71691136294976\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:47<01:31,  3.99s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  314.7266588206403\n","Test Loss:  64.22637995984405\n","Early stopping with best_loss:  63.85548828821629 and test_loss for this epoch:  64.22637995984405 ...\n","mae_validation:\n","0.9661878931533057\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:46,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  750.12350727804\n","Test Loss:  106.48062992841005\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:43,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  497.80886869877577\n","Test Loss:  91.74064523074776\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:38,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  427.3930424069986\n","Test Loss:  87.24852730240673\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:34,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  392.66308797709644\n","Test Loss:  81.90335478633642\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:31,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  372.8837129762396\n","Test Loss:  88.1314391437918\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:29,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  369.775682955049\n","Test Loss:  75.23060956737027\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:26,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  352.85219403449446\n","Test Loss:  76.69353046780452\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:23,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  344.3709735805169\n","Test Loss:  78.87414633948356\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:19,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  339.77110800426453\n","Test Loss:  75.80954598076642\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:15,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  344.15103678777814\n","Test Loss:  74.23802803456783\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:12,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  338.92959206271917\n","Test Loss:  75.87062134873122\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:09,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  331.93429426755756\n","Test Loss:  70.22176928445697\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:44<02:06,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  329.0868539093062\n","Test Loss:  74.58558821724728\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:47<02:03,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  327.968434529379\n","Test Loss:  69.88658091798425\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:51<02:00,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  323.3704768270254\n","Test Loss:  73.74368344899267\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:54<01:56,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  317.77816539723426\n","Test Loss:  70.56696875300258\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:57<01:52,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  328.10376931820065\n","Test Loss:  72.96049336809665\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:01<01:49,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  315.30251316865906\n","Test Loss:  69.95625007431954\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:04<01:45,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.2749217323726\n","Test Loss:  68.99250294361264\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:08<01:43,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  316.33763505425304\n","Test Loss:  68.85646561533213\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:11<01:39,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  312.0420438577421\n","Test Loss:  77.33953584777191\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:14<01:36,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  308.7389295063913\n","Test Loss:  69.11403451440856\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:18<01:32,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.5836545289494\n","Test Loss:  67.46059549087659\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:21<01:28,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  310.4428201187402\n","Test Loss:  66.95093068713322\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:25<01:25,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  311.64912468753755\n","Test Loss:  72.71973906643689\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:28<01:22,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  310.43708288727794\n","Test Loss:  67.09933952754363\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:32<01:19,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  304.5008741877973\n","Test Loss:  72.97461710358039\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:35<01:15,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  306.81821973994374\n","Test Loss:  82.86479002051055\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:38<01:17,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  295.87344531714916\n","Test Loss:  67.36715581035241\n","Early stopping with best_loss:  66.95093068713322 and test_loss for this epoch:  67.36715581035241 ...\n","mae_validation:\n","1.0201403382505747\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:51,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  652.2151186037809\n","Test Loss:  113.13415844738483\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:48,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  451.7211757339537\n","Test Loss:  93.04632142791525\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:46,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  400.5614159912802\n","Test Loss:  90.63667652383447\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:43,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  375.3192541664466\n","Test Loss:  88.4025347866118\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:39,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  362.1846141475253\n","Test Loss:  80.08621731214225\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:35,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  340.75685947109014\n","Test Loss:  82.60998793318868\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:31,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  338.5392581881024\n","Test Loss:  89.28075398132205\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:27,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  331.8418472111225\n","Test Loss:  101.3776021040976\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:23,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  327.7103003030643\n","Test Loss:  85.2652866160497\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:35<02:20,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  318.81724080536515\n","Test Loss:  77.80660428665578\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:38<02:16,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  322.6948095038533\n","Test Loss:  79.47594100236893\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:42<02:13,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  316.6969557274133\n","Test Loss:  86.84957492910326\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:45<02:10,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  318.9044502847828\n","Test Loss:  75.10515102837235\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:49<02:08,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  310.88712488720194\n","Test Loss:  83.8928361586295\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:53<02:04,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  310.5987006071955\n","Test Loss:  73.13770726753864\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:56<02:02,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  302.9312937250361\n","Test Loss:  71.30568477325141\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:00<01:58,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  308.4556262809783\n","Test Loss:  71.49108008481562\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:03<01:54,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  302.8438909116667\n","Test Loss:  71.21247104019858\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:07<01:50,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  302.8333010855131\n","Test Loss:  71.492423618678\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:10<01:46,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  298.92300754692405\n","Test Loss:  70.05236633028835\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:14<01:42,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  298.58789430279285\n","Test Loss:  69.5023937150836\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:17<01:38,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  297.62711544521153\n","Test Loss:  77.39901529811323\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:21<01:35,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  292.67840277124196\n","Test Loss:  68.77354629896581\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:24<01:31,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  293.0622099605389\n","Test Loss:  81.79791716858745\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:28<01:28,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  294.5044632870704\n","Test Loss:  69.13131196517497\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:32<01:25,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  288.5003139367327\n","Test Loss:  70.54613691032864\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:35<01:21,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  292.7484217463061\n","Test Loss:  72.55508195562288\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:39<01:24,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  288.7286961618811\n","Test Loss:  69.53489386569709\n","Early stopping with best_loss:  68.77354629896581 and test_loss for this epoch:  69.53489386569709 ...\n","mae_validation:\n","1.033480754967065\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:00,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  622.1054097879678\n","Test Loss:  99.7225312339142\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:56,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  424.6369941281155\n","Test Loss:  97.85846085473895\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:52,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  386.2173060378991\n","Test Loss:  81.02074305340648\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:47,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  360.2571722571738\n","Test Loss:  87.28893180936575\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:41,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  360.768667337019\n","Test Loss:  78.25475699268281\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:36,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  346.7345067979768\n","Test Loss:  81.98116065375507\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:33,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  334.9948726501316\n","Test Loss:  73.27496546972543\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:29,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  340.15902486792766\n","Test Loss:  78.10283734556288\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:32<02:24,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  326.8559745668899\n","Test Loss:  77.12708240235224\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:35<02:20,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  334.57676475914195\n","Test Loss:  74.3404724500142\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:39<02:17,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  327.14289073226973\n","Test Loss:  76.1639301571995\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:42<02:31,  3.89s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  329.04621808137745\n","Test Loss:  75.1751342844218\n","Early stopping with best_loss:  73.27496546972543 and test_loss for this epoch:  75.1751342844218 ...\n","mae_validation:\n","1.088425059293951\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:04,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  610.2200049459934\n","Test Loss:  93.39376239106059\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:01,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  433.03008201532066\n","Test Loss:  71.83370534516871\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:57,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  398.2763342773542\n","Test Loss:  70.0213387478143\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<02:55,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  375.7729119020514\n","Test Loss:  69.30504138115793\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:19<02:51,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  363.7667436664924\n","Test Loss:  68.10886744130403\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:47,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  355.200231351424\n","Test Loss:  67.6989932297729\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:43,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  350.729387921514\n","Test Loss:  84.0439033061266\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:30<02:39,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  350.6891873138957\n","Test Loss:  65.72230162285268\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:34<02:35,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  341.15932088531554\n","Test Loss:  65.7560206335038\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:37<02:31,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  334.52556866500527\n","Test Loss:  67.37043825723231\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:41<02:27,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  333.07908679544926\n","Test Loss:  66.1312365597114\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:45<02:24,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  338.3065174110234\n","Test Loss:  66.21218732371926\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:49<02:20,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  326.078948832117\n","Test Loss:  64.5259867887944\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:53<02:16,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  323.9975232440047\n","Test Loss:  68.10797512438148\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:56<02:13,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  326.0364818726666\n","Test Loss:  67.3823251859285\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [01:00<02:09,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  323.9045803574845\n","Test Loss:  70.25740273669362\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:04<02:05,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  319.2688215309754\n","Test Loss:  86.74068085476756\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:08<02:12,  4.02s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  316.7330379083287\n","Test Loss:  70.67981302272528\n","Early stopping with best_loss:  64.5259867887944 and test_loss for this epoch:  70.67981302272528 ...\n","mae_validation:\n","1.027142270937935\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:40,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  702.2402559914626\n","Test Loss:  89.23206890979782\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:38,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  479.5228766351938\n","Test Loss:  96.18317684903741\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:34,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  428.14203709270805\n","Test Loss:  75.40115200076252\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:33,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  391.6964006666094\n","Test Loss:  75.986566696316\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:32,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  362.88283534767106\n","Test Loss:  72.01151011511683\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:28,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  358.02654481027275\n","Test Loss:  70.0453737587668\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:25,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  344.3280385355465\n","Test Loss:  67.48208313714713\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:22,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  342.0124591817148\n","Test Loss:  68.9951142007485\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:19,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  344.61081349942833\n","Test Loss:  66.83415604522452\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:14,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  334.2184266485274\n","Test Loss:  69.42126387171447\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:36<02:11,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  340.4137251540087\n","Test Loss:  67.19742128159851\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:07,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  328.15835008909926\n","Test Loss:  66.94291775394231\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:03,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  323.3482550201006\n","Test Loss:  68.77726738061756\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:46<01:59,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  325.34003329253756\n","Test Loss:  62.02226629666984\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:50<01:56,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  313.351369892247\n","Test Loss:  64.80545673891902\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:53<01:52,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  316.49069373495877\n","Test Loss:  63.34963268134743\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:56<01:49,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  317.525477666175\n","Test Loss:  63.86443596705794\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:00<01:46,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  320.55355332326144\n","Test Loss:  64.77628046413884\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:03<01:52,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  310.56743359938264\n","Test Loss:  64.22097907029092\n","Early stopping with best_loss:  62.02226629666984 and test_loss for this epoch:  64.22097907029092 ...\n","mae_validation:\n","0.980910137840494\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:45,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  650.7756235823035\n","Test Loss:  96.73601925186813\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:41,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  455.0525144506246\n","Test Loss:  81.05400718282908\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:37,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  393.25956640485674\n","Test Loss:  74.29932444728911\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:34,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  376.093431673944\n","Test Loss:  75.7250850237906\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:31,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  348.0328633096069\n","Test Loss:  75.8020356008783\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:28,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  335.3343984312378\n","Test Loss:  76.88898604735732\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:25,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  334.69968710048124\n","Test Loss:  69.0988855548203\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:22,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  328.9901412380859\n","Test Loss:  74.49076795880683\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:18,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  323.8962707724422\n","Test Loss:  79.48221298260614\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:15,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  314.7234020009637\n","Test Loss:  66.52139516733587\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:13,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  318.43550608633086\n","Test Loss:  66.41487333085388\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:09,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  312.9562984062359\n","Test Loss:  72.9390344247222\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:44<02:07,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  301.3830881388858\n","Test Loss:  64.98674022546038\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:47<02:03,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  301.5165003570728\n","Test Loss:  66.18826825916767\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:51<02:00,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  308.94289584551007\n","Test Loss:  65.95717510487884\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:54<01:55,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  302.5182736455463\n","Test Loss:  70.95703321159817\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:57<01:51,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  311.65065440442413\n","Test Loss:  67.31128498027101\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:01<01:48,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  299.0094959959388\n","Test Loss:  63.24418522929773\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:04<01:44,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  294.2006387510337\n","Test Loss:  64.78435574565083\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:07<01:41,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  297.02967271069065\n","Test Loss:  63.22856649616733\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:11<01:38,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  290.8868954172358\n","Test Loss:  69.05350660160184\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:14<01:34,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  284.4777845386416\n","Test Loss:  61.49832207709551\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:18<01:31,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  288.0188631983474\n","Test Loss:  61.85996650811285\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:21<01:27,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  286.38789200317115\n","Test Loss:  63.21583198802546\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:24<01:24,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  285.3289020508528\n","Test Loss:  62.25200504111126\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:28<01:21,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  282.62597296759486\n","Test Loss:  60.68191082589328\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:31<01:17,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  288.78664195304736\n","Test Loss:  60.40374549361877\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:35<01:14,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  278.8855826072395\n","Test Loss:  63.79388921754435\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:38<01:11,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  280.1210604731459\n","Test Loss:  62.08147739432752\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:41<01:07,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  271.76672527519986\n","Test Loss:  59.730221080593765\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:45<01:04,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  278.3873348394409\n","Test Loss:  59.4413138369564\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:48<01:00,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  278.6010405216366\n","Test Loss:  62.52903243061155\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:51<00:57,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  275.3285129731521\n","Test Loss:  57.38966762647033\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:55<00:54,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  271.5780992163345\n","Test Loss:  58.620686972513795\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:58<00:51,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  270.69867651117966\n","Test Loss:  59.048578697256744\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:02<00:47,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  268.86075815418735\n","Test Loss:  62.566723516210914\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:05<00:44,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  273.839918543119\n","Test Loss:  58.913696131668985\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:08<00:45,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  267.42059186287224\n","Test Loss:  63.96883612964302\n","Early stopping with best_loss:  57.38966762647033 and test_loss for this epoch:  63.96883612964302 ...\n","mae_validation:\n","0.9335543110311099\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:49,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  603.2880457472056\n","Test Loss:  101.85213582310826\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:52,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  423.12331219483167\n","Test Loss:  83.8906297441572\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:47,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  381.6844308665022\n","Test Loss:  81.76219237642363\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:41,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  356.93691058829427\n","Test Loss:  92.06517556123435\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:36,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  348.8194515518844\n","Test Loss:  77.90949004492722\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:31,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  336.58448437508196\n","Test Loss:  73.9787362376228\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:30,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  324.9480308499187\n","Test Loss:  74.10645063687116\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:27,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  317.2650336008519\n","Test Loss:  75.93461976549588\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:23,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  312.12728081503883\n","Test Loss:  80.05014578066766\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:34<02:19,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  315.9261270388961\n","Test Loss:  75.19876887416467\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:38<02:33,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  307.0784571315162\n","Test Loss:  93.507838210091\n","Early stopping with best_loss:  73.9787362376228 and test_loss for this epoch:  93.507838210091 ...\n","mae_validation:\n","1.1148239769642927\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:03,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  574.9628658406436\n","Test Loss:  88.7245834171772\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:58,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  403.23119860515\n","Test Loss:  94.59198015276343\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:57,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  377.7631998239085\n","Test Loss:  111.21337528899312\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<02:53,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  355.7935667419806\n","Test Loss:  82.55586299812421\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:48,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  351.9943496538326\n","Test Loss:  74.36487056221813\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:45,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  332.28053139708936\n","Test Loss:  68.18930155597627\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:41,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  331.7634238139726\n","Test Loss:  67.93987924605608\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:30<02:37,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  323.604816660285\n","Test Loss:  73.59225506335497\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:33<02:34,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  320.4128965160344\n","Test Loss:  68.39133235160261\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:37<02:30,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  311.1804410023615\n","Test Loss:  69.0553999422118\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:41<02:25,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  317.94680427294225\n","Test Loss:  70.25495978444815\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:45<02:39,  4.09s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  310.3033777093515\n","Test Loss:  68.07346221152693\n","Early stopping with best_loss:  67.93987924605608 and test_loss for this epoch:  68.07346221152693 ...\n","mae_validation:\n","1.0545220140237346\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:40,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  661.365921116434\n","Test Loss:  106.18393931165338\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:35,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  464.03906930983067\n","Test Loss:  94.16095059271902\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:31,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  406.865988553036\n","Test Loss:  85.62708193343133\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:29,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  373.0156266638078\n","Test Loss:  92.52676405012608\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:27,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  358.9227368282154\n","Test Loss:  77.14993485156447\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:24,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  338.15778782684356\n","Test Loss:  75.24563943594694\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:22<02:19,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  338.12688168510795\n","Test Loss:  82.88464026059955\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:17,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  329.426628947258\n","Test Loss:  71.35189565829933\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:29<02:13,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  316.8827709844336\n","Test Loss:  72.81070043100044\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:32<02:09,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  315.93671944225207\n","Test Loss:  72.19996583461761\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:35<02:05,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  310.61076007410884\n","Test Loss:  72.13505830615759\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:38<02:03,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  307.272446998395\n","Test Loss:  66.53844802151434\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:42<01:59,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  305.48576964624226\n","Test Loss:  66.51305854227394\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:45<01:55,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  303.7723882854916\n","Test Loss:  66.87588786287233\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:48<01:52,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  302.8647428485565\n","Test Loss:  67.69750136882067\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:51<01:49,  3.21s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  302.4178031133488\n","Test Loss:  67.81497508380562\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:55<01:46,  3.22s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  290.679182684049\n","Test Loss:  63.5976585149765\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [00:58<01:42,  3.20s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  290.7808009162545\n","Test Loss:  70.26410797145218\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:01<01:40,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  294.32500661863014\n","Test Loss:  64.03089982550591\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:04<01:37,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  281.05790297687054\n","Test Loss:  63.7340945713222\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:07<01:33,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  289.097614957951\n","Test Loss:  66.20629788003862\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:11<01:38,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  287.94656171416864\n","Test Loss:  68.0806097611785\n","Early stopping with best_loss:  63.5976585149765 and test_loss for this epoch:  68.0806097611785 ...\n","mae_validation:\n","1.0188535979916666\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:44,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  619.8689607772976\n","Test Loss:  95.55400236789137\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:39,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  434.7983580902219\n","Test Loss:  81.54804757609963\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:35,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  391.65712550561875\n","Test Loss:  86.96814296767116\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:32,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  351.01448616199195\n","Test Loss:  77.44341667182744\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:29,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  337.96036018338054\n","Test Loss:  69.06571632763371\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:25,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  337.9993029301986\n","Test Loss:  77.56037190556526\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:22,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  323.7698046709411\n","Test Loss:  76.67586230114102\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:18,  3.30s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  320.05940950941294\n","Test Loss:  66.31688760523684\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:29<02:15,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  313.4210983365774\n","Test Loss:  62.68190350010991\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:13,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  309.38056259416044\n","Test Loss:  77.36341555044055\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:36<02:10,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  303.5267720143311\n","Test Loss:  61.159910566173494\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:39<02:07,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  304.2333686109632\n","Test Loss:  59.19762119348161\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:04,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  298.13910734886304\n","Test Loss:  64.13941921666265\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:46<02:00,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  300.88705178536475\n","Test Loss:  65.80246321856976\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:50<01:59,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  295.6834092428908\n","Test Loss:  62.65276075527072\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:53<01:56,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  287.138800740242\n","Test Loss:  65.6846568249166\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:56<02:01,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  285.19975896086544\n","Test Loss:  76.22859796136618\n","Early stopping with best_loss:  59.19762119348161 and test_loss for this epoch:  76.22859796136618 ...\n","mae_validation:\n","0.9743725825944333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:46,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  566.0592245571315\n","Test Loss:  89.36274063494056\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:45,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  407.43714243825525\n","Test Loss:  80.02760720346123\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:40,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  367.4423277191818\n","Test Loss:  78.71835771389306\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:36,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  342.28560502035543\n","Test Loss:  73.5753973275423\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:31,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  334.1188428197056\n","Test Loss:  76.5935588516295\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:27,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  322.6991215692833\n","Test Loss:  70.05030276160687\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:22,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  312.6943322136067\n","Test Loss:  83.71246287506074\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:19,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  309.07325736805797\n","Test Loss:  71.81231796555221\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:17,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  299.68912567477673\n","Test Loss:  72.19909720215946\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:14,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  307.93760780151933\n","Test Loss:  65.62248575594276\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:11,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  299.0105535965413\n","Test Loss:  64.27212612051517\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:06,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  292.3690808303654\n","Test Loss:  68.04957551741973\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:03,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  291.8888821126893\n","Test Loss:  66.66154634952545\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:46<01:59,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  290.62808793457225\n","Test Loss:  68.32297344133258\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:50<01:56,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  282.6153238043189\n","Test Loss:  65.62220314517617\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:53<02:05,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  279.9923919066787\n","Test Loss:  65.81092772074044\n","Early stopping with best_loss:  64.27212612051517 and test_loss for this epoch:  65.81092772074044 ...\n","mae_validation:\n","1.0346022143031124\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:04,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  543.2499540662393\n","Test Loss:  94.31038876716048\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:00,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  399.75277437642217\n","Test Loss:  88.71112721413374\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:56,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  346.23421555943787\n","Test Loss:  75.28858095221221\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:50,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  338.64242739230394\n","Test Loss:  89.09742209361866\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:47,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  323.31686774268746\n","Test Loss:  69.6590029252693\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:43,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  318.82756078150123\n","Test Loss:  70.32637234032154\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:40,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  308.7921952055767\n","Test Loss:  76.85495728161186\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:29<02:37,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  306.82461972907186\n","Test Loss:  70.05608775373548\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:33<02:32,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.22384717687964\n","Test Loss:  69.18648446630687\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:37<02:28,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  293.6243438841775\n","Test Loss:  70.38605537777767\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:40<02:25,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  288.8952965904027\n","Test Loss:  64.70726850256324\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:44<02:21,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  290.1857341849245\n","Test Loss:  65.73118954105303\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:48<02:17,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  292.41390488576144\n","Test Loss:  62.16191313136369\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:52<02:13,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  287.10386821068823\n","Test Loss:  68.02539581013843\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:55<02:09,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  286.9063192959875\n","Test Loss:  67.14145632367581\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:59<02:05,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  276.1623801118694\n","Test Loss:  70.63118069246411\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:03<02:01,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  281.4471016312018\n","Test Loss:  63.32809694297612\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:06<01:58,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  274.06880202982575\n","Test Loss:  61.20836869580671\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:10<01:54,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  280.7460394008085\n","Test Loss:  67.82360532600433\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:14<01:51,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  279.92292025871575\n","Test Loss:  62.3922105114907\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:18<01:48,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  280.63378548435867\n","Test Loss:  66.36795746348798\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:21<01:43,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  271.52960616350174\n","Test Loss:  64.82136764144525\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:25<01:48,  3.88s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  263.7580164009705\n","Test Loss:  62.72511661378667\n","Early stopping with best_loss:  61.20836869580671 and test_loss for this epoch:  62.72511661378667 ...\n","mae_validation:\n","0.9995372339028361\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:51,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  1037.9830073788762\n","Test Loss:  150.69778342917562\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:48,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  675.4280960857868\n","Test Loss:  122.62828133255243\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:46,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  567.4242927208543\n","Test Loss:  112.590254527051\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:42,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  491.6753442399204\n","Test Loss:  106.41652837395668\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:40,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  473.1456493260339\n","Test Loss:  102.13385869190097\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:35,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  453.71606148406863\n","Test Loss:  100.30834614951164\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:33,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  436.3679552273825\n","Test Loss:  96.21054688235745\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:28,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  424.90343487728387\n","Test Loss:  95.10430399700999\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:24,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  404.9938434786163\n","Test Loss:  92.69085225462914\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:35<02:22,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  394.42597689758986\n","Test Loss:  91.15415586624295\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:39<02:19,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  395.5222567678429\n","Test Loss:  94.32755603035912\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:42<02:14,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  387.50072629004717\n","Test Loss:  89.52479055617005\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:46<02:10,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  376.72986555704847\n","Test Loss:  91.46049974579364\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:49<02:07,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  373.1857802923769\n","Test Loss:  85.71432827226818\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:53<02:04,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  366.9355150014162\n","Test Loss:  85.04484605975449\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:56<02:00,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  366.9161931909621\n","Test Loss:  86.4233520696871\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:00<01:56,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  352.36472410615534\n","Test Loss:  84.30613358318806\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:03<01:52,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  355.110480427742\n","Test Loss:  83.59732840489596\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:07<01:48,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  355.7574453819543\n","Test Loss:  82.35224453965202\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:10<01:44,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  346.3534887167625\n","Test Loss:  85.40661692712456\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:14<01:41,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  343.3189505771734\n","Test Loss:  81.79018652113155\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:17<01:38,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  336.9984146421775\n","Test Loss:  79.72869538422674\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:21<01:34,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  342.6634001932107\n","Test Loss:  82.2202254012227\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:24<01:31,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  333.98163793748245\n","Test Loss:  84.20290703186765\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:28<01:27,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  339.9458435289562\n","Test Loss:  79.96180052077398\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:31<01:23,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  329.7561167161912\n","Test Loss:  78.62333367392421\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:35<01:20,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  323.79177286615595\n","Test Loss:  81.97850929852575\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:38<01:16,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  333.79705401230603\n","Test Loss:  81.17848267685622\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:42<01:13,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  328.47807599185035\n","Test Loss:  76.94158635137137\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:45<01:10,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  324.8620985262096\n","Test Loss:  78.28715628152713\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:49<01:06,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  325.6422185432166\n","Test Loss:  78.62620246689767\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:52<01:02,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  321.99806650867686\n","Test Loss:  77.04168822616339\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:56<00:59,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  320.6599762099795\n","Test Loss:  76.38132268597838\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:59<00:55,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  327.11978151556104\n","Test Loss:  79.79819287452847\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:03<00:52,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  320.2617651298642\n","Test Loss:  77.72387354448438\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:06<00:48,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  314.65348955849186\n","Test Loss:  77.67583456356078\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:10<00:45,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  316.48602874716744\n","Test Loss:  77.30917340889573\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:13<00:46,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  318.72376685449854\n","Test Loss:  78.23546055331826\n","Early stopping with best_loss:  76.38132268597838 and test_loss for this epoch:  78.23546055331826 ...\n","mae_validation:\n","1.0766090789062386\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:52,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  944.9985296353698\n","Test Loss:  139.36803651973605\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:51,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  602.4196544587612\n","Test Loss:  110.51793458405882\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:51,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  514.5148022789508\n","Test Loss:  101.78812072554138\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:47,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  482.8687615972012\n","Test Loss:  96.80449897516519\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:43,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  456.0210281368345\n","Test Loss:  93.53530690446496\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:40,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  436.1247263867408\n","Test Loss:  92.79308640118688\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:35,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  427.21910481061786\n","Test Loss:  88.5472252862528\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:32,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  411.0999357467517\n","Test Loss:  85.38173601776361\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:32<02:28,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  404.22944329772145\n","Test Loss:  82.73013219656423\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:36<02:25,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  390.85725207673386\n","Test Loss:  84.55584815144539\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:39<02:21,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  383.5575166316703\n","Test Loss:  80.65886723529547\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:43<02:18,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  374.9343424416147\n","Test Loss:  77.18852479290217\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:47<02:14,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  366.123871141579\n","Test Loss:  81.23970238119364\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:50<02:11,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  361.6783934121486\n","Test Loss:  77.30805335845798\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:54<02:06,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  359.9271656414494\n","Test Loss:  75.55029000714421\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:58<02:03,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  356.00157076399773\n","Test Loss:  74.28307998529635\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:01<01:59,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  347.4446891862899\n","Test Loss:  76.00493413908407\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:05<01:56,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  339.0740500781685\n","Test Loss:  75.29851614916697\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:08<01:52,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  343.58908526645973\n","Test Loss:  71.96661795582622\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:12<01:49,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  339.90991386724636\n","Test Loss:  77.71326354215853\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:16<01:45,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  339.9801053721458\n","Test Loss:  73.19232276733965\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:19<01:40,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  340.47751097544096\n","Test Loss:  72.10767339589074\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:23<01:38,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  332.9518654695712\n","Test Loss:  69.84540639526676\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:27<01:35,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  335.46738819312304\n","Test Loss:  69.62792583880946\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:30<01:31,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  326.57753201946616\n","Test Loss:  79.2261180402711\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:34<01:27,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  329.6001268629916\n","Test Loss:  71.25425649061799\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:37<01:22,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  325.6496329624206\n","Test Loss:  69.68831736873835\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:41<01:19,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  329.74039824446663\n","Test Loss:  70.52706363331527\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:45<01:15,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  327.11920844670385\n","Test Loss:  69.409553270787\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:48<01:12,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  324.79595658322796\n","Test Loss:  69.50152456830256\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:52<01:08,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  326.5434869583696\n","Test Loss:  72.62701718788594\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:56<01:04,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  324.52705645468086\n","Test Loss:  70.06177113112062\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:59<01:01,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  319.41479420661926\n","Test Loss:  69.54648299375549\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [02:03<00:57,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  317.60106721520424\n","Test Loss:  69.40244584996253\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:06<00:54,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  321.94287984538823\n","Test Loss:  69.83945715380833\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:10<00:50,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  315.1159976120107\n","Test Loss:  68.36672543222085\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:14<00:46,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  318.4653283599764\n","Test Loss:  76.24121938087046\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:17<00:43,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  316.7146308692172\n","Test Loss:  68.69850447913632\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:21<00:39,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  310.9248523474671\n","Test Loss:  69.34955671150237\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:24<00:36,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  315.51955816615373\n","Test Loss:  70.30731552280486\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:28<00:37,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  316.2617054078728\n","Test Loss:  69.62692869734019\n","Early stopping with best_loss:  68.36672543222085 and test_loss for this epoch:  69.62692869734019 ...\n","mae_validation:\n","0.9998957419162936\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:00,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  806.7157067619264\n","Test Loss:  133.66116257570684\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:59,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  561.6968012452126\n","Test Loss:  117.68735320027918\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:53,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  497.2034963648766\n","Test Loss:  101.98974802158773\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:48,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  463.41873799450696\n","Test Loss:  94.23430604021996\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:45,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  437.6708227721974\n","Test Loss:  89.27697843266651\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:39,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  423.13409696985036\n","Test Loss:  87.68041657586582\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:34,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  403.42996854521334\n","Test Loss:  84.2333878595382\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:29<02:32,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  393.8286833781749\n","Test Loss:  80.80929237790406\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:32<02:29,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  384.12845575716347\n","Test Loss:  81.13842861680314\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:36<02:25,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  376.18938070582226\n","Test Loss:  78.9359626211226\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:40<02:21,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  362.8496665228158\n","Test Loss:  77.84531294181943\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:43<02:18,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  361.282987665385\n","Test Loss:  76.13043794129044\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:47<02:13,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  357.0080297542736\n","Test Loss:  75.32044147700071\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:50<02:10,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  351.0128668001853\n","Test Loss:  73.7741151638329\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:54<02:07,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  349.6562753817998\n","Test Loss:  72.2923337072134\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:58<02:03,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  340.46238638740033\n","Test Loss:  72.31114239944145\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:01<01:59,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  334.88269006926566\n","Test Loss:  71.15335494047031\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:05<01:56,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  333.8940914198756\n","Test Loss:  70.7900588051416\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:09<01:52,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  331.99308702675626\n","Test Loss:  71.56421509338543\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:12<01:48,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  334.7072772886604\n","Test Loss:  74.2612614640966\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:16<01:45,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  337.2378107253462\n","Test Loss:  69.64444043394178\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:20<01:42,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  324.29371431004256\n","Test Loss:  85.72597581706941\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:23<01:38,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  328.34204329922795\n","Test Loss:  71.59435652499087\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:27<01:34,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  324.73818672588095\n","Test Loss:  69.95072882436216\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:30<01:30,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  317.78784469049424\n","Test Loss:  68.35049863927998\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:34<01:27,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  315.0951583222486\n","Test Loss:  68.4792072493583\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:38<01:24,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  318.1575779498089\n","Test Loss:  71.66770527139306\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:42<01:21,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  313.01642188755795\n","Test Loss:  77.16530406381935\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:45<01:18,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  314.6965635884553\n","Test Loss:  67.63667971175164\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:49<01:13,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  314.9677687911317\n","Test Loss:  67.32930738013238\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:53<01:09,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  312.9435530640185\n","Test Loss:  69.67049359856173\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:56<01:06,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  313.47645171312615\n","Test Loss:  68.66792472917587\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [02:00<01:02,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  309.9496553763747\n","Test Loss:  67.5934076118283\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [02:04<00:59,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  313.68001152947545\n","Test Loss:  68.99612305825576\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:07<00:55,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  306.8636737782508\n","Test Loss:  66.52550050988793\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:11<00:51,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  303.12404681928456\n","Test Loss:  67.25205180514604\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:15<00:47,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  306.28720540646464\n","Test Loss:  69.20962056796998\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:18<00:43,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  299.4557561967522\n","Test Loss:  69.75313224783167\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:22<00:39,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  301.96253385115415\n","Test Loss:  66.41976592782885\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:25<00:36,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  296.63369778729975\n","Test Loss:  69.1341923577711\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:29<00:32,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  297.81314400350675\n","Test Loss:  67.0828818641603\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:33<00:29,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  299.89796209009364\n","Test Loss:  66.52330807643011\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:36<00:25,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  298.4207277111709\n","Test Loss:  66.35931271640584\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:40<00:21,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  293.66970242746174\n","Test Loss:  70.70701757073402\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:44<00:18,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  297.5177714433521\n","Test Loss:  66.18686231132597\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 92%|█████████▏| 46/50 [02:47<00:14,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  294.23967202194035\n","Test Loss:  64.97922843322158\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 94%|█████████▍| 47/50 [02:51<00:10,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  288.53946306789294\n","Test Loss:  69.98929165303707\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 96%|█████████▌| 48/50 [02:55<00:07,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  292.7347318623215\n","Test Loss:  65.26881082449108\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 49/50 [02:58<00:03,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  289.14493099274114\n","Test Loss:  64.38594180624932\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 50/50 [03:02<00:00,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  292.49111962737516\n","Test Loss:  64.46705958247185\n","mae_validation:\n","0.9618972454343567\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:04<03:16,  4.00s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  769.3450218979269\n","Test Loss:  133.22854175418615\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:11,  3.99s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  523.5251229861751\n","Test Loss:  107.28717691451311\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<03:05,  3.94s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  467.07727660797536\n","Test Loss:  100.70680843107402\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<03:01,  3.95s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  425.606457689777\n","Test Loss:  91.32971644960344\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:19<02:57,  3.95s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  412.41463884338737\n","Test Loss:  87.9841976230964\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:23<02:53,  3.93s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  397.08448486728594\n","Test Loss:  87.06566682690755\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:27<02:49,  3.94s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  378.1770257120952\n","Test Loss:  80.95419532060623\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:31<02:45,  3.93s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  367.645119914785\n","Test Loss:  78.96123675722629\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:35<02:40,  3.91s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  358.1609733197838\n","Test Loss:  79.9789242837578\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:39<02:36,  3.90s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  352.86066522449255\n","Test Loss:  78.90785707626492\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:43<02:32,  3.91s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  354.5384592823684\n","Test Loss:  89.10688132978976\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:47<02:28,  3.90s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  348.662615084555\n","Test Loss:  76.76715106377378\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:50<02:24,  3.89s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  337.6911550825462\n","Test Loss:  76.41138915577903\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:54<02:20,  3.90s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  326.6038839071407\n","Test Loss:  73.91044243844226\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:58<02:16,  3.89s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  330.6616116138175\n","Test Loss:  82.46083299349993\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [01:02<02:12,  3.90s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  328.64884145045653\n","Test Loss:  86.91882498189807\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:06<02:09,  3.92s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  323.5984337644186\n","Test Loss:  76.11453309748322\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:10<02:05,  3.92s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  328.1799840554595\n","Test Loss:  72.4934741826728\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:14<02:00,  3.89s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  320.2330206865445\n","Test Loss:  75.82541492627934\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:18<01:57,  3.91s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  319.5170667413622\n","Test Loss:  72.2100149015896\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:22<01:53,  3.91s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  318.22876606695354\n","Test Loss:  74.46127688651904\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:26<01:49,  3.91s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  312.60760099301115\n","Test Loss:  71.04233235958964\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:30<01:45,  3.91s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  312.758878512308\n","Test Loss:  73.30533895269036\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:33<01:41,  3.89s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  310.2627111179754\n","Test Loss:  69.32716954080388\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:37<01:37,  3.90s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  310.70828841393813\n","Test Loss:  73.88650377746671\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:41<01:33,  3.91s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  302.7519516889006\n","Test Loss:  71.51398714259267\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:45<01:29,  3.89s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  305.48104374296963\n","Test Loss:  69.77074487227947\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:49<01:25,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  306.9117951160297\n","Test Loss:  69.52729362156242\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:53<01:29,  4.05s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  300.9804360740818\n","Test Loss:  69.7132980725728\n","Early stopping with best_loss:  69.32716954080388 and test_loss for this epoch:  69.7132980725728 ...\n","mae_validation:\n","1.0056454791572689\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:47,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  980.8988360501826\n","Test Loss:  146.2880984414369\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:45,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  689.9074956327677\n","Test Loss:  123.69591479748487\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:41,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  560.9330277517438\n","Test Loss:  108.71431592479348\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:39,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  500.2883494347334\n","Test Loss:  106.27555879764259\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:34,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  460.90358407236636\n","Test Loss:  100.4261910431087\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:31,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  450.6404907768592\n","Test Loss:  97.9157805358991\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:27,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  427.1244966676459\n","Test Loss:  93.16825881134719\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:24,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  415.6314429230988\n","Test Loss:  90.96393378125504\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:20,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  396.5248811598867\n","Test Loss:  92.2197949718684\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:34<02:17,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  388.23139877803624\n","Test Loss:  86.70277803111821\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:15,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  375.93955051805824\n","Test Loss:  84.46579822199419\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:41<02:12,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  381.1842817114666\n","Test Loss:  85.24936560029164\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:44<02:08,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  372.04446767270565\n","Test Loss:  82.00047130743042\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:48<02:06,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  359.415896368213\n","Test Loss:  80.10878914222121\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:51<02:02,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  350.55132129928097\n","Test Loss:  82.23141575790942\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:55<01:58,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  344.7115097111091\n","Test Loss:  79.97749259322882\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:58<01:55,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  346.87042859429494\n","Test Loss:  84.53422202728689\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:02<01:51,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  338.5013812938705\n","Test Loss:  78.22841322235763\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:05<01:47,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  335.8658703714609\n","Test Loss:  82.44535471964628\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:09<01:43,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  333.63265086943284\n","Test Loss:  76.13298285671044\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:12<01:40,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  330.8962407563813\n","Test Loss:  76.75625126017258\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:16<01:36,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  319.8248376958072\n","Test Loss:  80.616574363783\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:19<01:33,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  327.98773660697043\n","Test Loss:  76.83595175202936\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:23<01:29,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  320.34172085719183\n","Test Loss:  79.50137884356081\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:26<01:33,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  323.03738911193796\n","Test Loss:  76.63873760262504\n","Early stopping with best_loss:  76.13298285671044 and test_loss for this epoch:  76.63873760262504 ...\n","mae_validation:\n","1.1056588983903026\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:54,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  885.0792256221175\n","Test Loss:  146.18019145727158\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:50,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  596.9067475367337\n","Test Loss:  121.57485039159656\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:47,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  508.1614473350346\n","Test Loss:  121.31949104368687\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:43,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  462.50846638390794\n","Test Loss:  108.31417690427043\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:38,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  441.2811351502314\n","Test Loss:  105.20895321946591\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:35,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  426.94611096102744\n","Test Loss:  100.6188528323546\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:32,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  413.8005826831795\n","Test Loss:  97.76412367448211\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:28,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  383.52619670145214\n","Test Loss:  102.83171316795051\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:25,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  367.85578743042424\n","Test Loss:  97.04175248369575\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:35<02:21,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  366.38408872205764\n","Test Loss:  91.40035256929696\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:38<02:17,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  358.0014912020415\n","Test Loss:  88.51225596107543\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:42<02:13,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  353.98749796068296\n","Test Loss:  87.92485498636961\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:45<02:10,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  352.5469556003809\n","Test Loss:  86.53683793498203\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:49<02:06,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  343.9073360748589\n","Test Loss:  85.64417172688991\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:53<02:03,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  336.7931190831587\n","Test Loss:  84.32807745831087\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:56<01:59,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  328.1847596359439\n","Test Loss:  87.6264146193862\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:00<01:56,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  334.6877613363322\n","Test Loss:  93.32192966062576\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:03<01:52,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  328.0756673058495\n","Test Loss:  85.43434687424451\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:07<01:48,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  321.4506372399628\n","Test Loss:  83.97370063746348\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:10<01:44,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  324.5132319359109\n","Test Loss:  84.77199699636549\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:14<01:41,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  320.28025746019557\n","Test Loss:  80.98616504389793\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:17<01:38,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  319.8890142259188\n","Test Loss:  80.57602319028229\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:21<01:35,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  316.01696938765235\n","Test Loss:  80.7494800593704\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:24<01:32,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  315.0627918411046\n","Test Loss:  81.19349590293132\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:28<01:28,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  315.27816546941176\n","Test Loss:  78.7040551812388\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:31<01:24,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  310.2279693018645\n","Test Loss:  83.30462518148124\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:35<01:21,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  308.922038320452\n","Test Loss:  78.9138566236943\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:38<01:18,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.9626291031018\n","Test Loss:  77.18655387405306\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:42<01:15,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  309.26545933494344\n","Test Loss:  77.89180757384747\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:46<01:11,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  301.81345100002363\n","Test Loss:  76.28069475945085\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:49<01:07,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  306.94297114480287\n","Test Loss:  78.15597915928811\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:53<01:03,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  306.31610748544335\n","Test Loss:  77.30814695497975\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:56<00:59,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  302.0574332457036\n","Test Loss:  77.32139234244823\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [02:00<00:56,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  298.44945685379207\n","Test Loss:  83.52658505737782\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:03<00:52,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  296.53982430230826\n","Test Loss:  76.24730256572366\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:07<00:49,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  296.6391980317421\n","Test Loss:  83.32136241625994\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:10<00:46,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  293.25922226021066\n","Test Loss:  80.91991493664682\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:14<00:42,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  295.39800143428147\n","Test Loss:  75.81581152509898\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:17<00:38,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  290.45270871929824\n","Test Loss:  75.1517113558948\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:21<00:35,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  292.1534973306116\n","Test Loss:  74.37863450590521\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:24<00:31,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  295.1620060522109\n","Test Loss:  73.98431654367596\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:28<00:28,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  289.28887043194845\n","Test Loss:  85.44356378074735\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:31<00:24,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  285.4693045420572\n","Test Loss:  72.68684661202133\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:35<00:21,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  291.55312854330987\n","Test Loss:  74.10048977658153\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:38<00:17,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  288.85040686419234\n","Test Loss:  72.83673326577991\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 92%|█████████▏| 46/50 [02:42<00:13,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  287.3144061774947\n","Test Loss:  78.27313065482304\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 94%|█████████▍| 47/50 [02:45<00:10,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  280.8861939231865\n","Test Loss:  76.96554175857455\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 96%|█████████▌| 48/50 [02:49<00:07,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  283.76275615766644\n","Test Loss:  71.91027164831758\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 49/50 [02:52<00:03,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  288.4932676567696\n","Test Loss:  73.38614889094606\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 50/50 [02:56<00:00,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  283.6625260282308\n","Test Loss:  74.98096506251022\n","mae_validation:\n","1.0593444675943295\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:52,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  797.4416438499466\n","Test Loss:  131.4050775859505\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:48,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  558.9299921113998\n","Test Loss:  101.9217660240829\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:45,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  477.97246986068785\n","Test Loss:  94.4893338996917\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:42,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  446.3784002996981\n","Test Loss:  89.69576790370047\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:39,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  411.3908688323572\n","Test Loss:  83.31309507507831\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:36,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  391.00496488343924\n","Test Loss:  85.74514882918447\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:32,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  378.77738429699093\n","Test Loss:  79.24862055853009\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:28,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  366.88023876957595\n","Test Loss:  79.7417982352199\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:24,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  352.8374597048387\n","Test Loss:  74.27731282450259\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:35<02:21,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  353.85031095705926\n","Test Loss:  77.90815899521112\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:38<02:17,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  343.8638520827517\n","Test Loss:  76.81082170363516\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:42<02:14,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  345.0393702732399\n","Test Loss:  72.29040487669408\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:45<02:10,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  336.7602564021945\n","Test Loss:  72.35531732486561\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:49<02:07,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  335.5602979622781\n","Test Loss:  71.47759484010749\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:53<02:04,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  329.45388760068454\n","Test Loss:  69.25533088762313\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:56<02:00,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  325.95200443896465\n","Test Loss:  71.00084269419312\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:00<01:56,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  326.34275959897786\n","Test Loss:  68.18795971106738\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:03<01:53,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  323.16215369617566\n","Test Loss:  73.19709563814104\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:07<01:49,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  317.311770029366\n","Test Loss:  73.73053938988596\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:10<01:45,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.7130767730996\n","Test Loss:  67.81152405310422\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:14<01:42,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  310.1171517823823\n","Test Loss:  67.44274642551318\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:17<01:38,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  304.9707760582678\n","Test Loss:  67.82819859148003\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:21<01:35,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  305.61638485919684\n","Test Loss:  65.69871798716486\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:24<01:32,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  301.4454349167645\n","Test Loss:  67.81294590653852\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:28<01:29,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  304.6399009921588\n","Test Loss:  65.1724617574364\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:32<01:25,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  303.3380642379634\n","Test Loss:  66.04861270450056\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:35<01:22,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  298.20663306862116\n","Test Loss:  67.72137158736587\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:39<01:18,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  297.66615014057606\n","Test Loss:  67.26063789380714\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:42<01:14,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  294.91954133752733\n","Test Loss:  79.55615259613842\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:46<01:11,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  292.2721206038259\n","Test Loss:  63.87825492024422\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:49<01:07,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  295.3041661255993\n","Test Loss:  64.15116729214787\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:53<01:04,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  291.38023917237297\n","Test Loss:  64.81732852291316\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:57<01:00,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  290.13707628054544\n","Test Loss:  66.13138857670128\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [02:00<00:56,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  285.01079171244055\n","Test Loss:  67.64806849230081\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [02:04<00:58,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  286.12440352281556\n","Test Loss:  64.67433152254671\n","Early stopping with best_loss:  63.87825492024422 and test_loss for this epoch:  64.67433152254671 ...\n","mae_validation:\n","0.9770853588014322\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:07,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  753.4392946530133\n","Test Loss:  114.56488390825689\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:03,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  512.4502385221422\n","Test Loss:  99.16482494678348\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:59,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  454.986457971856\n","Test Loss:  87.93773470632732\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<02:56,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  415.3194613372907\n","Test Loss:  83.25631575495936\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:19<02:52,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  386.3001706280047\n","Test Loss:  82.49156420398504\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:23<02:49,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  368.77869992982596\n","Test Loss:  75.7263020761311\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:45,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  359.97520724637434\n","Test Loss:  72.94370492640883\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:30<02:40,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  356.8912969296798\n","Test Loss:  74.1907562961569\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:34<02:36,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  343.81813671067357\n","Test Loss:  71.36880828347057\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:38<02:33,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  345.6101005175151\n","Test Loss:  74.77515880856663\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:42<02:29,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  333.3132980223745\n","Test Loss:  75.49093501688913\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:45<02:24,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  332.304770974908\n","Test Loss:  70.98736656503752\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:49<02:20,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  328.26929363096133\n","Test Loss:  69.02894902601838\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:53<02:17,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  322.57977765914984\n","Test Loss:  69.46383384987712\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:57<02:13,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  320.03626261185855\n","Test Loss:  68.46697248332202\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [01:01<02:09,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  308.9314594678581\n","Test Loss:  65.84794643055648\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:05<02:06,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  315.8246237265412\n","Test Loss:  69.23602752899751\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:08<02:01,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  310.0844287935179\n","Test Loss:  77.11193546652794\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:12<01:58,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  304.90980475069955\n","Test Loss:  64.95755313686095\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:16<01:54,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  303.7383991810493\n","Test Loss:  67.0845792684704\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:20<01:51,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  300.55293321143836\n","Test Loss:  73.27732434589416\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:24<01:47,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  300.75723200058565\n","Test Loss:  63.425049751996994\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:28<01:43,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  299.4891188829206\n","Test Loss:  67.8552789692767\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:31<01:39,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  295.64103198610246\n","Test Loss:  66.9292351801414\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:35<01:35,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  293.0332117732614\n","Test Loss:  64.01507678907365\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:39<01:31,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  290.2653642585501\n","Test Loss:  65.22185448091477\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:43<01:35,  3.97s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  289.89512544171885\n","Test Loss:  64.12638967251405\n","Early stopping with best_loss:  63.425049751996994 and test_loss for this epoch:  64.12638967251405 ...\n","mae_validation:\n","0.9760736501423338\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:42,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  983.8091291934252\n","Test Loss:  149.7461473736912\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:38,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  699.8864755183458\n","Test Loss:  115.67929818667471\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:37,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  564.5908778775483\n","Test Loss:  110.86359935626388\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:32,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  502.35716965794563\n","Test Loss:  96.20477474760264\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:30,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  467.61662464030087\n","Test Loss:  92.26494639459997\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:26,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  439.603974731639\n","Test Loss:  94.96114845573902\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:24,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  424.3871561232954\n","Test Loss:  90.12019232334569\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:20,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  409.76641012821347\n","Test Loss:  91.7146364748478\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:29<02:16,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  391.3262397851795\n","Test Loss:  78.6456145872362\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:13,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  378.04346184199676\n","Test Loss:  77.92539191443939\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:36<02:11,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  367.40836063213646\n","Test Loss:  76.77972292387858\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:08,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  360.674576388672\n","Test Loss:  75.0019211564213\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:05,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  358.95921047450975\n","Test Loss:  73.8531718342565\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:46<02:01,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  344.8524083402008\n","Test Loss:  78.12234206777066\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:50<01:57,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  340.8370354650542\n","Test Loss:  73.11306806001812\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:53<01:53,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  333.38539937883615\n","Test Loss:  73.31875698710792\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:56<01:49,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  338.0699491496198\n","Test Loss:  76.25952535681427\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:00<01:47,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  326.96159052522853\n","Test Loss:  71.74208803707734\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:03<01:44,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  330.3680836604908\n","Test Loss:  70.7554544354789\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:06<01:40,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  321.948772481177\n","Test Loss:  76.32917280402035\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:10<01:37,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  323.1518701114692\n","Test Loss:  70.1795014552772\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:13<01:33,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  313.5783849209547\n","Test Loss:  76.66415487788618\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:17<01:30,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  313.76141779124737\n","Test Loss:  69.48380486061797\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:20<01:27,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  310.10639750771224\n","Test Loss:  71.56657519005239\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:23<01:23,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  313.6521966555156\n","Test Loss:  69.06133976066485\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:27<01:20,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  307.3466544188559\n","Test Loss:  68.82931489078328\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:30<01:16,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  306.38997510052286\n","Test Loss:  72.30911647062749\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:33<01:13,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  302.6974714510143\n","Test Loss:  69.24430005159229\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:37<01:10,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  299.12952253967524\n","Test Loss:  71.47456822730601\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:40<01:06,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  296.5618063467555\n","Test Loss:  69.28107414068654\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:43<01:09,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  294.6955952786375\n","Test Loss:  69.00040399469435\n","Early stopping with best_loss:  68.82931489078328 and test_loss for this epoch:  69.00040399469435 ...\n","mae_validation:\n","1.0387145950617585\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:46,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  867.5546075589955\n","Test Loss:  130.47550881281495\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:44,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  568.8078816384077\n","Test Loss:  115.79901556298137\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:41,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  494.26194786839187\n","Test Loss:  106.1855847183615\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:37,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  440.6387467663735\n","Test Loss:  97.63150101806968\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:34,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  416.19930293178186\n","Test Loss:  92.38393296021968\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:30,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  390.0866572186351\n","Test Loss:  87.56279157660902\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:26,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  389.42494837287813\n","Test Loss:  86.09135161433369\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:23,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  376.58151203021407\n","Test Loss:  84.84883701661602\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:20,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  361.82803084747866\n","Test Loss:  81.0963737219572\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:34<02:17,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  354.7590561811812\n","Test Loss:  82.0877831839025\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:13,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  348.28506695700344\n","Test Loss:  84.5239494908601\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:41<02:09,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  345.39090237533674\n","Test Loss:  80.17092975135893\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:44<02:06,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  338.6308503244072\n","Test Loss:  76.2642457948532\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:47<02:03,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  326.13625099137425\n","Test Loss:  77.58238031785004\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:51<01:59,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  326.37283121654764\n","Test Loss:  77.0585455801338\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:54<01:55,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  316.38558817468584\n","Test Loss:  80.12792153540067\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:58<01:52,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  316.7099451860413\n","Test Loss:  73.89401631709188\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:01<01:48,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  311.5309080751613\n","Test Loss:  73.45074887108058\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:04<01:44,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  312.80346087552607\n","Test Loss:  72.76734154857695\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:08<01:41,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  309.75156164867803\n","Test Loss:  73.5288605382666\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:11<01:38,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  303.65878823166713\n","Test Loss:  83.56034802272916\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:15<01:35,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  303.662456728518\n","Test Loss:  72.84118807176128\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:18<01:31,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  304.43851595791057\n","Test Loss:  70.64730382524431\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:21<01:29,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  297.5320663927123\n","Test Loss:  70.04389506671578\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:25<01:25,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  293.7776318411343\n","Test Loss:  70.56938469596207\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:28<01:21,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  303.49557539331727\n","Test Loss:  70.07859119586647\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:31<01:17,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  290.73016979172826\n","Test Loss:  71.03064681775868\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:35<01:14,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  296.513735585846\n","Test Loss:  71.26514836633578\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:38<01:11,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  286.301720155403\n","Test Loss:  68.9283815054223\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:42<01:07,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  290.45844780420884\n","Test Loss:  74.30032420088537\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:45<01:04,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  288.77947009075433\n","Test Loss:  68.12564358767122\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:49<01:01,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  287.911318205297\n","Test Loss:  69.46476170606911\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:52<00:57,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  287.0112186288461\n","Test Loss:  68.91120156412944\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:55<00:54,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  281.9638744723052\n","Test Loss:  73.91645819181576\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:59<00:51,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  286.4808095274493\n","Test Loss:  69.15175729990005\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:02<00:47,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  285.198342428077\n","Test Loss:  67.79264274425805\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:06<00:44,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  280.54975017230026\n","Test Loss:  67.57281125336885\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:09<00:41,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  280.4298196081072\n","Test Loss:  70.800506667234\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:12<00:37,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  275.8903727699071\n","Test Loss:  67.96721955947578\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:16<00:33,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  277.17264304868877\n","Test Loss:  68.69088948983699\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:19<00:30,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  280.18539751693606\n","Test Loss:  66.56748506962322\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:23<00:27,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  279.4221069086343\n","Test Loss:  65.67662095860578\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:26<00:24,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  274.45949642709456\n","Test Loss:  65.46640864526853\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:30<00:20,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  273.98081052629277\n","Test Loss:  65.34716071095318\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:33<00:17,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  279.05670514318626\n","Test Loss:  66.31286067096516\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 92%|█████████▏| 46/50 [02:36<00:13,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  276.1966434395872\n","Test Loss:  65.28143125679344\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 94%|█████████▍| 47/50 [02:40<00:10,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  270.74970438238233\n","Test Loss:  66.6698146937415\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 96%|█████████▌| 48/50 [02:43<00:06,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  271.13510908931494\n","Test Loss:  65.60344565892592\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 49/50 [02:47<00:03,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  273.7229601549916\n","Test Loss:  65.11967197339982\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 50/50 [02:50<00:00,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  273.6668709493242\n","Test Loss:  64.93903133017011\n","mae_validation:\n","1.0061596481804151\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:47,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  791.74690480344\n","Test Loss:  138.75849451310933\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:46,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  530.6486075203866\n","Test Loss:  99.21268442645669\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:42,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  461.020167671144\n","Test Loss:  95.74910786584951\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:38,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  430.39277289807796\n","Test Loss:  85.48671588301659\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:34,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  390.93684254027903\n","Test Loss:  82.52334532141685\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:30,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  379.67710059834644\n","Test Loss:  79.32077046157792\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:28,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  362.0680964719504\n","Test Loss:  77.82981736550573\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:24,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  354.6251526493579\n","Test Loss:  75.44117797538638\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:21,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  342.7030525384471\n","Test Loss:  73.5495661702007\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:34<02:18,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  342.30136632267386\n","Test Loss:  73.80307509936392\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:14,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  335.6530907712877\n","Test Loss:  76.11432086210698\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:41<02:10,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  325.4115269300528\n","Test Loss:  71.90856674034148\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:44<02:07,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  326.2930156607181\n","Test Loss:  72.33941847458482\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:48<02:04,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  325.2980348682031\n","Test Loss:  72.47093371395022\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:51<02:01,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  314.99213392613456\n","Test Loss:  68.3147748708725\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:55<01:57,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  310.51162872463465\n","Test Loss:  72.47480710968375\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:58<01:53,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  300.3622457324527\n","Test Loss:  75.67652730480768\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:02<01:50,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  303.57192404707894\n","Test Loss:  70.69548392854631\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:05<01:46,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  300.2446698551066\n","Test Loss:  73.33628384582698\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:08<01:52,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  296.0010633391794\n","Test Loss:  71.28427457530051\n","Early stopping with best_loss:  68.3147748708725 and test_loss for this epoch:  71.28427457530051 ...\n","mae_validation:\n","1.0417945123258923\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:07,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  726.7750499583781\n","Test Loss:  111.5421718005091\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:03,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  501.76493765227497\n","Test Loss:  89.85373472981155\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<03:00,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  438.43330433033407\n","Test Loss:  86.28470657393336\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<02:57,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  404.1535265976563\n","Test Loss:  85.38138980977237\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:19<02:53,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  380.0298284770688\n","Test Loss:  74.31300287507474\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:23<02:51,  3.89s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  365.85141405509785\n","Test Loss:  72.5975020704791\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:27<02:47,  3.88s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  353.770220329985\n","Test Loss:  68.61331997718662\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:30<02:42,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  346.8046124735847\n","Test Loss:  67.33230819553137\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:34<02:38,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  334.51582318451256\n","Test Loss:  72.43972580181435\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:38<02:34,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  332.8982501984574\n","Test Loss:  68.29933857079595\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:42<02:30,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  325.57249443233013\n","Test Loss:  65.92317430744879\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:46<02:27,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  323.70386818097904\n","Test Loss:  63.869801660999656\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:50<02:23,  3.88s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  310.33961814828217\n","Test Loss:  62.54911195579916\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:54<02:19,  3.88s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  311.9127679336816\n","Test Loss:  63.4985444676131\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:58<02:15,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  307.338476369041\n","Test Loss:  61.960833771154284\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [01:01<02:12,  3.89s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  304.01119688060135\n","Test Loss:  59.39781419606879\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:05<02:07,  3.88s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  299.15944794984534\n","Test Loss:  61.37240579910576\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:09<02:03,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  298.83793163252994\n","Test Loss:  65.00059553608298\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:13<02:01,  3.92s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  291.1737878783606\n","Test Loss:  57.898780901450664\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:17<01:56,  3.89s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  291.80184787651524\n","Test Loss:  59.222770266234875\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:21<01:52,  3.88s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  297.913568301592\n","Test Loss:  60.298620896879584\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:25<01:49,  3.90s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  289.4617441962473\n","Test Loss:  66.65785571164452\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:29<01:45,  3.89s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  293.2363178431988\n","Test Loss:  59.97625210741535\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:33<01:49,  4.05s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  278.11581446323544\n","Test Loss:  60.43573270179331\n","Early stopping with best_loss:  57.898780901450664 and test_loss for this epoch:  60.43573270179331 ...\n","mae_validation:\n","0.9466616942363838\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:39,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  925.9804597645998\n","Test Loss:  154.02093796432018\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:36,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  673.2804049551487\n","Test Loss:  129.1372773386538\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:34,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  536.5059625036083\n","Test Loss:  112.15500766970217\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:30,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  472.051870983094\n","Test Loss:  109.57145768776536\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:27,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  459.29941597767174\n","Test Loss:  99.16921663284302\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:23,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  434.82268560957164\n","Test Loss:  96.89208296965808\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:22<02:20,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  414.27954168990254\n","Test Loss:  98.8365089809522\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:18,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  405.12197723053396\n","Test Loss:  92.76349686319008\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:29<02:14,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  395.04069431545213\n","Test Loss:  89.11085488600656\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:32<02:10,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  382.9011256014928\n","Test Loss:  87.16387938335538\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:35<02:07,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  369.45911751128733\n","Test Loss:  84.36458278726786\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:39<02:04,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  361.4272343600169\n","Test Loss:  81.68690286856145\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:42<02:01,  3.29s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  362.5860426449217\n","Test Loss:  80.15004746709019\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:45<01:57,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  349.47229995206\n","Test Loss:  77.89448783081025\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:49<01:54,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  344.1872497801669\n","Test Loss:  77.18114978214726\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:52<01:51,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  343.88158861594275\n","Test Loss:  76.44122212193906\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:55<01:47,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  339.9932171087712\n","Test Loss:  76.38124414999038\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [00:58<01:44,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  334.5096142110415\n","Test Loss:  73.72054914524779\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:02<01:41,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  330.19165292661637\n","Test Loss:  72.9321184642613\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:05<01:38,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  324.7127552591264\n","Test Loss:  74.28763728775084\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:08<01:35,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  318.5698220646009\n","Test Loss:  74.34351915400475\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:12<01:31,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  322.6985905356705\n","Test Loss:  72.18659496726468\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:15<01:28,  3.28s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  316.6032919222489\n","Test Loss:  71.99309510923922\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:18<01:24,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  316.2619903073646\n","Test Loss:  69.44342607911676\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:21<01:21,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  306.23462355788797\n","Test Loss:  71.72429901920259\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:25<01:18,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  304.7657645754516\n","Test Loss:  70.5418406361714\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:28<01:14,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  305.4483950219583\n","Test Loss:  68.79763298202306\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:31<01:11,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  302.49606702825986\n","Test Loss:  68.45303444284946\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:34<01:07,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  303.9081326578744\n","Test Loss:  70.12942612543702\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:37<01:04,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  300.4308140305802\n","Test Loss:  68.6742111267522\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:41<01:01,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  291.93747555231676\n","Test Loss:  68.86540339421481\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:44<00:58,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  291.80772408586927\n","Test Loss:  67.1730650216341\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:47<00:55,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  296.1264936942607\n","Test Loss:  66.89371837535873\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:50<00:52,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  296.4390281303786\n","Test Loss:  66.28407432325184\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:54<00:48,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  295.15545162092894\n","Test Loss:  68.79031907022\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [01:57<00:45,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  295.36529380083084\n","Test Loss:  66.82996216975152\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:00<00:42,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  295.43242281861603\n","Test Loss:  69.28289856854826\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:03<00:38,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  287.91014614282176\n","Test Loss:  70.8980054622516\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:07<00:40,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  282.42365543870255\n","Test Loss:  70.62988296942785\n","Early stopping with best_loss:  66.28407432325184 and test_loss for this epoch:  70.62988296942785 ...\n","mae_validation:\n","1.0383890558687086\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:42,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  858.9183233864605\n","Test Loss:  147.5678115785122\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:41,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  607.9790989309549\n","Test Loss:  109.22496620379388\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:36,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  505.7889994941652\n","Test Loss:  97.47886470425874\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:33,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  460.31728195771575\n","Test Loss:  91.08374537155032\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:30,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  427.5424432018772\n","Test Loss:  86.93233377300203\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:27,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  402.95251060044393\n","Test Loss:  90.11139826849103\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:23,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  387.2215062631294\n","Test Loss:  82.0579199725762\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:20,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  377.6312653692439\n","Test Loss:  75.71611240925267\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:16,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  372.4324747174978\n","Test Loss:  75.90794039517641\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:14,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  366.5298003894277\n","Test Loss:  81.81244349479675\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:36<02:11,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  356.5656878654845\n","Test Loss:  72.95579422451556\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:07,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  348.9547385633923\n","Test Loss:  79.97175552137196\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:04,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  343.5766487997025\n","Test Loss:  71.74183039041236\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:46<02:00,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  333.33207064215094\n","Test Loss:  71.84614553162828\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:50<01:56,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  326.6934294998646\n","Test Loss:  68.67373551707715\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:53<01:53,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  327.233549321536\n","Test Loss:  69.83292413223535\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:57<01:51,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  325.7679475606419\n","Test Loss:  69.37781070405617\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:00<01:47,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  319.12176412763074\n","Test Loss:  66.03209156636149\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:03<01:44,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  313.19193501747213\n","Test Loss:  66.9326357175596\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:07<01:41,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  311.64866800187156\n","Test Loss:  68.72662366461009\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:10<01:37,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  307.3917103363201\n","Test Loss:  67.59080455824733\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:13<01:33,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  307.5291477567516\n","Test Loss:  66.83463593618944\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:17<01:38,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  298.6994320033118\n","Test Loss:  66.58754760399461\n","Early stopping with best_loss:  66.03209156636149 and test_loss for this epoch:  66.58754760399461 ...\n","mae_validation:\n","1.0311107992460389\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:44,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  748.2631526626647\n","Test Loss:  124.31171005032957\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:40,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  511.11113252677023\n","Test Loss:  103.94075660780072\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:37,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  452.1468607261777\n","Test Loss:  96.0315015297383\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:35,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  412.2049647765234\n","Test Loss:  92.14027493540198\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:32,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  384.1426628381014\n","Test Loss:  85.31567235756665\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:29,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  367.59265298582613\n","Test Loss:  79.76235146215186\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:25,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  353.3784430352971\n","Test Loss:  80.21175821963698\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:22,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  352.77808224316686\n","Test Loss:  76.12698566215113\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:18,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  333.4889999143779\n","Test Loss:  74.851647708565\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:14,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  327.5696972897276\n","Test Loss:  76.54597649886273\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:11,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  328.3387196203694\n","Test Loss:  78.7970791105181\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:08,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  320.701544996351\n","Test Loss:  66.51253426726907\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:05,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  312.72440692503005\n","Test Loss:  69.18419074546546\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:47<02:01,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  305.96515671815723\n","Test Loss:  70.1150197503157\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:50<01:58,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  301.3120285817422\n","Test Loss:  67.56834255158901\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:54<01:54,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  305.7569442936219\n","Test Loss:  64.79127970617265\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:57<01:50,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  294.3952042637393\n","Test Loss:  64.12868691678159\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:00<01:48,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  290.2132943170145\n","Test Loss:  64.09714456275105\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:04<01:44,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  286.1357696633786\n","Test Loss:  62.772841800935566\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:07<01:41,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  288.39071723632514\n","Test Loss:  62.18743798183277\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:10<01:37,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  282.7767786527984\n","Test Loss:  62.75801732111722\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:14<01:34,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  277.9067196743563\n","Test Loss:  65.84335950668901\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:17<01:30,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  272.1962042078376\n","Test Loss:  62.14781861146912\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:20<01:27,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  267.5576594546437\n","Test Loss:  71.85550849186257\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:24<01:23,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  273.9914510040544\n","Test Loss:  63.447546588256955\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:27<01:20,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  266.86039236281067\n","Test Loss:  65.39447945728898\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:31<01:17,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  271.9074032464996\n","Test Loss:  60.80570240318775\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:34<01:13,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  269.39448739076033\n","Test Loss:  63.28636318165809\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:37<01:10,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  263.26537228608504\n","Test Loss:  60.44329305831343\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:41<01:07,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  261.4618640495464\n","Test Loss:  60.09145434387028\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:44<01:03,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  262.3408177946694\n","Test Loss:  62.83920877240598\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:47<01:00,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  260.98054361576214\n","Test Loss:  59.105085174553096\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:51<00:57,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  253.99267184222117\n","Test Loss:  58.69639884214848\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:54<00:53,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  257.0061160349287\n","Test Loss:  63.10760116856545\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:57<00:50,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  254.12570619303733\n","Test Loss:  59.42491368600167\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:01<00:47,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  248.4697322663851\n","Test Loss:  58.3798847948201\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:04<00:43,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  256.96274322643876\n","Test Loss:  59.16964537813328\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:08<00:40,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  254.9592781229876\n","Test Loss:  62.40727470256388\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:11<00:36,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  249.4551243162714\n","Test Loss:  67.04685232881457\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:14<00:33,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  248.4046632689424\n","Test Loss:  56.612544011324644\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:18<00:30,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  251.89105358021334\n","Test Loss:  57.228454146534204\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:21<00:26,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  246.6633006730117\n","Test Loss:  57.1738305692561\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [02:24<00:23,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  246.0832231757231\n","Test Loss:  57.8913669353351\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [02:28<00:20,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  245.48083191458136\n","Test Loss:  57.773054133635014\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [02:31<00:16,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  246.67989559145644\n","Test Loss:  56.54478358197957\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 92%|█████████▏| 46/50 [02:35<00:13,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  243.61215486587025\n","Test Loss:  55.57625964330509\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 94%|█████████▍| 47/50 [02:38<00:10,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  238.25957686454058\n","Test Loss:  59.06655431026593\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 96%|█████████▌| 48/50 [02:41<00:06,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  240.71950645092875\n","Test Loss:  56.40296370955184\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 49/50 [02:45<00:03,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  238.16960805421695\n","Test Loss:  56.893011601641774\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 50/50 [02:48<00:00,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  234.89063900057226\n","Test Loss:  54.580121404491365\n","mae_validation:\n","0.9158333366557058\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:08,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  700.6467157900333\n","Test Loss:  116.56961573287845\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:00,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  472.690505778417\n","Test Loss:  94.67399397585541\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:56,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  423.8279102751985\n","Test Loss:  86.03046751068905\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<02:52,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  388.70853395666927\n","Test Loss:  83.03772569494322\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:48,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  366.0499681280926\n","Test Loss:  78.49150553718209\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:45,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  352.4789845207706\n","Test Loss:  75.24995628744364\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:40,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  337.69764073286206\n","Test Loss:  74.72440709918737\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:29<02:37,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  325.5745472838171\n","Test Loss:  71.25408861460164\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:33<02:33,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  321.97171525657177\n","Test Loss:  70.58301920071244\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:37<02:30,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  316.5752734364942\n","Test Loss:  82.81338464329019\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:41<02:27,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  303.92784935841337\n","Test Loss:  67.65860181534663\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:45<02:22,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  308.1625537732616\n","Test Loss:  67.05974761396646\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:48<02:18,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  294.5469094575383\n","Test Loss:  78.04517699591815\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:52<02:14,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  293.6265825876035\n","Test Loss:  64.49606412742287\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:56<02:11,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  285.08930979063734\n","Test Loss:  72.46297958213836\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:59<02:07,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  289.29327075602487\n","Test Loss:  64.77971119340509\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:03<02:03,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  276.20696916570887\n","Test Loss:  63.76443600002676\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:07<01:59,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  283.38533533457667\n","Test Loss:  62.98343949764967\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:11<01:55,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  276.3507624845952\n","Test Loss:  65.26337143033743\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:15<01:52,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  269.73002017615363\n","Test Loss:  62.87711520586163\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:18<01:48,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  267.9790785643272\n","Test Loss:  63.716057032346725\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:22<01:44,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  265.0722361030057\n","Test Loss:  60.205090136267245\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:26<01:40,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  265.81074290769175\n","Test Loss:  59.82376001914963\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:29<01:37,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  265.95959014468826\n","Test Loss:  59.4399864571169\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:33<01:33,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  261.3535786969587\n","Test Loss:  59.9852776452899\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:37<01:30,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  255.80350318108685\n","Test Loss:  60.134656202979386\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:41<01:26,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  255.0829647362698\n","Test Loss:  58.015637101605535\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:44<01:22,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  251.0056835046271\n","Test Loss:  65.62679457012564\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:48<01:18,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  254.48445215215907\n","Test Loss:  57.709004887379706\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:52<01:15,  3.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  253.72751635545865\n","Test Loss:  57.421852787490934\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:56<01:11,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  255.21504857717082\n","Test Loss:  58.68873910373077\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:59<01:07,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  249.07968972623348\n","Test Loss:  56.90581489726901\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [02:03<01:03,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  250.7143789352849\n","Test Loss:  57.50376105494797\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [02:07<00:59,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  247.3304667272605\n","Test Loss:  56.71256188163534\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [02:11<00:55,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  241.74820557166822\n","Test Loss:  56.2757834950462\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:14<00:52,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  242.4573349854909\n","Test Loss:  57.845595251303166\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [02:18<00:48,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  236.0478337854147\n","Test Loss:  55.57844757474959\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [02:22<00:44,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  234.05883074039593\n","Test Loss:  54.29878035187721\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [02:26<00:41,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  244.1970965252258\n","Test Loss:  55.91952115483582\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [02:29<00:37,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  236.6637856978923\n","Test Loss:  54.73108392301947\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [02:33<00:33,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  231.7450195234269\n","Test Loss:  55.340687956660986\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:37<00:29,  3.74s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  235.04224517708644\n","Test Loss:  55.02598762814887\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [02:41<00:30,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  233.2497395942919\n","Test Loss:  56.2746161762625\n","Early stopping with best_loss:  54.29878035187721 and test_loss for this epoch:  56.2746161762625 ...\n","mae_validation:\n","0.9105673107847067\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:54,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  736.4540311470628\n","Test Loss:  115.64151898771524\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:51,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  527.9333202866837\n","Test Loss:  98.18410164024681\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:50,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  462.1016521397978\n","Test Loss:  107.02907916903496\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:44,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  423.0379898371175\n","Test Loss:  83.20425425190479\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:39,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  410.37077974621207\n","Test Loss:  89.4032136965543\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:35,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  380.4815731719136\n","Test Loss:  76.44644499057904\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:32,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  379.48292841017246\n","Test Loss:  80.15647692140192\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:30,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  369.16224056947976\n","Test Loss:  97.8914844263345\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:32<02:25,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  376.92337273294106\n","Test Loss:  74.69586567953229\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:35<02:21,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  363.11446651350707\n","Test Loss:  81.62034212425351\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:39<02:18,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  354.07002257648855\n","Test Loss:  75.03329647425562\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:42<02:14,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  359.01631186995655\n","Test Loss:  73.34326288290322\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:46<02:10,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  346.3799669314176\n","Test Loss:  74.80566882714629\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:49<02:06,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  350.6789244269021\n","Test Loss:  71.99379644030705\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:53<02:03,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  341.8270344771445\n","Test Loss:  73.42888263054192\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:56<02:00,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  341.8318553911522\n","Test Loss:  76.21760636940598\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:00<01:57,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  344.759800712578\n","Test Loss:  80.35047347284853\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:03<01:53,  3.56s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  332.82474569417536\n","Test Loss:  73.02236834261566\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:07<01:50,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  327.51014999859035\n","Test Loss:  68.08222803100944\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:11<01:46,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  337.8554761148989\n","Test Loss:  70.15796317439526\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:14<01:42,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  332.2036166121252\n","Test Loss:  75.87365869432688\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:18<01:39,  3.55s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  331.70515184942633\n","Test Loss:  71.26551801338792\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:21<01:35,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  330.20038472115993\n","Test Loss:  68.17830844409764\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:25<01:40,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  319.60977580724284\n","Test Loss:  78.91915030963719\n","Early stopping with best_loss:  68.08222803100944 and test_loss for this epoch:  78.91915030963719 ...\n","mae_validation:\n","1.0080388088847438\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:57,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  684.0944676715881\n","Test Loss:  115.56778994761407\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:53,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  483.1924814581871\n","Test Loss:  113.03201147727668\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:50,  3.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  419.0959997791797\n","Test Loss:  88.59752513282001\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:46,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  411.2307541007176\n","Test Loss:  113.19530982291326\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:42,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  380.57213960029185\n","Test Loss:  79.94374010898173\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:38,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  373.96647294284776\n","Test Loss:  84.78639310784638\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:35,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  370.6599199222401\n","Test Loss:  79.28405833803117\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:30,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  360.12838628003374\n","Test Loss:  76.33873382024467\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:32<02:27,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  354.48812454380095\n","Test Loss:  76.43103413656354\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:36<02:24,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  354.2805608618073\n","Test Loss:  90.19518585479818\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:39<02:20,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  348.58233109954745\n","Test Loss:  78.90857032313943\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:43<02:16,  3.58s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  347.2494314452633\n","Test Loss:  82.38498144317418\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:46<02:13,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  345.82627471070737\n","Test Loss:  72.76451777666807\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:50<02:09,  3.60s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  340.1704177800566\n","Test Loss:  79.52523055672646\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:54<02:05,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  335.14007759746164\n","Test Loss:  73.88228773698211\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:57<02:02,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  335.4191773929633\n","Test Loss:  66.91579512227327\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:01<01:59,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  335.8623611489311\n","Test Loss:  71.73414617730305\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:04<01:55,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  335.6111789941788\n","Test Loss:  74.99899740330875\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:08<01:51,  3.61s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  331.76059663761407\n","Test Loss:  67.51966840960085\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:12<01:48,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  324.7596587224398\n","Test Loss:  73.50920768547803\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:15<01:53,  3.78s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  323.8667527209036\n","Test Loss:  68.10705659817904\n","Early stopping with best_loss:  66.91579512227327 and test_loss for this epoch:  68.10705659817904 ...\n","mae_validation:\n","0.9964915560446121\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:55,  3.59s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  652.4756611641496\n","Test Loss:  97.15567384660244\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:55,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  450.04508620034903\n","Test Loss:  88.44044751673937\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:51,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  406.6605143202469\n","Test Loss:  89.28931992128491\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:48,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  386.0818644827232\n","Test Loss:  87.63209135830402\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:45,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  375.74866101518273\n","Test Loss:  78.36146035976708\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:40,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  357.01249541551806\n","Test Loss:  78.12282419204712\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:25<02:36,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  353.6936080753803\n","Test Loss:  81.06713287439197\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:29<02:33,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  356.58532446064055\n","Test Loss:  72.32194332126528\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:32<02:30,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  350.93093058280647\n","Test Loss:  86.49373055249453\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:36<02:27,  3.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  348.2188714607619\n","Test Loss:  72.05143647035584\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:40<02:22,  3.66s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  342.4610013491474\n","Test Loss:  79.55189234251156\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:43<02:19,  3.67s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  341.69523372501135\n","Test Loss:  79.23821892309934\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:47<02:15,  3.65s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  334.78767869994044\n","Test Loss:  72.48002105019987\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:51<02:11,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  334.2514682468027\n","Test Loss:  76.1640015989542\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:54<02:21,  3.93s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  334.22292703483254\n","Test Loss:  76.30858940444887\n","Early stopping with best_loss:  72.05143647035584 and test_loss for this epoch:  76.30858940444887 ...\n","mae_validation:\n","1.0436505106868235\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:07,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  609.9630693318322\n","Test Loss:  113.84224630147219\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:04,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  431.1203756649047\n","Test Loss:  122.49985709693283\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<03:01,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  400.63808907195926\n","Test Loss:  87.85169666353613\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<02:57,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  385.293040686287\n","Test Loss:  92.57889246195555\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:19<02:53,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  379.477526047267\n","Test Loss:  76.20800411282107\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:23<02:50,  3.88s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  363.1855206498876\n","Test Loss:  80.44788352958858\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:27<02:46,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  358.3092718683183\n","Test Loss:  90.62147684115916\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:30<02:42,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  347.171497058589\n","Test Loss:  87.99745025066659\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:34<02:38,  3.88s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  351.32315221987665\n","Test Loss:  75.03112757112831\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:38<02:34,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  346.94676161464304\n","Test Loss:  75.82372125796974\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:42<02:30,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  346.95173698849976\n","Test Loss:  77.5838231742382\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:46<02:27,  3.88s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  339.79285212792456\n","Test Loss:  79.63279735669494\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:50<02:23,  3.89s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  350.77759871724993\n","Test Loss:  85.60783307626843\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:54<02:20,  3.89s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  337.03103973483667\n","Test Loss:  70.85936312936246\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:58<02:16,  3.90s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  333.50088483281434\n","Test Loss:  76.92931743338704\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [01:02<02:12,  3.89s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  339.8550389269367\n","Test Loss:  76.53493349812925\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:05<02:08,  3.90s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  328.0628767106682\n","Test Loss:  67.20344342314638\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:09<02:03,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  328.6581846224144\n","Test Loss:  74.37701033707708\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:13<01:59,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  322.62376272585243\n","Test Loss:  72.44652417581528\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:17<01:55,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  320.6094486443326\n","Test Loss:  68.43428956065327\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:21<01:51,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  317.4242873676121\n","Test Loss:  73.24945011874661\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:25<01:57,  4.06s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  318.4122315114364\n","Test Loss:  70.47879194281995\n","Early stopping with best_loss:  67.20344342314638 and test_loss for this epoch:  70.47879194281995 ...\n","mae_validation:\n","0.9792450420361231\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:47,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  725.7394738718867\n","Test Loss:  199.06779773533344\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:45,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  493.35508186742663\n","Test Loss:  98.29794347472489\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:43,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  442.514928964898\n","Test Loss:  85.05012363288552\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:38,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  399.02376603055745\n","Test Loss:  81.25891880784184\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:34,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  383.75640254840255\n","Test Loss:  79.47433222737163\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:31,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  370.71083343122154\n","Test Loss:  91.61465480178595\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:28,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  357.9802940757945\n","Test Loss:  74.53102694544941\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:24,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  355.7205350920558\n","Test Loss:  73.23864830331877\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:21,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  354.0673851026222\n","Test Loss:  74.92891145590693\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:34<02:19,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  349.74999869894236\n","Test Loss:  71.68583320826292\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:14,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  341.7999594055582\n","Test Loss:  73.05334383202717\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:41<02:10,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  344.5940780027304\n","Test Loss:  71.24523139931262\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:44<02:06,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  343.1224807333201\n","Test Loss:  74.43085773382336\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:48<02:03,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  341.0451309932396\n","Test Loss:  71.10181299538817\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:51<02:00,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  329.36574441660196\n","Test Loss:  74.14240367524326\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:55<01:56,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  328.3154722149484\n","Test Loss:  75.47407406102866\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:58<01:53,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  330.44699758058414\n","Test Loss:  67.45359172765166\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:01<01:49,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  331.37468882184476\n","Test Loss:  66.9835006177891\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:05<01:46,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  321.73568250169046\n","Test Loss:  74.82753626443446\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:08<01:43,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  317.90646115411073\n","Test Loss:  70.66621295362711\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:12<01:40,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  320.0543933012523\n","Test Loss:  69.4285360397771\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:15<01:36,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  324.8338091475889\n","Test Loss:  67.13336905604228\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:19<01:33,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  325.8959439508617\n","Test Loss:  65.6790600467939\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:22<01:30,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  316.5113060101867\n","Test Loss:  65.03172659361735\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:26<01:26,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  310.6851724330336\n","Test Loss:  63.053339881356806\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:29<01:22,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  312.8826972148381\n","Test Loss:  66.67428930662572\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:33<01:19,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  316.68278974387795\n","Test Loss:  67.4551426935941\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:36<01:15,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  305.8119410057552\n","Test Loss:  74.5271062888205\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:39<01:12,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  310.27094386983663\n","Test Loss:  68.73920644400641\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:43<01:14,  3.57s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  304.9987495932728\n","Test Loss:  81.21412615478039\n","Early stopping with best_loss:  63.053339881356806 and test_loss for this epoch:  81.21412615478039 ...\n","mae_validation:\n","0.9648093736243984\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:53,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  669.7513502985239\n","Test Loss:  105.46228898130357\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:49,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  476.9021109715104\n","Test Loss:  94.47259723767638\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:44,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  422.868708494585\n","Test Loss:  90.07033488433808\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:41,  3.51s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  391.0983342062682\n","Test Loss:  81.37478921795264\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:38,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  363.5243242494762\n","Test Loss:  81.17557954462245\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:34,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  355.40662184078246\n","Test Loss:  76.62277736933902\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:30,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  352.27618302218616\n","Test Loss:  79.04230441059917\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:26,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  341.15158093487844\n","Test Loss:  82.73720486275852\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:22,  3.48s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  334.8735548518598\n","Test Loss:  71.87335925921798\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:35<02:19,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  334.3511984376237\n","Test Loss:  69.99145236145705\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:38<02:16,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  328.3069114368409\n","Test Loss:  71.81778065580875\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:42<02:12,  3.50s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  327.5524038784206\n","Test Loss:  71.9251921037212\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:45<02:09,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  331.8757107676938\n","Test Loss:  76.57405824679881\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:49<02:05,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  325.9352258434519\n","Test Loss:  72.5514510218054\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:52<02:14,  3.75s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  327.5244162157178\n","Test Loss:  73.87272440548986\n","Early stopping with best_loss:  69.99145236145705 and test_loss for this epoch:  73.87272440548986 ...\n","mae_validation:\n","1.0408037441933642\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:52,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  626.0108280908316\n","Test Loss:  88.76484697544947\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:49,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  444.1219600457698\n","Test Loss:  79.96413643099368\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:45,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  397.00208138721064\n","Test Loss:  72.51622797641903\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:42,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  382.3832261976786\n","Test Loss:  76.08372304216027\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:38,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  363.0141016310081\n","Test Loss:  80.74072789307684\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:21<02:34,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  350.1073833759874\n","Test Loss:  65.69242504378781\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:31,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  346.3532015369274\n","Test Loss:  73.34185409452766\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:28<02:28,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  341.53941326774657\n","Test Loss:  68.72724846098572\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:31<02:24,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  346.0907592959702\n","Test Loss:  67.27988640032709\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:35<02:21,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  330.62928299978375\n","Test Loss:  72.54851538967341\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:38<02:17,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  326.0187728498131\n","Test Loss:  63.864169168286026\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:42<02:14,  3.53s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  328.9842059062794\n","Test Loss:  72.02000152692199\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:45<02:10,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  329.42300242139027\n","Test Loss:  64.2099002758041\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:49<02:07,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  326.8401120333001\n","Test Loss:  71.43724712356925\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:52<02:03,  3.54s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  324.1844554506242\n","Test Loss:  68.87941659707576\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:56<02:11,  3.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  315.8091178769246\n","Test Loss:  68.07565993349999\n","Early stopping with best_loss:  63.864169168286026 and test_loss for this epoch:  68.07565993349999 ...\n","mae_validation:\n","0.9810235175434366\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:08,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  595.1661102501675\n","Test Loss:  100.93732929136604\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:03,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  426.47199655044824\n","Test Loss:  89.10021386668086\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:59,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  388.15501872450113\n","Test Loss:  78.85350351780653\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<02:55,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  371.31875391025096\n","Test Loss:  86.96560773719102\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:19<02:51,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  362.1066229240969\n","Test Loss:  81.86020137369633\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:47,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  348.25658393185586\n","Test Loss:  85.59014632552862\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:44,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  346.64718397171237\n","Test Loss:  74.30706384219229\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:30<02:40,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  340.5493465375621\n","Test Loss:  72.87635912047699\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:34<02:35,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  329.15830756537616\n","Test Loss:  76.471589169465\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:38<02:32,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  335.4235090462025\n","Test Loss:  74.89726873487234\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:41<02:28,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  332.8373089758679\n","Test Loss:  82.04138826671988\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:45<02:24,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  328.7552889343351\n","Test Loss:  74.5709029044956\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:49<02:21,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  319.5111574390903\n","Test Loss:  69.95073957275599\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:53<02:18,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  323.8572941403836\n","Test Loss:  70.81561728008091\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:57<02:15,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  318.7008433723822\n","Test Loss:  72.8305315587204\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [01:01<02:12,  3.88s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  316.63366175303236\n","Test Loss:  72.97471609059721\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:05<02:07,  3.87s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  315.9942782833241\n","Test Loss:  76.82478055171669\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:08<02:03,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  310.9824396530166\n","Test Loss:  69.55392435751855\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:12<01:59,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  313.9861006918363\n","Test Loss:  72.20299896877259\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:16<01:55,  3.85s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  313.48885783646256\n","Test Loss:  79.66398371336982\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:20<01:51,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  303.82164106238633\n","Test Loss:  66.44174576364458\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:24<01:47,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  304.95104897674173\n","Test Loss:  65.91050771810114\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:28<01:43,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  309.12285693222657\n","Test Loss:  66.7972837598063\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:31<01:39,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  304.5050119217485\n","Test Loss:  69.72683110460639\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:35<01:36,  3.84s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  301.8945950148627\n","Test Loss:  88.27654726989567\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:39<01:32,  3.86s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  307.1879502609372\n","Test Loss:  73.60840616095811\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:43<01:35,  3.98s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  300.0526576070115\n","Test Loss:  66.52006105985492\n","Early stopping with best_loss:  65.91050771810114 and test_loss for this epoch:  66.52006105985492 ...\n","mae_validation:\n","0.9951883745130657\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:45,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  725.5251856967807\n","Test Loss:  105.13549346011132\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:41,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  485.08154606074095\n","Test Loss:  88.9108033683151\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:37,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  423.91140782646835\n","Test Loss:  85.29695092327893\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:34,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  394.2026609610766\n","Test Loss:  79.07150716148317\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:30,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  374.452155216597\n","Test Loss:  76.6431983737275\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:27,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  363.8689358532429\n","Test Loss:  75.0667983321473\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:24,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  349.0718102203682\n","Test Loss:  86.09532383084297\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:20,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  344.1561678829603\n","Test Loss:  73.02703107520938\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:16,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  333.2352374503389\n","Test Loss:  73.12429443001747\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:12,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  336.6465683141723\n","Test Loss:  70.82521972618997\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:36<02:09,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  327.99991481471807\n","Test Loss:  84.38312594406307\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:06,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  324.13004126027226\n","Test Loss:  68.70403608027846\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:03,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  325.594788861461\n","Test Loss:  70.31462069321424\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:46<01:59,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  325.91220295242965\n","Test Loss:  67.00781212281436\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:49<01:55,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  316.7288718568161\n","Test Loss:  67.71883719740435\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:53<01:52,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.6298223854974\n","Test Loss:  65.66326414095238\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:56<01:49,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  314.5976085155271\n","Test Loss:  82.05592132359743\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:00<01:46,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  315.8660160526633\n","Test Loss:  67.53229674417526\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:03<01:43,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  308.1342214019969\n","Test Loss:  73.8084022756666\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:06<01:40,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  303.9430812895298\n","Test Loss:  64.05466254241765\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:10<01:36,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  304.34877889510244\n","Test Loss:  65.34966492839158\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:13<01:32,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  309.6531147090718\n","Test Loss:  72.83280934812501\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:16<01:29,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  309.8191072521731\n","Test Loss:  67.06897511333227\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:19<01:26,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  292.007534209406\n","Test Loss:  64.49920795951039\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:23<01:22,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  296.16586561128497\n","Test Loss:  63.696080659050494\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:26<01:19,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  293.60336828418076\n","Test Loss:  66.38336415588856\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:29<01:16,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  288.16950139030814\n","Test Loss:  67.25217891205102\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:33<01:12,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  293.18493588874117\n","Test Loss:  67.42276675393805\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:36<01:09,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  299.0896947006695\n","Test Loss:  66.17652772832662\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:39<01:12,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  298.86165062058717\n","Test Loss:  66.1855813683942\n","Early stopping with best_loss:  63.696080659050494 and test_loss for this epoch:  66.1855813683942 ...\n","mae_validation:\n","0.996360511026108\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:48,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  654.8279287591577\n","Test Loss:  94.49634460359812\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:43,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  453.4204477388412\n","Test Loss:  79.73889971710742\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:39,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  379.86879569571465\n","Test Loss:  80.02233570627868\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:37,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  371.22113969083875\n","Test Loss:  74.64478617999703\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:33,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  348.76829579565674\n","Test Loss:  73.06360091269016\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:30,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  350.9714622441679\n","Test Loss:  68.46616447158158\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:26,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  332.62243839213625\n","Test Loss:  77.70318375714123\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:23,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  336.1124499421567\n","Test Loss:  71.4401269480586\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:19,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  322.19489713711664\n","Test Loss:  71.78033825941384\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:34<02:16,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  319.47208593180403\n","Test Loss:  67.91167078260332\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:12,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  321.8430871325545\n","Test Loss:  67.84362965496257\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:09,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  313.36686752922833\n","Test Loss:  68.48835982196033\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:44<02:06,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  309.52810391783714\n","Test Loss:  69.02473473269492\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:47<02:03,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  306.8846088042483\n","Test Loss:  65.611176752951\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:51<02:00,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.55773874302395\n","Test Loss:  65.55660057067871\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:54<01:57,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  304.5702858599834\n","Test Loss:  65.25729534262791\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:58<01:53,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  304.9363058647141\n","Test Loss:  65.17311622854322\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:01<01:49,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  301.38345187250525\n","Test Loss:  64.79690449172631\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:05<01:46,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  297.2661066609435\n","Test Loss:  67.40406637685373\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:08<01:42,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  296.3478018641472\n","Test Loss:  64.28538322076201\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [01:11<01:40,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  291.47835946595296\n","Test Loss:  65.772496458143\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [01:15<01:36,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  291.7570345378481\n","Test Loss:  74.8164573572576\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [01:18<01:32,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  287.0388103518635\n","Test Loss:  62.96522224787623\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [01:22<01:28,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  286.5386964119971\n","Test Loss:  61.439933037385345\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [01:25<01:25,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  287.01375523535535\n","Test Loss:  65.92431429727003\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [01:29<01:22,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  281.8548100446351\n","Test Loss:  69.73572382098064\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [01:32<01:18,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  280.35507408576086\n","Test Loss:  61.613996556494385\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [01:35<01:14,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  282.46079978812486\n","Test Loss:  64.87876227777451\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [01:39<01:11,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  281.9663177328184\n","Test Loss:  60.670396036468446\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [01:42<01:08,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  271.2141721146181\n","Test Loss:  62.08498962922022\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [01:46<01:05,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  276.65324191655964\n","Test Loss:  63.44095295201987\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [01:49<01:01,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  275.6019878964871\n","Test Loss:  60.47349268104881\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [01:52<00:58,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  268.56281602336094\n","Test Loss:  64.39065885730088\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [01:56<00:54,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  272.07935702614486\n","Test Loss:  60.73294035438448\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [01:59<00:50,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  267.85620751976967\n","Test Loss:  62.202126330696046\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:03<00:47,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  269.8680613413453\n","Test Loss:  69.2239208072424\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [02:06<00:49,  3.52s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  269.3257574290037\n","Test Loss:  61.96361233945936\n","Early stopping with best_loss:  60.47349268104881 and test_loss for this epoch:  61.96361233945936 ...\n","mae_validation:\n","0.9605662358595084\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:46,  3.40s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  605.8881009295583\n","Test Loss:  88.14822398731485\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:43,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  430.1765550393611\n","Test Loss:  77.89478537160903\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:40,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  378.24330829642713\n","Test Loss:  74.99061695253477\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:37,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  359.98220158647746\n","Test Loss:  74.73836149927229\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:17<02:34,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  351.0177093092352\n","Test Loss:  69.30656909756362\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:31,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  344.6738920933567\n","Test Loss:  74.64853024482727\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:24<02:28,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  340.92654613405466\n","Test Loss:  68.62738815275952\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:27<02:24,  3.43s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  331.0574373109266\n","Test Loss:  71.17431031935848\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:21,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  327.9731693267822\n","Test Loss:  65.932010313496\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:34<02:18,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  321.1826838753186\n","Test Loss:  66.02115517389029\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:14,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  316.01147142937407\n","Test Loss:  67.69792438205332\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:41<02:10,  3.44s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  316.3445279961452\n","Test Loss:  65.73549533542246\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:44<02:07,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  315.2289793882519\n","Test Loss:  62.98137825028971\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:48<02:04,  3.45s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  308.5794269051403\n","Test Loss:  69.96550464443862\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:51<02:01,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  307.5615763189271\n","Test Loss:  65.53293860005215\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:55<01:57,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  303.61243861261755\n","Test Loss:  60.26506387954578\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:58<01:54,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  299.5033687213436\n","Test Loss:  61.541633117944\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:02<01:51,  3.47s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  295.75484870094806\n","Test Loss:  62.244508899748325\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:05<01:47,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  298.9958692640066\n","Test Loss:  60.9684780575335\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:08<01:43,  3.46s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  292.01099236588925\n","Test Loss:  63.19423806667328\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:12<01:48,  3.62s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  292.96722333692014\n","Test Loss:  63.563826439902186\n","Early stopping with best_loss:  60.26506387954578 and test_loss for this epoch:  63.563826439902186 ...\n","mae_validation:\n","0.9624859961553551\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:07,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  579.1366685610265\n","Test Loss:  92.7003062274307\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<03:03,  3.83s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  408.37877395004034\n","Test Loss:  91.18157544033602\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:59,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  372.94903059722856\n","Test Loss:  89.62339050509036\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:15<02:55,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  363.33593185339123\n","Test Loss:  77.25898623047397\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:19<02:51,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  347.16929595917463\n","Test Loss:  74.18780207447708\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:22<02:47,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  336.8843581043184\n","Test Loss:  96.4579375013709\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:26<02:43,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  331.95835934439674\n","Test Loss:  72.99360128678381\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:30<02:39,  3.79s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  321.96070703398436\n","Test Loss:  74.76663797488436\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:34<02:36,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  321.82071314100176\n","Test Loss:  84.49527014605701\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:38<02:32,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  319.7401082571596\n","Test Loss:  71.9828670527786\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:41<02:28,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  313.0928702922538\n","Test Loss:  73.65023483755067\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:45<02:25,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.8881701277569\n","Test Loss:  66.97838424472138\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:49<02:20,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  305.8220802806318\n","Test Loss:  68.11421180237085\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:53<02:16,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  308.67327051982284\n","Test Loss:  75.90048752073199\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:57<02:13,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  304.23422127682716\n","Test Loss:  72.02492990903556\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [01:00<02:09,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  301.7526829508133\n","Test Loss:  63.92035669833422\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [01:04<02:05,  3.81s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  298.93107405211776\n","Test Loss:  71.11322505772114\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [01:08<02:02,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  302.00915149925277\n","Test Loss:  69.06067007035017\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:12<01:58,  3.82s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  292.1488262685016\n","Test Loss:  66.90706688724458\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:16<01:54,  3.80s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  292.62709017517045\n","Test Loss:  66.25765939243138\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:19<01:59,  4.00s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  285.4942501671612\n","Test Loss:  68.38654980901629\n","Early stopping with best_loss:  63.92035669833422 and test_loss for this epoch:  68.38654980901629 ...\n","mae_validation:\n","0.9919599073196571\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:39,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  681.3183623272926\n","Test Loss:  108.5020391587168\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:35,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  471.15910618938506\n","Test Loss:  95.32306793890893\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:33,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  394.15813922323287\n","Test Loss:  81.98253670800477\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:30,  3.27s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  368.82985602971166\n","Test Loss:  78.39337366819382\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:26,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  352.742226600647\n","Test Loss:  77.26770492084324\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:22,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  348.1973653896712\n","Test Loss:  82.73057003412396\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:22<02:20,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  330.99760484742\n","Test Loss:  79.58739261515439\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:17,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  331.0046001235023\n","Test Loss:  73.16143128322437\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:29<02:13,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  314.3453576937318\n","Test Loss:  72.25170691078529\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:32<02:09,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  318.41092511825264\n","Test Loss:  77.4824138181284\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:35<02:06,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  312.58883719868027\n","Test Loss:  68.12812506221235\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:38<02:02,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  306.3411923199892\n","Test Loss:  68.46138950483873\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:42<01:59,  3.23s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  305.80340568115935\n","Test Loss:  68.54686685092747\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:45<01:56,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  300.75109654664993\n","Test Loss:  73.96988872997463\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:48<01:53,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  296.7718141237274\n","Test Loss:  65.95811218675226\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:51<01:50,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  297.226429968141\n","Test Loss:  71.65611325902864\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:55<01:47,  3.25s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  293.0715453457087\n","Test Loss:  67.77843073708937\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [00:58<01:43,  3.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  293.60760316532105\n","Test Loss:  69.54723932594061\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:01<01:41,  3.26s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  294.5010690772906\n","Test Loss:  69.48253477737308\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:05<01:46,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  286.4372741188854\n","Test Loss:  71.94362602941692\n","Early stopping with best_loss:  65.95811218675226 and test_loss for this epoch:  71.94362602941692 ...\n","mae_validation:\n","1.0409638550424525\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:42,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  644.7334524914622\n","Test Loss:  98.66219015978277\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:39,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  448.51175437960774\n","Test Loss:  81.29626460094005\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:09<02:36,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  392.8159123212099\n","Test Loss:  91.46250611543655\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:32,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  371.6830868013203\n","Test Loss:  71.2509835306555\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:29,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  356.77614731993526\n","Test Loss:  71.8127622148022\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:19<02:26,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  341.2637034757063\n","Test Loss:  67.27066546864808\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:23,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  332.74142987653613\n","Test Loss:  60.50799283385277\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:20,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  317.8820221628994\n","Test Loss:  62.08143654617015\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:17,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  315.94248140975833\n","Test Loss:  71.95442594308406\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:13,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  314.19123118929565\n","Test Loss:  61.18244933523238\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:36<02:10,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  305.7265201420523\n","Test Loss:  60.59839233662933\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:39<02:06,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  303.112072694581\n","Test Loss:  60.372824241407216\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:03,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  303.6316372756846\n","Test Loss:  83.71374368760735\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:46<01:59,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  299.23162384610623\n","Test Loss:  56.78344788122922\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:49<01:56,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  292.32446768390946\n","Test Loss:  58.74832251947373\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:53<01:53,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  292.09906054567546\n","Test Loss:  56.070820736698806\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:56<01:50,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  288.2528576939367\n","Test Loss:  63.95784460566938\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [00:59<01:46,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  284.8651207871735\n","Test Loss:  58.056312028318644\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [01:03<01:42,  3.31s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  280.2400773395784\n","Test Loss:  58.327862289734185\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:06<01:39,  3.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  280.3774306788109\n","Test Loss:  57.2129581887275\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [01:09<01:44,  3.49s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  277.3559302627109\n","Test Loss:  70.35184095613658\n","Early stopping with best_loss:  56.070820736698806 and test_loss for this epoch:  70.35184095613658 ...\n","mae_validation:\n","0.9422511436191635\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<02:43,  3.34s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  605.7040132181719\n","Test Loss:  91.93076859600842\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:06<02:39,  3.33s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  422.50766274519265\n","Test Loss:  74.43175537511706\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:10<02:37,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  366.1463767848909\n","Test Loss:  76.76972293574363\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:13<02:35,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  353.5723832109943\n","Test Loss:  69.34597554663196\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:16<02:32,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  333.1709393206984\n","Test Loss:  69.6211120756343\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:20<02:28,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  325.12341817608103\n","Test Loss:  65.32066694181412\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:23<02:25,  3.37s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  316.50422236090526\n","Test Loss:  70.23706817161292\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:26<02:20,  3.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  314.94690052838996\n","Test Loss:  64.22400338202715\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:30<02:17,  3.36s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  309.5786986127496\n","Test Loss:  63.03609621711075\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:33<02:16,  3.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 1 of 5\n","Train Loss:  304.29576039500535\n","Test Loss:  65.3989856056869\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:37<02:12,  3.41s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 2 of 5\n","Train Loss:  297.76506414916366\n","Test Loss:  69.80784973315895\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:40<02:08,  3.39s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 3 of 5\n","Train Loss:  299.76402794755995\n","Test Loss:  75.35639755893499\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:43<02:04,  3.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 4 of 5\n","Train Loss:  296.5434185373597\n","Test Loss:  63.79106536274776\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:47<02:14,  3.64s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Counter 5 of 5\n","Train Loss:  288.2663215594366\n","Test Loss:  69.54457095451653\n","Early stopping with best_loss:  63.03609621711075 and test_loss for this epoch:  69.54457095451653 ...\n","mae_validation:\n","1.0215259727856307\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:03<03:00,  3.69s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  568.7592961955816\n","Test Loss:  77.12325402768329\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:07<02:57,  3.70s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  398.7831020075828\n","Test Loss:  76.76069181412458\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:11<02:55,  3.73s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  366.8091919706203\n","Test Loss:  71.47834909521043\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:14<02:51,  3.72s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  351.83486212510616\n","Test Loss:  69.32254057656974\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:18<02:46,  3.71s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss:  337.7494169222191\n","Test Loss:  66.08943027071655\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:22<02:43,  3.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  326.1900024712086\n","Test Loss:  66.22419223794714\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:26<02:40,  3.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  312.4043437973596\n","Test Loss:  69.89767307462171\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:29<02:36,  3.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  310.9029476963915\n","Test Loss:  67.46370932459831\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:33<02:32,  3.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  306.10113461269066\n","Test Loss:  74.26921822177246\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:37<02:49,  4.15s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  305.6361909820698\n","Test Loss:  68.33788884244859\n","Early stopping with best_loss:  66.08943027071655 and test_loss for this epoch:  68.33788884244859 ...\n","mae_validation:\n","1.0354920037579025\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]}]},{"cell_type":"code","source":["\n","#test_week = '2023-10-01'\n","#test_week = '2023-11-01'\n","#test_week = '2023-12-01'\n","test_week = '2023-09-30'\n","#test_week = '2024-02-01'\n","option = 0\n","summarytable,b = LSTM_train(test_week,option)\n","summarytable"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ueomiA5l7u5I","outputId":"ab8daaa2-c2f7-42b5-fdf6-538539523e50","executionInfo":{"status":"ok","timestamp":1713223887456,"user_tz":-480,"elapsed":2386870,"user":{"displayName":"Shawn XU","userId":"03794891599117456463"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:06,  2.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  533.8468897119164\n","Test Loss:  64.75391590129584\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:08,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  341.4048210764304\n","Test Loss:  58.83299034598167\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:08<02:06,  2.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  264.7575320189353\n","Test Loss:  54.3762166936649\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:10<02:01,  2.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  232.07966139307246\n","Test Loss:  51.32126997655723\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:13<01:58,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  212.6392536330968\n","Test Loss:  47.71031658916036\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:15<01:55,  2.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  195.6402524881414\n","Test Loss:  45.4508691834053\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:18<01:52,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  189.09327137097716\n","Test Loss:  42.27304002136225\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:21<01:50,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  181.3042071124073\n","Test Loss:  39.59585928381421\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:23<01:47,  2.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  170.4141482996638\n","Test Loss:  38.98238669105922\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:26<01:44,  2.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  170.2268162962282\n","Test Loss:  37.406153613119386\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:28<01:42,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  158.18355444082408\n","Test Loss:  35.89382206875598\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:31<01:40,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  153.0199713339971\n","Test Loss:  34.85337525757495\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:34<01:37,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  151.6096344597172\n","Test Loss:  33.65085390326567\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:36<01:35,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  148.28049983014353\n","Test Loss:  34.00144055893179\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:39<01:33,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  148.70924113906221\n","Test Loss:  32.24831745348638\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:42<01:30,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  146.25769323148415\n","Test Loss:  32.06451347123948\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:44<01:27,  2.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  139.4288252866827\n","Test Loss:  31.23965935590968\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:47<01:24,  2.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  135.68437840533443\n","Test Loss:  30.808743797941133\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:50<01:22,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  133.65933417362976\n","Test Loss:  30.90298192400951\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:52<01:19,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  133.4095275986765\n","Test Loss:  30.62496933233342\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [00:55<01:16,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  134.61971132305916\n","Test Loss:  30.43286013137549\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 22/50 [00:58<01:13,  2.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  133.14469543250743\n","Test Loss:  30.640022227104055\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 23/50 [01:00<01:10,  2.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  130.9234789784532\n","Test Loss:  30.104073804803193\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 24/50 [01:03<01:07,  2.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  128.0993848744256\n","Test Loss:  29.1045613035094\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 25/50 [01:05<01:05,  2.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  123.93861011177069\n","Test Loss:  29.532144710072316\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 26/50 [01:08<01:02,  2.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  128.5411997637566\n","Test Loss:  28.789535157673527\n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 27/50 [01:11<00:59,  2.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  126.99635021227004\n","Test Loss:  28.484263928767177\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 28/50 [01:13<00:57,  2.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  125.95120386467897\n","Test Loss:  28.247175056341803\n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 29/50 [01:16<00:54,  2.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  123.13512884645024\n","Test Loss:  30.322770621161908\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 30/50 [01:18<00:52,  2.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  123.81103083939524\n","Test Loss:  27.80510745002539\n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▏   | 31/50 [01:21<00:49,  2.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  127.6586655285937\n","Test Loss:  30.1028315743024\n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▍   | 32/50 [01:24<00:47,  2.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  124.67639236793912\n","Test Loss:  27.259647287719417\n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▌   | 33/50 [01:26<00:44,  2.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  119.63060657439928\n","Test Loss:  27.796664139052154\n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 34/50 [01:29<00:41,  2.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  122.59476921049645\n","Test Loss:  27.571787466586102\n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 35/50 [01:31<00:39,  2.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  122.84346730951802\n","Test Loss:  27.7659073031391\n"]},{"output_type":"stream","name":"stderr","text":["\r 72%|███████▏  | 36/50 [01:34<00:36,  2.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  124.64202701760223\n","Test Loss:  27.090544809238054\n"]},{"output_type":"stream","name":"stderr","text":["\r 74%|███████▍  | 37/50 [01:37<00:34,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  120.02941200138594\n","Test Loss:  27.074854691702058\n"]},{"output_type":"stream","name":"stderr","text":["\r 76%|███████▌  | 38/50 [01:40<00:32,  2.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  118.7069686933537\n","Test Loss:  27.638275545468787\n"]},{"output_type":"stream","name":"stderr","text":["\r 78%|███████▊  | 39/50 [01:42<00:29,  2.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  123.43123899394413\n","Test Loss:  27.53296516672708\n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|████████  | 40/50 [01:45<00:26,  2.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  119.31045125526725\n","Test Loss:  27.316676607937552\n"]},{"output_type":"stream","name":"stderr","text":["\r 82%|████████▏ | 41/50 [01:48<00:24,  2.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  115.9575918399496\n","Test Loss:  27.387449720001314\n"]},{"output_type":"stream","name":"stderr","text":["\r 84%|████████▍ | 42/50 [01:50<00:21,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  119.05970994688687\n","Test Loss:  26.92646699838224\n"]},{"output_type":"stream","name":"stderr","text":["\r 86%|████████▌ | 43/50 [01:53<00:18,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  118.9724348424279\n","Test Loss:  26.934636064863298\n"]},{"output_type":"stream","name":"stderr","text":["\r 88%|████████▊ | 44/50 [01:55<00:15,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  119.52465263433987\n","Test Loss:  26.587668975364068\n"]},{"output_type":"stream","name":"stderr","text":["\r 90%|█████████ | 45/50 [01:58<00:13,  2.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  120.60518791340292\n","Test Loss:  26.309267970340443\n"]},{"output_type":"stream","name":"stderr","text":["\r 92%|█████████▏| 46/50 [02:01<00:10,  2.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  115.56514228100423\n","Test Loss:  26.08867530807038\n"]},{"output_type":"stream","name":"stderr","text":["\r 94%|█████████▍| 47/50 [02:03<00:07,  2.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  118.54913787086844\n","Test Loss:  26.447310560848564\n"]},{"output_type":"stream","name":"stderr","text":["\r 96%|█████████▌| 48/50 [02:06<00:05,  2.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  116.57358269768883\n","Test Loss:  26.263736454740865\n"]},{"output_type":"stream","name":"stderr","text":["\r 98%|█████████▊| 49/50 [02:09<00:02,  2.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  118.0420161633374\n","Test Loss:  27.14961461146595\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [02:11<00:00,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  117.14893757463142\n","Test Loss:  26.654089689036482\n","mae_validation:\n","0.46477153831431456\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:16,  2.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  406.5370761808008\n","Test Loss:  66.00640429905616\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:11,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  266.43334878096357\n","Test Loss:  59.36206516204402\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:08<02:08,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  222.54763871862087\n","Test Loss:  53.84677204140462\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:10<02:05,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  200.74948183761444\n","Test Loss:  49.39630998764187\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:13<02:02,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  182.46929203683976\n","Test Loss:  43.674924269493204\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:16<01:59,  2.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  176.4248804138042\n","Test Loss:  41.078375044075074\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:19<01:56,  2.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  169.08183125487994\n","Test Loss:  37.77310785462032\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:21<01:53,  2.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  156.81525128654903\n","Test Loss:  37.67306370404549\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:24<01:50,  2.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  154.95657533913618\n","Test Loss:  33.13038650658564\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:27<01:48,  2.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  148.21356216078857\n","Test Loss:  32.70898621651577\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:29<01:46,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  142.31943474728905\n","Test Loss:  32.66269433806883\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:32<01:42,  2.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  146.08001846910338\n","Test Loss:  32.66375439855619\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:35<01:40,  2.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  140.52061237802263\n","Test Loss:  30.184731636749348\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:38<01:37,  2.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  135.39398794497538\n","Test Loss:  28.996340718324063\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:40<01:35,  2.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  132.43062337004812\n","Test Loss:  28.96907461574301\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:43<01:32,  2.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  131.13292992237257\n","Test Loss:  28.239643251523376\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:46<01:29,  2.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  131.0703668045462\n","Test Loss:  28.260890026867855\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:48<01:26,  2.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  128.69623739962117\n","Test Loss:  26.742393901076866\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:51<01:24,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  128.03902199401637\n","Test Loss:  26.89602893468691\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:54<01:21,  2.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  125.25503025273792\n","Test Loss:  25.84861036881921\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [00:57<01:18,  2.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  128.5305097135133\n","Test Loss:  26.20527508338273\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 22/50 [00:59<01:16,  2.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  124.47981532698032\n","Test Loss:  26.32226112828357\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 23/50 [01:02<01:13,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  124.13975040288642\n","Test Loss:  30.79679325595498\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 24/50 [01:05<01:10,  2.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  125.25977372791385\n","Test Loss:  25.522225859342143\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 25/50 [01:08<01:08,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  121.69157904617896\n","Test Loss:  25.55201259581372\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 26/50 [01:10<01:05,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  118.2158418490435\n","Test Loss:  26.11490660716663\n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 27/50 [01:13<01:02,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  119.22657124083344\n","Test Loss:  24.805801069334848\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 28/50 [01:16<00:59,  2.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  120.8748695772083\n","Test Loss:  24.638833333330695\n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 29/50 [01:18<00:57,  2.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  117.96000610469491\n","Test Loss:  26.211835352005437\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 30/50 [01:21<00:54,  2.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  116.50751628738362\n","Test Loss:  25.34794842658448\n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▏   | 31/50 [01:24<00:51,  2.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  121.35618608182995\n","Test Loss:  24.634152227197774\n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▍   | 32/50 [01:27<00:49,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  116.5291290191235\n","Test Loss:  25.77179425925715\n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▌   | 33/50 [01:29<00:46,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  115.18805119759054\n","Test Loss:  26.195115793379955\n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 34/50 [01:32<00:43,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  115.96112421178259\n","Test Loss:  25.331697953908588\n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 35/50 [01:35<00:40,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  116.52284314914141\n","Test Loss:  27.722189360079938\n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 35/50 [01:38<00:42,  2.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  113.98349302652059\n","Test Loss:  25.15312523691682\n","Early stopping with best_loss:  24.634152227197774 and test_loss for this epoch:  25.15312523691682 ...\n","mae_validation:\n","0.4405147034795709\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:16,  2.78s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  342.11755102314055\n","Test Loss:  59.978058765642345\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:11,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  250.2057431177236\n","Test Loss:  52.29748826625291\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:08<02:08,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  221.1979007779155\n","Test Loss:  46.494694013148546\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:10<02:05,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  197.53733969538007\n","Test Loss:  39.62162352743326\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:13<02:03,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  180.60478292126209\n","Test Loss:  36.41522379941307\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:16<02:01,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  170.40980302088428\n","Test Loss:  34.827883279154776\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:19<01:57,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  159.16382325990708\n","Test Loss:  33.995733992545865\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:22<01:56,  2.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  149.34255439325352\n","Test Loss:  28.031120824220125\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:24<01:53,  2.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  147.4634214626858\n","Test Loss:  27.220330114971148\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:27<01:50,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  142.65341854360304\n","Test Loss:  27.543058335664682\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:30<01:47,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  138.7793810919975\n","Test Loss:  25.82832037092885\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:33<01:45,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  132.07460117829032\n","Test Loss:  29.414160210930277\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:35<01:42,  2.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  136.04252648015972\n","Test Loss:  28.17224077496212\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:38<01:41,  2.82s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  129.75518868525978\n","Test Loss:  24.28130732360296\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:41<01:38,  2.81s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  134.4645944997901\n","Test Loss:  25.414261239318876\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:44<01:34,  2.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  128.45621784610557\n","Test Loss:  24.12778727116529\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:47<01:31,  2.78s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  129.09909966343548\n","Test Loss:  26.213346002041362\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:49<01:28,  2.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  123.9143659673573\n","Test Loss:  24.673928250849713\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:52<01:26,  2.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  125.08948129923374\n","Test Loss:  25.43265269190306\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:55<01:23,  2.78s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  125.33290767864673\n","Test Loss:  23.856053229013924\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [00:58<01:20,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  128.5368576325127\n","Test Loss:  26.572796574997483\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 22/50 [01:00<01:16,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  124.8385752219474\n","Test Loss:  24.05804490679293\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 23/50 [01:03<01:14,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  126.44247958116466\n","Test Loss:  23.56851823744364\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 24/50 [01:06<01:11,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  119.06225732236635\n","Test Loss:  23.08544713354786\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 25/50 [01:09<01:08,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  121.78224319437868\n","Test Loss:  23.16651497851126\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 26/50 [01:11<01:06,  2.78s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  120.47204786949442\n","Test Loss:  24.787090945523232\n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 27/50 [01:14<01:04,  2.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  115.50068173540058\n","Test Loss:  24.048623597729602\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 28/50 [01:17<01:01,  2.81s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  120.03017744139652\n","Test Loss:  23.667118525023398\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 28/50 [01:20<01:03,  2.87s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  120.69310460088309\n","Test Loss:  23.71767235250445\n","Early stopping with best_loss:  23.08544713354786 and test_loss for this epoch:  23.71767235250445 ...\n","mae_validation:\n","0.4390424155580921\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:26,  2.98s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  293.5130469731521\n","Test Loss:  54.129614523029886\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:20,  2.93s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  217.98815274320077\n","Test Loss:  45.419231367035536\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:08<02:17,  2.93s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  176.40341735142283\n","Test Loss:  37.05081444687676\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:11<02:15,  2.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  162.95076186372899\n","Test Loss:  34.94129560282454\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:14<02:12,  2.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  153.47029492392903\n","Test Loss:  32.80194764514454\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:17<02:09,  2.93s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  145.84349247804494\n","Test Loss:  31.793118689674884\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:20<02:05,  2.93s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  141.5350619663659\n","Test Loss:  32.24960891524097\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:23<02:02,  2.93s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  137.9222762787249\n","Test Loss:  30.45260464369494\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:26<02:00,  2.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  133.29724555110442\n","Test Loss:  28.403598640463315\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:29<01:57,  2.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  130.84807825117605\n","Test Loss:  31.295056200149702\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:32<01:55,  2.96s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  124.85708743505529\n","Test Loss:  27.09110651689116\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:35<01:52,  2.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  124.93002199832699\n","Test Loss:  30.113390779821202\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:38<01:48,  2.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  126.57238004740793\n","Test Loss:  27.694670274911914\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:41<01:45,  2.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  124.82189378817566\n","Test Loss:  27.429919467191212\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:44<01:43,  2.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  118.78228317329194\n","Test Loss:  25.960688978928374\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:47<01:42,  3.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  122.99566450325074\n","Test Loss:  26.329795633559115\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:50<01:39,  3.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  119.71366227493854\n","Test Loss:  26.205795875604963\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:53<01:37,  3.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  117.06182373221964\n","Test Loss:  25.57866373546858\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:56<01:32,  3.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  121.62999055511318\n","Test Loss:  25.743423760330188\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:59<01:29,  2.97s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  120.37259409938997\n","Test Loss:  25.618870080186753\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [01:02<01:26,  2.98s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  113.91669367137365\n","Test Loss:  27.15016577579081\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 22/50 [01:05<01:22,  2.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  113.03568238182925\n","Test Loss:  24.968333405442536\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 23/50 [01:08<01:19,  2.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  113.2290585355222\n","Test Loss:  25.596952131338185\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 24/50 [01:11<01:16,  2.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  110.31124910202925\n","Test Loss:  25.582775851507904\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 25/50 [01:13<01:13,  2.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  112.86395242714207\n","Test Loss:  24.197708004445303\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 26/50 [01:16<01:10,  2.93s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  113.79889869032195\n","Test Loss:  25.287900051218458\n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 27/50 [01:19<01:07,  2.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  112.10809480081662\n","Test Loss:  28.754214912827592\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 28/50 [01:22<01:04,  2.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  108.42138700716896\n","Test Loss:  24.83091311895987\n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 29/50 [01:25<01:01,  2.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  110.8142168908962\n","Test Loss:  24.10769270255696\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 30/50 [01:28<00:58,  2.93s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  107.9612345774367\n","Test Loss:  24.245921266570804\n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▏   | 31/50 [01:31<00:55,  2.92s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  105.7948928300757\n","Test Loss:  24.89561023999704\n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▍   | 32/50 [01:34<00:52,  2.93s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  107.25160687539028\n","Test Loss:  24.768903059128206\n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▌   | 33/50 [01:37<00:50,  2.98s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  105.83812322077574\n","Test Loss:  24.0581964419398\n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 34/50 [01:40<00:47,  2.97s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  106.4267802137474\n","Test Loss:  23.517433250512113\n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 35/50 [01:43<00:44,  2.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  108.01976820532582\n","Test Loss:  26.31501093573752\n"]},{"output_type":"stream","name":"stderr","text":["\r 72%|███████▏  | 36/50 [01:46<00:41,  2.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  103.13984898131457\n","Test Loss:  24.42942299169954\n"]},{"output_type":"stream","name":"stderr","text":["\r 74%|███████▍  | 37/50 [01:49<00:38,  2.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  102.47951131919399\n","Test Loss:  25.472482092445716\n"]},{"output_type":"stream","name":"stderr","text":["\r 76%|███████▌  | 38/50 [01:52<00:35,  2.92s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  104.91881139324687\n","Test Loss:  23.65984647293226\n"]},{"output_type":"stream","name":"stderr","text":["\r 76%|███████▌  | 38/50 [01:55<00:36,  3.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  101.3565529838379\n","Test Loss:  24.604310399503447\n","Early stopping with best_loss:  23.517433250512113 and test_loss for this epoch:  24.604310399503447 ...\n","mae_validation:\n","0.4268521621794503\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:07,  2.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  499.225798971951\n","Test Loss:  80.11268363590352\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:02,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  328.47021712362766\n","Test Loss:  73.194002646429\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:07<02:02,  2.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  248.08449652325362\n","Test Loss:  65.93337412009714\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:10<01:59,  2.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  213.82384650968015\n","Test Loss:  60.209442279301584\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:12<01:55,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  196.55222105880966\n","Test Loss:  57.170892737514805\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:15<01:53,  2.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  185.7440111784963\n","Test Loss:  53.117663102697406\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:18<01:50,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  176.06157733197324\n","Test Loss:  51.146526088385144\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:20<01:48,  2.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  167.9461407075869\n","Test Loss:  47.97832808678504\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:23<01:45,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  159.72379864304094\n","Test Loss:  46.332757372700144\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:25<01:42,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  154.66209884075215\n","Test Loss:  45.08263027362409\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:28<01:40,  2.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  149.9170568002737\n","Test Loss:  42.899700669862796\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:30<01:38,  2.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  144.63581439090194\n","Test Loss:  41.79715491179377\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:33<01:36,  2.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  138.40400621015579\n","Test Loss:  40.75597494235262\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:36<01:33,  2.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  138.60303587501403\n","Test Loss:  39.76703601432382\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:38<01:31,  2.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  137.50397613586392\n","Test Loss:  38.758841215691064\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:41<01:28,  2.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  135.2500761222327\n","Test Loss:  40.30285770748742\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:43<01:24,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  132.79906430761912\n","Test Loss:  37.46988315548515\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:46<01:21,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  130.9668067905004\n","Test Loss:  37.475147674034815\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:48<01:19,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  125.93706275615841\n","Test Loss:  36.765271344949724\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:51<01:16,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  126.47565919728368\n","Test Loss:  39.44027489097789\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [00:54<01:13,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  125.56076547258999\n","Test Loss:  34.96557778446004\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 22/50 [00:56<01:11,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  125.48297803747118\n","Test Loss:  34.64671650485252\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 23/50 [00:59<01:09,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  120.8426608646987\n","Test Loss:  34.81892215175321\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 24/50 [01:01<01:06,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  123.55230491154362\n","Test Loss:  37.40056160761742\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 25/50 [01:04<01:04,  2.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  120.49824756802991\n","Test Loss:  34.079135979001876\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 26/50 [01:06<01:01,  2.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  121.68020544649335\n","Test Loss:  33.23848991608247\n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 27/50 [01:09<00:59,  2.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  120.2275961053092\n","Test Loss:  33.48613814491546\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 28/50 [01:12<00:56,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  120.37627681688173\n","Test Loss:  33.504232717939885\n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 29/50 [01:14<00:53,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  118.28268841438694\n","Test Loss:  32.80123412865214\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 30/50 [01:17<00:51,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  118.36534145852784\n","Test Loss:  34.852733947336674\n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▏   | 31/50 [01:19<00:48,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  114.35098657908384\n","Test Loss:  34.84167797799455\n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▍   | 32/50 [01:22<00:45,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  112.52044674969511\n","Test Loss:  34.78859804326203\n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▌   | 33/50 [01:24<00:43,  2.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  109.43993870579288\n","Test Loss:  33.90283155735233\n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▌   | 33/50 [01:27<00:45,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  116.20937591127586\n","Test Loss:  36.592841307399794\n","Early stopping with best_loss:  32.80123412865214 and test_loss for this epoch:  36.592841307399794 ...\n","mae_validation:\n","0.55899496721433\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:08,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  406.70762415230274\n","Test Loss:  62.556787518784404\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:08,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  264.26014840835705\n","Test Loss:  51.45066607449553\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:07<02:05,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  213.38496399507858\n","Test Loss:  45.267128720646724\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:10<02:03,  2.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  193.49155201320536\n","Test Loss:  39.83511271252064\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:13<01:59,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  180.7881184074795\n","Test Loss:  39.44196072273189\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:15<01:56,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  170.72109464392997\n","Test Loss:  34.218135723494925\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:18<01:53,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  163.04191861324944\n","Test Loss:  38.51987456122879\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:21<01:51,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  154.1845958395861\n","Test Loss:  33.42759632039815\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:23<01:48,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  147.49353702244116\n","Test Loss:  30.79798135883175\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:26<01:45,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  146.64729091885965\n","Test Loss:  31.90647360944422\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:29<01:42,  2.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  134.6744331914815\n","Test Loss:  30.4916722929338\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:31<01:40,  2.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  139.48003784916364\n","Test Loss:  32.48226787720341\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:34<01:37,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  134.015766827506\n","Test Loss:  30.910995250043925\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:36<01:34,  2.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  133.03266358503606\n","Test Loss:  31.015132680651732\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:39<01:31,  2.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  131.3948124917224\n","Test Loss:  28.97076241351897\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:42<01:29,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  132.05047239236592\n","Test Loss:  31.521433471381897\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:44<01:26,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  124.0080615291954\n","Test Loss:  28.104486965632532\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:47<01:24,  2.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  127.83187494601589\n","Test Loss:  29.44192766363267\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:50<01:21,  2.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  125.5543300769059\n","Test Loss:  29.06647272533155\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:52<01:18,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  120.54597627866315\n","Test Loss:  29.672401686315425\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [00:55<01:16,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  121.05606950349465\n","Test Loss:  32.19534160135663\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 22/50 [00:58<01:13,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  123.64861078525428\n","Test Loss:  27.907222236681264\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 23/50 [01:00<01:11,  2.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  122.35439846050576\n","Test Loss:  28.789138676424045\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 24/50 [01:03<01:08,  2.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  121.30115000723163\n","Test Loss:  27.160091277532047\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 25/50 [01:05<01:05,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  119.87955414620228\n","Test Loss:  27.201487028214615\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 26/50 [01:08<01:03,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  118.48350687650964\n","Test Loss:  27.933481309737545\n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 27/50 [01:11<01:00,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  116.16704013448907\n","Test Loss:  29.871008333051577\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 28/50 [01:13<00:58,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  118.42469083031756\n","Test Loss:  29.523004186165053\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 28/50 [01:16<01:00,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  116.22262443351792\n","Test Loss:  29.75448601404787\n","Early stopping with best_loss:  27.160091277532047 and test_loss for this epoch:  29.75448601404787 ...\n","mae_validation:\n","0.4945836792724623\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:09,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  325.76641522999853\n","Test Loss:  60.0757166063413\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:08,  2.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  232.4569705077447\n","Test Loss:  49.92025502119213\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:08<02:05,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  197.92629913205747\n","Test Loss:  44.37603202991886\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:10<02:03,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  176.68284357120865\n","Test Loss:  40.92728646247997\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:13<01:59,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  166.19175074092345\n","Test Loss:  44.88382801739499\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:16<01:57,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  158.90479136351496\n","Test Loss:  38.19793949028826\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:18<01:54,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  148.9220007560798\n","Test Loss:  36.16888830959215\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:21<01:51,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  143.34983482037205\n","Test Loss:  34.71445869866875\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:24<01:49,  2.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  140.7789888746338\n","Test Loss:  35.118530038686\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:26<01:47,  2.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  139.5275351149612\n","Test Loss:  35.574870870797895\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:29<01:44,  2.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  134.92354956932832\n","Test Loss:  31.965158564795274\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:32<01:42,  2.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  130.70839297532802\n","Test Loss:  33.81360188379767\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:34<01:39,  2.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  129.8270158736268\n","Test Loss:  30.934793208551127\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:37<01:36,  2.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  123.18679844084545\n","Test Loss:  30.77608060342027\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:40<01:33,  2.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  125.26022592943627\n","Test Loss:  32.23735532403225\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:42<01:31,  2.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  120.4553986947285\n","Test Loss:  30.558448602387216\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:45<01:28,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  124.29099992808187\n","Test Loss:  30.532775235245936\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:48<01:25,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  122.71040069719311\n","Test Loss:  29.829966769713792\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:50<01:22,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  119.84230300466879\n","Test Loss:  29.289395475090714\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:53<01:19,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  120.31488829344744\n","Test Loss:  31.396394113020506\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [00:56<01:18,  2.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  116.20201875636121\n","Test Loss:  28.230773565679556\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 22/50 [00:58<01:15,  2.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  119.35455883911345\n","Test Loss:  30.11612729785702\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 23/50 [01:01<01:11,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  115.735999904864\n","Test Loss:  28.968635449942667\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 24/50 [01:04<01:09,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  117.02966639594524\n","Test Loss:  28.955644888832467\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 25/50 [01:06<01:06,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  112.00244422754622\n","Test Loss:  28.91258746793028\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 26/50 [01:09<01:03,  2.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  111.26448379046633\n","Test Loss:  27.3166505247209\n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 27/50 [01:12<01:00,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  110.6516008511826\n","Test Loss:  27.804957214335445\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 28/50 [01:14<00:58,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  111.67314757462009\n","Test Loss:  28.050427030248102\n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 29/50 [01:17<00:56,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  110.12196709090495\n","Test Loss:  27.098298196709948\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 30/50 [01:20<00:52,  2.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  108.97312625480117\n","Test Loss:  26.48023916594684\n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▏   | 31/50 [01:22<00:49,  2.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  107.34201128475252\n","Test Loss:  28.26393856319919\n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▍   | 32/50 [01:25<00:47,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  107.6022728747339\n","Test Loss:  28.13107034546556\n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▌   | 33/50 [01:28<00:45,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  109.48879013734404\n","Test Loss:  27.401149312383495\n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 34/50 [01:30<00:42,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  110.17369900314952\n","Test Loss:  29.640573152224533\n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 34/50 [01:33<00:43,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  105.81951474718517\n","Test Loss:  26.658501258556498\n","Early stopping with best_loss:  26.48023916594684 and test_loss for this epoch:  26.658501258556498 ...\n","mae_validation:\n","0.4904329979877506\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:19,  2.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  292.57264575501904\n","Test Loss:  58.51981099846307\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:18,  2.88s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  213.91634910344146\n","Test Loss:  46.701917677768506\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:08<02:16,  2.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  181.04195376869757\n","Test Loss:  41.35945910977898\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:11<02:14,  2.92s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  159.24700102233328\n","Test Loss:  38.219420131645165\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:14<02:11,  2.92s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  148.38129302940797\n","Test Loss:  35.09818545455346\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:17<02:08,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  145.16248528280994\n","Test Loss:  34.042933299264405\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:20<02:04,  2.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  142.0310741585272\n","Test Loss:  32.99829074338777\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:23<02:02,  2.92s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  130.73345618409803\n","Test Loss:  33.15800866461359\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:26<02:00,  2.93s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  132.43231854843907\n","Test Loss:  32.18186332890764\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:29<01:56,  2.92s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  125.37008571915794\n","Test Loss:  31.48564586028806\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:32<01:53,  2.92s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  124.76373400716693\n","Test Loss:  30.224402828433085\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:34<01:50,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  123.2889408110932\n","Test Loss:  30.73471254183096\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:37<01:47,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  118.9330904558592\n","Test Loss:  28.716219641908538\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:40<01:44,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  116.75798171566566\n","Test Loss:  29.89324599155225\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:43<01:41,  2.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  117.45039266414824\n","Test Loss:  30.00686897567357\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:46<01:38,  2.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  116.94350557497819\n","Test Loss:  29.539967358781723\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:49<01:35,  2.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  117.05996535229497\n","Test Loss:  27.968714919523336\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:52<01:32,  2.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  115.81035769634764\n","Test Loss:  29.041502921812935\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:55<01:29,  2.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  117.69831140805036\n","Test Loss:  27.997331891325302\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:58<01:26,  2.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  113.24650235197623\n","Test Loss:  26.79536527072196\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [01:00<01:23,  2.88s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  111.70527607068652\n","Test Loss:  27.319315822634962\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 22/50 [01:03<01:20,  2.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  111.16763812791032\n","Test Loss:  26.907495788356755\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 23/50 [01:06<01:18,  2.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  108.61965185511508\n","Test Loss:  27.224845989316236\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 24/50 [01:09<01:15,  2.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  105.72091540758265\n","Test Loss:  26.848374005159712\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 24/50 [01:12<01:18,  3.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  108.67707168700872\n","Test Loss:  26.994660547366948\n","Early stopping with best_loss:  26.79536527072196 and test_loss for this epoch:  26.994660547366948 ...\n","mae_validation:\n","0.473990840782929\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<01:58,  2.41s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  544.8133274242282\n","Test Loss:  65.34866469353437\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:04<01:57,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  364.547574872151\n","Test Loss:  57.12210243707523\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:07<01:55,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  280.63552832137793\n","Test Loss:  51.36583940242417\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:09<01:53,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  230.84723549941555\n","Test Loss:  45.98525773570873\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:12<01:51,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  204.81564928113949\n","Test Loss:  45.758108780602925\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:14<01:48,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  184.96200760139618\n","Test Loss:  41.392373144219164\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:17<01:46,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  179.80320170032792\n","Test Loss:  39.80067875538953\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:19<01:44,  2.50s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  171.48666719940957\n","Test Loss:  38.12979375576833\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:22<01:42,  2.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  167.51659963693237\n","Test Loss:  37.74930569285061\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:24<01:39,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  153.71934910386335\n","Test Loss:  36.298645636299625\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:27<01:36,  2.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  154.57778642710764\n","Test Loss:  34.959511780994944\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:29<01:34,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  149.17938019888243\n","Test Loss:  34.20394877658691\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:32<01:31,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  141.18784912879346\n","Test Loss:  33.25771255174186\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:34<01:27,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  143.09814898500917\n","Test Loss:  33.0973715755099\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:36<01:25,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  139.38493614597246\n","Test Loss:  32.787006143524195\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:39<01:23,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  139.61913517402718\n","Test Loss:  33.33002318139188\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:41<01:21,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  134.15580247479375\n","Test Loss:  31.699615868215915\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:44<01:19,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  134.0619359324919\n","Test Loss:  31.687172746052966\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:46<01:16,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  134.43176457780646\n","Test Loss:  31.655557235586457\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:49<01:14,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  131.64676128292922\n","Test Loss:  30.695846674963832\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [00:51<01:12,  2.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  127.81012281164294\n","Test Loss:  30.00554774919874\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 22/50 [00:54<01:09,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  126.58243958587991\n","Test Loss:  31.03563220181968\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 23/50 [00:56<01:06,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  123.01562871475471\n","Test Loss:  29.61529807851184\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 24/50 [00:59<01:04,  2.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  122.90286887734692\n","Test Loss:  29.40554644895019\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 25/50 [01:01<01:02,  2.50s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  121.61705213077948\n","Test Loss:  29.016623744479148\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 26/50 [01:04<01:00,  2.50s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  127.77343424159335\n","Test Loss:  29.619103018252645\n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 27/50 [01:06<00:57,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  120.8485399846686\n","Test Loss:  30.06213463455788\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 28/50 [01:09<00:54,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  122.27487744562677\n","Test Loss:  29.07991352066165\n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 29/50 [01:11<00:51,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  120.93838437166414\n","Test Loss:  28.88490180396184\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 30/50 [01:14<00:49,  2.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  120.93545685175923\n","Test Loss:  27.96275732968934\n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▏   | 31/50 [01:16<00:47,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  116.02280822215835\n","Test Loss:  28.524348512670258\n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▍   | 32/50 [01:19<00:44,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  116.75320691923844\n","Test Loss:  27.209464210551232\n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▌   | 33/50 [01:21<00:41,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  115.24258669122355\n","Test Loss:  27.69359685962263\n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 34/50 [01:24<00:39,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  116.18431075190892\n","Test Loss:  28.176433079119306\n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 35/50 [01:26<00:36,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  115.72121277477709\n","Test Loss:  28.286490819969913\n"]},{"output_type":"stream","name":"stderr","text":["\r 72%|███████▏  | 36/50 [01:28<00:34,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  113.72003720627981\n","Test Loss:  27.47389296477195\n"]},{"output_type":"stream","name":"stderr","text":["\r 74%|███████▍  | 37/50 [01:31<00:32,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  112.93770820030477\n","Test Loss:  27.02702889974171\n"]},{"output_type":"stream","name":"stderr","text":["\r 76%|███████▌  | 38/50 [01:33<00:29,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  109.95252762257587\n","Test Loss:  27.4120349586301\n"]},{"output_type":"stream","name":"stderr","text":["\r 78%|███████▊  | 39/50 [01:36<00:27,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  115.89373377407901\n","Test Loss:  27.33307243033778\n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|████████  | 40/50 [01:38<00:24,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  112.59271043288754\n","Test Loss:  26.632746880757622\n"]},{"output_type":"stream","name":"stderr","text":["\r 82%|████████▏ | 41/50 [01:41<00:22,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  108.25758661516011\n","Test Loss:  26.38338364844094\n"]},{"output_type":"stream","name":"stderr","text":["\r 84%|████████▍ | 42/50 [01:43<00:19,  2.50s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  109.68613537665806\n","Test Loss:  26.998010073410114\n"]},{"output_type":"stream","name":"stderr","text":["\r 86%|████████▌ | 43/50 [01:46<00:17,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  111.10847610415658\n","Test Loss:  26.41666197797167\n"]},{"output_type":"stream","name":"stderr","text":["\r 88%|████████▊ | 44/50 [01:48<00:14,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  110.82269985499443\n","Test Loss:  25.947693643160164\n"]},{"output_type":"stream","name":"stderr","text":["\r 90%|█████████ | 45/50 [01:51<00:12,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  109.43195417776587\n","Test Loss:  26.896492855245015\n"]},{"output_type":"stream","name":"stderr","text":["\r 92%|█████████▏| 46/50 [01:53<00:09,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  110.24697116861353\n","Test Loss:  27.905834760749713\n"]},{"output_type":"stream","name":"stderr","text":["\r 94%|█████████▍| 47/50 [01:56<00:07,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  105.60773974453332\n","Test Loss:  28.517018533762894\n"]},{"output_type":"stream","name":"stderr","text":["\r 96%|█████████▌| 48/50 [01:58<00:04,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  107.21303349503432\n","Test Loss:  25.333342519326834\n"]},{"output_type":"stream","name":"stderr","text":["\r 98%|█████████▊| 49/50 [02:01<00:02,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  107.37028957123403\n","Test Loss:  26.293536288721953\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [02:03<00:00,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  108.60501456423663\n","Test Loss:  25.81555763812503\n","mae_validation:\n","0.4835059719915185\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:04,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  387.7860867921263\n","Test Loss:  67.3658918129513\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:02,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  263.41857052221894\n","Test Loss:  54.03901868034154\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:07<01:59,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  211.6929098876426\n","Test Loss:  45.6376370700018\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:10<01:58,  2.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  192.8506850064732\n","Test Loss:  40.528147603123216\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:12<01:56,  2.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  174.74011550610885\n","Test Loss:  38.24783633823972\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:15<01:52,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  168.06103950482793\n","Test Loss:  36.09139289113227\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:17<01:50,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  156.1799575779878\n","Test Loss:  34.92714639179758\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:20<01:47,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  150.61291749298107\n","Test Loss:  36.3912587154191\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:23<01:44,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  145.8198755303747\n","Test Loss:  33.93355052836705\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:25<01:41,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  142.45006655383622\n","Test Loss:  34.57553776484565\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:28<01:38,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  136.86245357786538\n","Test Loss:  34.64974451297894\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:30<01:35,  2.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  136.37301674479386\n","Test Loss:  31.830471627181396\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:33<01:33,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  132.19401986926096\n","Test Loss:  31.433092234190553\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:35<01:31,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  134.69877710874425\n","Test Loss:  36.83289401023649\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:38<01:28,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  130.3521809031954\n","Test Loss:  32.62148296897067\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:40<01:27,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  125.29044605582021\n","Test Loss:  30.72362971137045\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:43<01:25,  2.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  125.73978755643475\n","Test Loss:  30.507967409328558\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:45<01:22,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  122.00758807125385\n","Test Loss:  30.274420915171504\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:48<01:19,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  120.56292165289051\n","Test Loss:  28.198661383765284\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:51<01:16,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  120.2559266485041\n","Test Loss:  29.78110322309658\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [00:53<01:13,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  118.99596949404804\n","Test Loss:  29.388940694509074\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 22/50 [00:56<01:11,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  118.22749850567197\n","Test Loss:  28.008690967108123\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 23/50 [00:58<01:08,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  117.44452250603354\n","Test Loss:  28.442101054941304\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 24/50 [01:01<01:06,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  118.55904874840053\n","Test Loss:  27.396976997551974\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 25/50 [01:03<01:03,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  116.29723514855141\n","Test Loss:  31.631471219101513\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 26/50 [01:06<01:00,  2.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  116.2093675104552\n","Test Loss:  27.097450232424308\n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 27/50 [01:08<00:57,  2.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  117.25146302598296\n","Test Loss:  27.87167004076764\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 28/50 [01:11<00:55,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  114.83679891994689\n","Test Loss:  26.42487865423027\n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 29/50 [01:13<00:52,  2.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  114.91480711690383\n","Test Loss:  30.541073604894336\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 30/50 [01:16<00:50,  2.51s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  111.31638434942579\n","Test Loss:  26.36310689605307\n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▏   | 31/50 [01:18<00:48,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  107.45771705932566\n","Test Loss:  26.350966633763164\n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▍   | 32/50 [01:21<00:45,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  114.06297516729683\n","Test Loss:  25.76798499855795\n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▌   | 33/50 [01:23<00:43,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  116.10766563180368\n","Test Loss:  27.986278148251586\n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 34/50 [01:26<00:40,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  105.29072631339659\n","Test Loss:  25.484444416302722\n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 35/50 [01:28<00:37,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  111.56763725174824\n","Test Loss:  27.161811972007854\n"]},{"output_type":"stream","name":"stderr","text":["\r 72%|███████▏  | 36/50 [01:31<00:35,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  112.78677048144164\n","Test Loss:  25.171516377973603\n"]},{"output_type":"stream","name":"stderr","text":["\r 74%|███████▍  | 37/50 [01:34<00:32,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  107.01642578409519\n","Test Loss:  27.20888108907093\n"]},{"output_type":"stream","name":"stderr","text":["\r 76%|███████▌  | 38/50 [01:36<00:30,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  109.08859800489154\n","Test Loss:  25.159302146930713\n"]},{"output_type":"stream","name":"stderr","text":["\r 78%|███████▊  | 39/50 [01:39<00:27,  2.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  104.04962115248782\n","Test Loss:  26.880261205951683\n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|████████  | 40/50 [01:41<00:25,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  110.47447612477117\n","Test Loss:  26.28641070012236\n"]},{"output_type":"stream","name":"stderr","text":["\r 82%|████████▏ | 41/50 [01:44<00:22,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  103.7860458187206\n","Test Loss:  26.868782107165316\n"]},{"output_type":"stream","name":"stderr","text":["\r 84%|████████▍ | 42/50 [01:46<00:20,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  106.10758258562419\n","Test Loss:  25.437251386465505\n"]},{"output_type":"stream","name":"stderr","text":["\r 86%|████████▌ | 43/50 [01:49<00:17,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  109.5811435088981\n","Test Loss:  24.450634788925527\n"]},{"output_type":"stream","name":"stderr","text":["\r 88%|████████▊ | 44/50 [01:51<00:15,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  106.24471825413639\n","Test Loss:  25.49315625691088\n"]},{"output_type":"stream","name":"stderr","text":["\r 90%|█████████ | 45/50 [01:54<00:12,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  105.22061748236592\n","Test Loss:  27.688171705922286\n"]},{"output_type":"stream","name":"stderr","text":["\r 92%|█████████▏| 46/50 [01:56<00:10,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  103.51127793538035\n","Test Loss:  26.92227070624358\n"]},{"output_type":"stream","name":"stderr","text":["\r 94%|█████████▍| 47/50 [01:59<00:07,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  104.7180922589614\n","Test Loss:  26.106786737043876\n"]},{"output_type":"stream","name":"stderr","text":["\r 94%|█████████▍| 47/50 [02:01<00:07,  2.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  107.42335291014751\n","Test Loss:  25.65584599575959\n","Early stopping with best_loss:  24.450634788925527 and test_loss for this epoch:  25.65584599575959 ...\n","mae_validation:\n","0.4672539915404764\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:05,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  322.7603524485603\n","Test Loss:  61.34016881673597\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:02,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  224.68817145598587\n","Test Loss:  47.658182688988745\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:07<01:59,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  189.52330230170628\n","Test Loss:  42.05549731408246\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:10<01:57,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  169.83777930453653\n","Test Loss:  41.84370396146551\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:12<01:54,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  155.64275298570283\n","Test Loss:  37.012299301684834\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:15<01:52,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  147.89253476972226\n","Test Loss:  34.6425810859364\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:17<01:50,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  145.99199087818852\n","Test Loss:  36.02411171270069\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:20<01:47,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  139.54286259692162\n","Test Loss:  34.323209417634644\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:23<01:44,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  137.64129541805596\n","Test Loss:  34.79892078426201\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:25<01:41,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  134.29028548311908\n","Test Loss:  33.87093569798162\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:28<01:39,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  132.06534099255805\n","Test Loss:  32.83987472386798\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:30<01:36,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  125.77490040374687\n","Test Loss:  31.25193386396859\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:33<01:34,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  127.77059964585351\n","Test Loss:  30.110185259283753\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:35<01:31,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  125.52190777170472\n","Test Loss:  29.21526587520202\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:38<01:29,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  121.90685657420545\n","Test Loss:  29.579984547774075\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:40<01:27,  2.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  117.00272136967396\n","Test Loss:  30.31281986052636\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:43<01:25,  2.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  118.98538801702671\n","Test Loss:  29.16568890097551\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:46<01:22,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  117.01137528184336\n","Test Loss:  29.57120748827583\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:48<01:19,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  117.41238109354163\n","Test Loss:  29.782455511798616\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:51<01:16,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  114.26839389657835\n","Test Loss:  31.52917394239921\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [00:53<01:14,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  113.68035330795101\n","Test Loss:  29.079107539640972\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 22/50 [00:56<01:11,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  114.57550296428963\n","Test Loss:  27.59186133244657\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 23/50 [00:58<01:09,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  109.1973732644692\n","Test Loss:  26.94828302357928\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 24/50 [01:01<01:06,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  110.8891785738524\n","Test Loss:  27.37426968102227\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 25/50 [01:04<01:04,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  107.0166859618621\n","Test Loss:  26.618740787322167\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 26/50 [01:06<01:01,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  112.11593173217261\n","Test Loss:  26.67735201746109\n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 27/50 [01:09<00:58,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  106.49000892136246\n","Test Loss:  33.32563813054003\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 28/50 [01:11<00:56,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  111.90206821562606\n","Test Loss:  27.387501488265116\n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 29/50 [01:14<00:53,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  104.70050629007164\n","Test Loss:  25.57944573587156\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 30/50 [01:16<00:51,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  105.40300819359254\n","Test Loss:  25.500840287073515\n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▏   | 31/50 [01:19<00:48,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  106.2126784461434\n","Test Loss:  27.032652032386977\n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▍   | 32/50 [01:21<00:46,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  107.65995670498523\n","Test Loss:  27.254725525330286\n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▌   | 33/50 [01:24<00:43,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  106.30849832561216\n","Test Loss:  29.66182068415219\n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 34/50 [01:27<00:41,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  104.18902804062236\n","Test Loss:  25.598464482231066\n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 34/50 [01:29<00:42,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  103.28162967853132\n","Test Loss:  25.525281441572588\n","Early stopping with best_loss:  25.500840287073515 and test_loss for this epoch:  25.525281441572588 ...\n","mae_validation:\n","0.47713682597536494\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:16,  2.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  290.79466591239907\n","Test Loss:  51.00172693259083\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:14,  2.81s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  205.91397837130353\n","Test Loss:  39.364740150631405\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:08<02:14,  2.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  176.08126125717536\n","Test Loss:  35.37194426666247\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:11<02:12,  2.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  163.7197432074463\n","Test Loss:  33.94402673316654\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:14<02:07,  2.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  153.33674195222557\n","Test Loss:  31.407134400273208\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:17<02:05,  2.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  139.2772167516814\n","Test Loss:  34.88320690958062\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:19<02:02,  2.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  136.4734567804262\n","Test Loss:  35.27331397717353\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:22<01:59,  2.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  130.95489916275255\n","Test Loss:  29.240168279386126\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:25<01:57,  2.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  134.65143646791694\n","Test Loss:  32.89671808795538\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:28<01:54,  2.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  124.50533329328755\n","Test Loss:  28.356351779890247\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:31<01:51,  2.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  125.13194903405383\n","Test Loss:  27.224092049029423\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:34<01:48,  2.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  123.08181438207976\n","Test Loss:  29.5808873345959\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:37<01:45,  2.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  119.70659307204187\n","Test Loss:  30.94137279986171\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:39<01:42,  2.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  120.4578772460809\n","Test Loss:  26.309714892151533\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:42<01:40,  2.87s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  117.31415395464865\n","Test Loss:  28.71349431760609\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:45<01:37,  2.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  116.64897620282136\n","Test Loss:  26.019512154744007\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:48<01:33,  2.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  111.40125830436591\n","Test Loss:  26.059647203190252\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:51<01:30,  2.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  107.6190173541545\n","Test Loss:  25.40669959707884\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:54<01:27,  2.83s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  112.31689072810696\n","Test Loss:  25.497029073827434\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:56<01:25,  2.83s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  110.80947202198149\n","Test Loss:  27.743274710606784\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [00:59<01:22,  2.83s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  107.19038427621126\n","Test Loss:  27.32390669186134\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 22/50 [01:02<01:19,  2.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  103.3712676585128\n","Test Loss:  24.532970634376397\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 23/50 [01:05<01:16,  2.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  104.38881449803011\n","Test Loss:  25.055620359169552\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 24/50 [01:08<01:13,  2.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  106.90978833321424\n","Test Loss:  25.670799856845406\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 25/50 [01:11<01:11,  2.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  104.14049755809538\n","Test Loss:  23.750461081654066\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 26/50 [01:14<01:08,  2.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  103.50128803402185\n","Test Loss:  25.55112254974665\n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 27/50 [01:16<01:05,  2.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  103.69446281825367\n","Test Loss:  25.391594593893387\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 28/50 [01:19<01:02,  2.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  101.38119475342683\n","Test Loss:  25.007524968124926\n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 29/50 [01:22<00:59,  2.83s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  99.75524794272496\n","Test Loss:  27.18633918123669\n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 29/50 [01:25<01:01,  2.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  102.03841192324762\n","Test Loss:  24.373319000995252\n","Early stopping with best_loss:  23.750461081654066 and test_loss for this epoch:  24.373319000995252 ...\n","mae_validation:\n","0.4578403248872103\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<01:58,  2.41s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  490.4884805046022\n","Test Loss:  72.71892279572785\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:04<01:56,  2.42s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  334.62468890845776\n","Test Loss:  58.62920141615905\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:07<01:53,  2.40s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  242.92916907090694\n","Test Loss:  49.50992707343539\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:09<01:50,  2.40s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  203.9970307883341\n","Test Loss:  47.07434906024719\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:11<01:47,  2.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  180.82388220983557\n","Test Loss:  42.948660963447765\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:14<01:45,  2.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  176.3551820954308\n","Test Loss:  40.50388797384221\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:16<01:43,  2.41s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  162.56129566277377\n","Test Loss:  37.65180278365733\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:19<01:40,  2.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  157.48112361406675\n","Test Loss:  36.7520661081071\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:21<01:37,  2.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  149.65930802864023\n","Test Loss:  36.5958254027646\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:23<01:34,  2.37s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  141.87834669311997\n","Test Loss:  33.843276877421886\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:26<01:32,  2.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  139.34941849461757\n","Test Loss:  33.62210920263897\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:28<01:30,  2.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  142.5338455546298\n","Test Loss:  33.96029250102583\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:31<01:28,  2.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  138.43174163991353\n","Test Loss:  31.657617517281324\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:33<01:25,  2.36s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  137.49115652707405\n","Test Loss:  31.380254197589238\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:35<01:22,  2.36s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  135.1114316129242\n","Test Loss:  31.407147728139535\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:38<01:20,  2.36s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  128.22005578747485\n","Test Loss:  30.88641117195948\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:40<01:18,  2.37s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  124.4830196260009\n","Test Loss:  30.958103552431567\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:42<01:15,  2.37s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  127.68597489772947\n","Test Loss:  30.477280753519153\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:45<01:13,  2.37s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  124.17744441085961\n","Test Loss:  29.72973375703441\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:47<01:11,  2.40s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  122.66012195748044\n","Test Loss:  29.845102208069875\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [00:50<01:09,  2.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  120.02561583620263\n","Test Loss:  29.330125839915127\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 22/50 [00:52<01:06,  2.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  117.18613569971058\n","Test Loss:  28.639479736157227\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 23/50 [00:54<01:04,  2.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  118.52082698020968\n","Test Loss:  28.64275889378041\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 24/50 [00:57<01:02,  2.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  118.4375500344031\n","Test Loss:  28.243655353900976\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 25/50 [00:59<00:59,  2.40s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  115.51728923956398\n","Test Loss:  28.335161653347313\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 26/50 [01:02<00:57,  2.40s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  113.288454888243\n","Test Loss:  28.16629245012882\n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 27/50 [01:04<00:55,  2.41s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  117.86884192777507\n","Test Loss:  27.65051189722726\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 28/50 [01:06<00:52,  2.40s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  113.88304236298427\n","Test Loss:  27.849131248833146\n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 29/50 [01:09<00:50,  2.40s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  112.5868058038177\n","Test Loss:  27.644286381459096\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 30/50 [01:11<00:47,  2.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  116.36728017681162\n","Test Loss:  27.587014176591765\n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▏   | 31/50 [01:14<00:45,  2.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  110.68881193155539\n","Test Loss:  27.395965744297428\n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▍   | 32/50 [01:16<00:43,  2.41s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  110.41253298515221\n","Test Loss:  28.196388345968444\n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▌   | 33/50 [01:18<00:40,  2.41s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  109.70825047674589\n","Test Loss:  27.36733360105427\n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 34/50 [01:21<00:38,  2.40s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  110.63176517770626\n","Test Loss:  27.339344454172533\n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 35/50 [01:23<00:36,  2.42s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  108.80104696523631\n","Test Loss:  26.85651564973523\n"]},{"output_type":"stream","name":"stderr","text":["\r 72%|███████▏  | 36/50 [01:26<00:33,  2.41s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  109.38074271078221\n","Test Loss:  27.46432931284653\n"]},{"output_type":"stream","name":"stderr","text":["\r 74%|███████▍  | 37/50 [01:28<00:31,  2.40s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  108.6510616474261\n","Test Loss:  26.81218692258699\n"]},{"output_type":"stream","name":"stderr","text":["\r 76%|███████▌  | 38/50 [01:30<00:28,  2.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  107.17518963679322\n","Test Loss:  27.134221738844644\n"]},{"output_type":"stream","name":"stderr","text":["\r 78%|███████▊  | 39/50 [01:33<00:26,  2.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  103.89782711112639\n","Test Loss:  26.24522210059513\n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|████████  | 40/50 [01:35<00:23,  2.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  107.22568032503477\n","Test Loss:  26.420120051538106\n"]},{"output_type":"stream","name":"stderr","text":["\r 82%|████████▏ | 41/50 [01:38<00:21,  2.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  105.82014902945957\n","Test Loss:  26.383037256542593\n"]},{"output_type":"stream","name":"stderr","text":["\r 84%|████████▍ | 42/50 [01:40<00:19,  2.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  103.9943078433862\n","Test Loss:  26.83794842925272\n"]},{"output_type":"stream","name":"stderr","text":["\r 86%|████████▌ | 43/50 [01:42<00:16,  2.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  103.81931247758621\n","Test Loss:  26.70553260948509\n"]},{"output_type":"stream","name":"stderr","text":["\r 88%|████████▊ | 44/50 [01:45<00:14,  2.37s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  105.10718810220715\n","Test Loss:  25.73386133162421\n"]},{"output_type":"stream","name":"stderr","text":["\r 90%|█████████ | 45/50 [01:47<00:11,  2.40s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  103.97196412933408\n","Test Loss:  25.584503297694027\n"]},{"output_type":"stream","name":"stderr","text":["\r 92%|█████████▏| 46/50 [01:49<00:09,  2.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  104.34492374485126\n","Test Loss:  25.202834592026193\n"]},{"output_type":"stream","name":"stderr","text":["\r 94%|█████████▍| 47/50 [01:52<00:07,  2.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  104.43664158089086\n","Test Loss:  24.805493850784842\n"]},{"output_type":"stream","name":"stderr","text":["\r 96%|█████████▌| 48/50 [01:54<00:04,  2.37s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  101.73705982803949\n","Test Loss:  24.984242054109927\n"]},{"output_type":"stream","name":"stderr","text":["\r 98%|█████████▊| 49/50 [01:56<00:02,  2.37s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  104.20309075148543\n","Test Loss:  25.70644859285676\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [01:59<00:00,  2.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  104.12516942052753\n","Test Loss:  25.929346267643268\n","mae_validation:\n","0.4830241062703184\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<01:58,  2.42s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  406.3959133476019\n","Test Loss:  56.83034867653623\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:04<01:56,  2.42s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  277.55858645774424\n","Test Loss:  46.027267481404124\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:07<01:54,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  220.29687114339322\n","Test Loss:  39.71462408005027\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:09<01:52,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  195.1163865585113\n","Test Loss:  36.378351130813826\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:12<01:49,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  177.38969740993343\n","Test Loss:  34.06662880210206\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:14<01:47,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  167.385924852686\n","Test Loss:  32.39098660746822\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:17<01:45,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  157.9050664282404\n","Test Loss:  30.97143979021348\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:19<01:43,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  156.92919610551326\n","Test Loss:  29.720137719181366\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:21<01:40,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  152.63710101076867\n","Test Loss:  29.4927274651709\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:24<01:37,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  146.9970179310185\n","Test Loss:  27.567240771779325\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:26<01:35,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  141.47340483334847\n","Test Loss:  27.266702368331607\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:29<01:33,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  139.83402690425282\n","Test Loss:  26.08546714836848\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:31<01:30,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  135.03915467276238\n","Test Loss:  26.473879995348398\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:34<01:27,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  136.43892121157842\n","Test Loss:  25.33313761698082\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:36<01:25,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  131.1514956660685\n","Test Loss:  26.761093043722212\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:39<01:22,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  132.37886674783658\n","Test Loss:  24.265211499936413\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:41<01:20,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  125.05650130551658\n","Test Loss:  27.153537817910546\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:43<01:17,  2.41s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  130.6590322276461\n","Test Loss:  27.076805486518424\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:46<01:15,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  129.5569866122678\n","Test Loss:  25.999939185450785\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:48<01:13,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  124.44712563988287\n","Test Loss:  23.68820109750959\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [00:51<01:11,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  124.472879236273\n","Test Loss:  24.50229311097064\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 22/50 [00:53<01:08,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  122.00577953287575\n","Test Loss:  24.13779244181933\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 23/50 [00:56<01:06,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  125.53239878319437\n","Test Loss:  23.126111167133786\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 24/50 [00:58<01:03,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  127.2939941668883\n","Test Loss:  24.880144401060534\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 25/50 [01:00<01:00,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  121.28757988021243\n","Test Loss:  22.79816884797765\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 26/50 [01:03<00:58,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  119.37502423435217\n","Test Loss:  21.749529877735768\n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 27/50 [01:05<00:55,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  120.42959738525678\n","Test Loss:  24.183522720762994\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 28/50 [01:08<00:53,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  117.88387495593634\n","Test Loss:  21.526065119396662\n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 29/50 [01:10<00:50,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  118.26891445391811\n","Test Loss:  21.374846257385798\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 30/50 [01:13<00:48,  2.42s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  113.24712765752338\n","Test Loss:  23.403782128210878\n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▏   | 31/50 [01:15<00:46,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  118.25389139627805\n","Test Loss:  20.902911784040043\n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▍   | 32/50 [01:18<00:43,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  116.68086352208047\n","Test Loss:  21.051255190046504\n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▌   | 33/50 [01:20<00:41,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  120.1490372508415\n","Test Loss:  24.11476726474939\n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 34/50 [01:22<00:39,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  114.49125367746456\n","Test Loss:  22.10082425145083\n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 35/50 [01:25<00:36,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  108.64740336759132\n","Test Loss:  20.804578708368354\n"]},{"output_type":"stream","name":"stderr","text":["\r 72%|███████▏  | 36/50 [01:27<00:34,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  113.55676091570058\n","Test Loss:  20.257566366548417\n"]},{"output_type":"stream","name":"stderr","text":["\r 74%|███████▍  | 37/50 [01:30<00:31,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  109.45025329454802\n","Test Loss:  20.19948185706744\n"]},{"output_type":"stream","name":"stderr","text":["\r 76%|███████▌  | 38/50 [01:32<00:29,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  112.47389231485431\n","Test Loss:  20.946388024487533\n"]},{"output_type":"stream","name":"stderr","text":["\r 78%|███████▊  | 39/50 [01:35<00:26,  2.42s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  113.3662945018441\n","Test Loss:  21.734821863356046\n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|████████  | 40/50 [01:37<00:24,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  110.69639980760985\n","Test Loss:  21.300569641811308\n"]},{"output_type":"stream","name":"stderr","text":["\r 82%|████████▏ | 41/50 [01:39<00:21,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  112.30465650270344\n","Test Loss:  21.75658760481747\n"]},{"output_type":"stream","name":"stderr","text":["\r 82%|████████▏ | 41/50 [01:42<00:22,  2.50s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  113.2333500879904\n","Test Loss:  20.32842189744406\n","Early stopping with best_loss:  20.19948185706744 and test_loss for this epoch:  20.32842189744406 ...\n","mae_validation:\n","0.4353728459679168\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<01:59,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  330.05447648931295\n","Test Loss:  54.319504622835666\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:04<01:58,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  226.91684256284498\n","Test Loss:  46.11389673012309\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:07<01:56,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  191.96129107836168\n","Test Loss:  39.52784566680202\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:09<01:54,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  170.13608273502905\n","Test Loss:  34.93386342973099\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:12<01:50,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  155.11171401129104\n","Test Loss:  33.063149774156045\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:14<01:48,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  151.86352242933935\n","Test Loss:  30.773740998251014\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:17<01:46,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  143.05449426319683\n","Test Loss:  33.28227048483677\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:19<01:44,  2.50s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  137.68618523486657\n","Test Loss:  27.583940937707666\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:22<01:41,  2.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  134.91310809901915\n","Test Loss:  29.961011716135545\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:24<01:39,  2.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  136.09367428405676\n","Test Loss:  32.635775642847875\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:27<01:36,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  129.05512648119475\n","Test Loss:  26.800009059283184\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:29<01:34,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  127.63524830093957\n","Test Loss:  25.505805128021166\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:32<01:31,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  123.76946956547908\n","Test Loss:  25.780413333151955\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:34<01:28,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  129.75549213244813\n","Test Loss:  25.37283595307963\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:37<01:25,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  123.63343784346944\n","Test Loss:  26.10441675304901\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:39<01:23,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  123.98470541581628\n","Test Loss:  25.588783783401595\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:42<01:21,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  119.51960681879427\n","Test Loss:  25.196496763062896\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:44<01:19,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  115.82564477762207\n","Test Loss:  24.297683963755844\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:46<01:16,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  113.62229967139137\n","Test Loss:  28.25313544110395\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:49<01:13,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  117.40914692499791\n","Test Loss:  22.822803739865776\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [00:51<01:11,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  115.0256351280259\n","Test Loss:  23.796889895253116\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 22/50 [00:54<01:08,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  112.01409835048253\n","Test Loss:  24.564170690253377\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 23/50 [00:56<01:06,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  116.63106036290992\n","Test Loss:  23.47062817949336\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 24/50 [00:59<01:03,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  111.98354087976622\n","Test Loss:  22.253435469174292\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 25/50 [01:01<01:00,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  109.75476814003196\n","Test Loss:  22.382209071947727\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 26/50 [01:04<00:58,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  108.36826932436088\n","Test Loss:  22.3237034088379\n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 27/50 [01:06<00:56,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  113.1775308578799\n","Test Loss:  23.987059382285224\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 28/50 [01:08<00:53,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  107.31323704711394\n","Test Loss:  21.555375536205247\n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 29/50 [01:11<00:51,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  109.04892056243261\n","Test Loss:  22.6415795586654\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 30/50 [01:13<00:49,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  108.7000757807109\n","Test Loss:  24.005963763513137\n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▏   | 31/50 [01:16<00:46,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  105.33958404394798\n","Test Loss:  23.348820873419754\n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▍   | 32/50 [01:18<00:44,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  106.00843464001082\n","Test Loss:  21.81980814403505\n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▌   | 33/50 [01:21<00:41,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  105.45321471283387\n","Test Loss:  21.0234742697794\n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 34/50 [01:23<00:39,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  105.65726340428228\n","Test Loss:  22.26190433424199\n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 35/50 [01:26<00:36,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  102.34277496594586\n","Test Loss:  26.422876949131023\n"]},{"output_type":"stream","name":"stderr","text":["\r 72%|███████▏  | 36/50 [01:28<00:34,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  102.8953609240707\n","Test Loss:  22.253720707798493\n"]},{"output_type":"stream","name":"stderr","text":["\r 74%|███████▍  | 37/50 [01:31<00:31,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  101.11860568559496\n","Test Loss:  21.045400685514323\n"]},{"output_type":"stream","name":"stderr","text":["\r 74%|███████▍  | 37/50 [01:33<00:32,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  100.79694237557123\n","Test Loss:  21.822518358239904\n","Early stopping with best_loss:  21.0234742697794 and test_loss for this epoch:  21.822518358239904 ...\n","mae_validation:\n","0.4367952370792421\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:14,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  280.18512865761295\n","Test Loss:  56.790477647446096\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:09,  2.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  199.44328684860375\n","Test Loss:  40.008539936097804\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:08<02:09,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  169.01505131612066\n","Test Loss:  38.7580227634171\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:11<02:07,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  155.4120124221663\n","Test Loss:  32.25018586864462\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:13<02:03,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  153.37417288887082\n","Test Loss:  31.01846669911174\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:16<02:00,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  137.2144292887533\n","Test Loss:  31.991644123292645\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:19<01:58,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  130.8579449755489\n","Test Loss:  28.871247144648805\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:22<01:56,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  133.27864937408594\n","Test Loss:  28.490365203877445\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:24<01:52,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  126.3857429116033\n","Test Loss:  33.98983014957048\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:27<01:49,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  124.1659585495363\n","Test Loss:  27.115591893394594\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:30<01:46,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  119.42297745580436\n","Test Loss:  29.781898532426567\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:32<01:43,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  120.64068514452083\n","Test Loss:  26.892655455390923\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:35<01:41,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  117.25784967120853\n","Test Loss:  26.062453345715767\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:38<01:39,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  112.65798125477158\n","Test Loss:  26.5640372610942\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:41<01:36,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  114.15922373742796\n","Test Loss:  25.065275419008685\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:43<01:33,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  110.16297010343987\n","Test Loss:  24.732053107349202\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:46<01:30,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  112.56194833520567\n","Test Loss:  25.560132698272355\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:49<01:27,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  108.97089999302989\n","Test Loss:  24.492153125698678\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:52<01:25,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  111.36021565672127\n","Test Loss:  25.027784696489107\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:54<01:22,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  103.23298913033796\n","Test Loss:  24.509747853968292\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [00:57<01:20,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  109.40166426583892\n","Test Loss:  23.512945396651048\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 22/50 [01:00<01:17,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  104.37710495264037\n","Test Loss:  23.29112081322819\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 23/50 [01:03<01:13,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  104.25801099630189\n","Test Loss:  24.092691640049452\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 24/50 [01:05<01:11,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  102.76003065766417\n","Test Loss:  22.826776169298682\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 25/50 [01:08<01:09,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  102.22201119930833\n","Test Loss:  22.80921504664002\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 26/50 [01:11<01:05,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  94.91865276941098\n","Test Loss:  24.18142181058647\n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 27/50 [01:14<01:02,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  99.4128032833687\n","Test Loss:  23.259468100353843\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 28/50 [01:16<00:59,  2.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  101.74583966145292\n","Test Loss:  23.27407435301575\n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 29/50 [01:19<00:57,  2.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  96.66256107701338\n","Test Loss:  22.892806355535868\n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 29/50 [01:22<00:59,  2.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  100.19852761607035\n","Test Loss:  23.181293509012903\n","Early stopping with best_loss:  22.80921504664002 and test_loss for this epoch:  23.181293509012903 ...\n","mae_validation:\n","0.4668035216602799\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:10,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  344.3200602703728\n","Test Loss:  54.88575569796376\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:06,  2.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  201.51276422466617\n","Test Loss:  52.018786681146594\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:07<02:05,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  173.91454409714788\n","Test Loss:  40.26789063241449\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:10<02:02,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  156.75162597179587\n","Test Loss:  37.54257352678542\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:13<01:59,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  151.38039838540135\n","Test Loss:  40.85995951096993\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:16<01:57,  2.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  139.88611113204388\n","Test Loss:  37.40444765065331\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:18<01:55,  2.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  139.42029121605447\n","Test Loss:  37.88926488009747\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:21<01:52,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  148.1148020962719\n","Test Loss:  37.57970380916959\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:23<01:48,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  138.02616238675546\n","Test Loss:  34.153982811956666\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:26<01:45,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  135.62401028026943\n","Test Loss:  30.986487733316608\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:29<01:43,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  131.411699961056\n","Test Loss:  31.74125938815996\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:31<01:40,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  132.49953834834741\n","Test Loss:  31.757051719119772\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:34<01:38,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  135.79411219601752\n","Test Loss:  46.05326358031016\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:37<01:35,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  131.83494688384235\n","Test Loss:  31.80246614984935\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:39<01:42,  2.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  124.61841060605366\n","Test Loss:  33.7947090928792\n","Early stopping with best_loss:  30.986487733316608 and test_loss for this epoch:  33.7947090928792 ...\n","mae_validation:\n","0.5573838952184347\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:13,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  297.5779315500986\n","Test Loss:  53.27899706090102\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:11,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  190.35388233774574\n","Test Loss:  40.21655266691232\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:08<02:10,  2.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  158.8152732233866\n","Test Loss:  41.8701249853475\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:10<02:06,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  154.67813588894205\n","Test Loss:  36.27461133769248\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:13<02:03,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  149.03483033081284\n","Test Loss:  34.43729477672605\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:16<02:00,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  148.24640428175917\n","Test Loss:  37.207917326268216\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:19<01:58,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  138.56521734117996\n","Test Loss:  31.327588472006028\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:21<01:54,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  137.72197154909372\n","Test Loss:  33.40845909982454\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:24<01:52,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  134.12015567952767\n","Test Loss:  32.361091715923976\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:27<01:49,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  129.4182054992416\n","Test Loss:  30.68439159076661\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:30<01:46,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  131.31472938827937\n","Test Loss:  31.17523222148884\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:32<01:43,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  130.9375906009227\n","Test Loss:  31.708454452629667\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:35<01:41,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  134.54649103269912\n","Test Loss:  28.96649385825731\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:38<01:38,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  121.18516038713278\n","Test Loss:  32.666642082389444\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:41<01:35,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  126.42361903353594\n","Test Loss:  33.60513373327558\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:43<01:32,  2.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  124.75987423420884\n","Test Loss:  31.001442100736313\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:46<01:29,  2.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  125.32259965178673\n","Test Loss:  29.057557339256164\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:49<01:35,  2.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  121.86465039319592\n","Test Loss:  33.59980526030995\n","Early stopping with best_loss:  28.96649385825731 and test_loss for this epoch:  33.59980526030995 ...\n","mae_validation:\n","0.485846144085688\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:14,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  256.9808996457141\n","Test Loss:  47.81226117699407\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:11,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  169.3494485791307\n","Test Loss:  42.7223199812579\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:08<02:08,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  153.60822753806133\n","Test Loss:  37.4228708678711\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:10<02:05,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  146.45811200607568\n","Test Loss:  36.461949879070744\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:13<02:03,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  140.11746079230215\n","Test Loss:  37.85187348234467\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:16<02:01,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  137.59790915151825\n","Test Loss:  38.17119511880446\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:19<01:58,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  132.60487384311273\n","Test Loss:  35.2790230747778\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:22<01:55,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  134.93571305429214\n","Test Loss:  34.17061805902631\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:24<01:53,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  128.83779724757187\n","Test Loss:  32.569486148131546\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:27<01:49,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  131.07248297400656\n","Test Loss:  34.47834743105341\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:30<01:46,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  129.5610838890134\n","Test Loss:  33.43077801585605\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:32<01:44,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  121.55605390081473\n","Test Loss:  34.487568534037564\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:35<01:41,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  125.78673283854732\n","Test Loss:  32.16162548668217\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:38<01:38,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  119.84122086332354\n","Test Loss:  31.52944687093259\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:41<01:35,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  121.72726011782652\n","Test Loss:  31.93023194023408\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:43<01:32,  2.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  121.06227760913316\n","Test Loss:  32.256223912525456\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:46<01:30,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  112.48520602751523\n","Test Loss:  33.97517239511944\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:49<01:28,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  120.71734860516153\n","Test Loss:  31.46414912835462\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:52<01:25,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  114.97772159054875\n","Test Loss:  31.859595235204324\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:54<01:22,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  116.99445876061509\n","Test Loss:  32.50314407807309\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [00:57<01:19,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  116.73256422416307\n","Test Loss:  34.814556817058474\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 22/50 [01:00<01:17,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  117.34169596101856\n","Test Loss:  31.37681008281652\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 23/50 [01:03<01:14,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  110.43010489980225\n","Test Loss:  34.89312537654769\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 24/50 [01:05<01:11,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  110.17168136831606\n","Test Loss:  30.91118811580236\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 25/50 [01:08<01:08,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  110.05883773812093\n","Test Loss:  32.009936191141605\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 26/50 [01:11<01:06,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  109.03568329714471\n","Test Loss:  31.7917039457825\n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 27/50 [01:14<01:03,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  113.84642363584135\n","Test Loss:  30.495103266788647\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 28/50 [01:16<01:00,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  108.30881041142857\n","Test Loss:  37.227507354691625\n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 29/50 [01:19<00:58,  2.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  111.57494381573633\n","Test Loss:  29.92964917756035\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 30/50 [01:22<00:55,  2.78s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  108.86578762019053\n","Test Loss:  29.71953136351658\n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▏   | 31/50 [01:25<00:52,  2.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  108.12353486928623\n","Test Loss:  33.528150004683994\n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▍   | 32/50 [01:27<00:49,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  109.39459483360406\n","Test Loss:  30.64439774584025\n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▌   | 33/50 [01:30<00:46,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  108.57146957403165\n","Test Loss:  29.07731788547244\n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 34/50 [01:33<00:44,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  112.26675416569924\n","Test Loss:  29.52509165427182\n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 35/50 [01:36<00:41,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  106.25047573749907\n","Test Loss:  28.71700669945858\n"]},{"output_type":"stream","name":"stderr","text":["\r 72%|███████▏  | 36/50 [01:38<00:38,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  108.71440926124342\n","Test Loss:  32.49712063756306\n"]},{"output_type":"stream","name":"stderr","text":["\r 74%|███████▍  | 37/50 [01:41<00:35,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  104.58219017053489\n","Test Loss:  31.027661295694998\n"]},{"output_type":"stream","name":"stderr","text":["\r 76%|███████▌  | 38/50 [01:44<00:32,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  108.0605592073407\n","Test Loss:  29.414096995838918\n"]},{"output_type":"stream","name":"stderr","text":["\r 78%|███████▊  | 39/50 [01:47<00:30,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  108.16096914245281\n","Test Loss:  29.110541252535768\n"]},{"output_type":"stream","name":"stderr","text":["\r 78%|███████▊  | 39/50 [01:49<00:31,  2.82s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  107.08403884319705\n","Test Loss:  29.779550442355685\n","Early stopping with best_loss:  28.71700669945858 and test_loss for this epoch:  29.779550442355685 ...\n","mae_validation:\n","0.4887068892261077\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:23,  2.93s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  238.9948959617177\n","Test Loss:  41.181113784899935\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:21,  2.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  168.012024411757\n","Test Loss:  37.42052273917943\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:08<02:19,  2.97s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  163.2345232006046\n","Test Loss:  31.20209627697477\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:11<02:15,  2.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  152.50896845181705\n","Test Loss:  33.8274124332238\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:14<02:12,  2.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  148.24219819925202\n","Test Loss:  35.38803775468841\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:17<02:09,  2.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  144.58561942301458\n","Test Loss:  28.40751045430079\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:20<02:06,  2.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  133.67660434756544\n","Test Loss:  33.75820173861575\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:23<02:03,  2.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  134.76259037997806\n","Test Loss:  39.41973632480949\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:26<02:00,  2.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  137.92340845844592\n","Test Loss:  26.398474001442082\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:29<01:58,  2.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  130.88964026782196\n","Test Loss:  25.18697028974566\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:32<01:54,  2.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  130.57610555383144\n","Test Loss:  28.57566557134851\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:35<01:52,  2.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  130.8157256177219\n","Test Loss:  28.20558677171357\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:38<01:49,  2.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  130.42794743319973\n","Test Loss:  26.909019165555947\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:41<01:45,  2.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  128.5490687933634\n","Test Loss:  26.150021485984325\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:44<01:53,  3.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  128.8621072883834\n","Test Loss:  25.302757342578843\n","Early stopping with best_loss:  25.18697028974566 and test_loss for this epoch:  25.302757342578843 ...\n","mae_validation:\n","0.46234876856007573\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:07,  2.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  346.1380179701373\n","Test Loss:  49.171616482432\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:04,  2.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  192.44059572869446\n","Test Loss:  39.32981838865089\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:07<02:02,  2.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  173.95118446234846\n","Test Loss:  37.52378263062565\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:10<01:59,  2.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  160.68516441533575\n","Test Loss:  29.891485242755152\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:13<01:57,  2.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  150.66034917734214\n","Test Loss:  28.645808160072193\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:15<01:54,  2.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  151.68655938160373\n","Test Loss:  27.713854075816926\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:18<01:50,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  150.68532572867116\n","Test Loss:  28.430397871066816\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:20<01:48,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  140.7323439512984\n","Test Loss:  26.948175721307052\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:23<01:45,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  143.2508088913455\n","Test Loss:  26.829799510189332\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:25<01:42,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  140.38040594983613\n","Test Loss:  25.36628075351473\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:28<01:40,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  135.42602225206792\n","Test Loss:  31.022105821641162\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:30<01:37,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  138.76094096741872\n","Test Loss:  26.561782288743416\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:33<01:35,  2.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  135.80597216257593\n","Test Loss:  25.0654283358017\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:36<01:32,  2.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  131.97304157353938\n","Test Loss:  26.817761510552373\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:38<01:30,  2.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  133.39077251133858\n","Test Loss:  26.517298994178418\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:41<01:28,  2.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  135.2256660045823\n","Test Loss:  24.10546460156911\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:44<01:26,  2.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  129.26676396850962\n","Test Loss:  25.70563132071402\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:46<01:23,  2.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  135.6998481034825\n","Test Loss:  25.032637915806845\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:49<01:20,  2.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  128.6894202452968\n","Test Loss:  24.690874880820047\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:51<01:17,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  132.46684950910276\n","Test Loss:  26.83547758599161\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:54<01:21,  2.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  130.04949796228902\n","Test Loss:  24.283282995107584\n","Early stopping with best_loss:  24.10546460156911 and test_loss for this epoch:  24.283282995107584 ...\n","mae_validation:\n","0.4639377227889232\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:09,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  280.22100082039833\n","Test Loss:  44.808377202250995\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:07,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  183.33916354784742\n","Test Loss:  36.45070842956193\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:07<02:05,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  162.84796626190655\n","Test Loss:  31.429481005645357\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:10<02:02,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  156.61649559548823\n","Test Loss:  30.219198033795692\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:13<01:59,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  154.7538768259692\n","Test Loss:  27.336259321600664\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:15<01:57,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  144.9791766215494\n","Test Loss:  28.423808533116244\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:18<01:53,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  141.74726472521434\n","Test Loss:  27.10711985011585\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:21<01:51,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  144.15777304332005\n","Test Loss:  28.18930447846651\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:23<01:48,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  133.4056735971826\n","Test Loss:  33.650958613696275\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:26<01:46,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  136.8393863858655\n","Test Loss:  34.74401936709182\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:29<01:43,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  135.05049582757056\n","Test Loss:  23.119938132585958\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:31<01:40,  2.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  135.21208222501446\n","Test Loss:  24.64920512813842\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:34<01:37,  2.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  129.9437062337529\n","Test Loss:  33.07492945859849\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:37<01:34,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  131.87984900022275\n","Test Loss:  27.791535863419995\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:39<01:32,  2.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  126.17755031122942\n","Test Loss:  25.739872711710632\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:42<01:38,  2.82s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  126.08303859928856\n","Test Loss:  23.817314901738428\n","Early stopping with best_loss:  23.119938132585958 and test_loss for this epoch:  23.817314901738428 ...\n","mae_validation:\n","0.4653466824582597\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:10,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  255.53624074021354\n","Test Loss:  36.31210966137587\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:05,  2.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  176.2865417524008\n","Test Loss:  37.052304007229395\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:08<02:06,  2.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  164.24023176607443\n","Test Loss:  29.687032524496317\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:10<02:03,  2.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  150.69492659973912\n","Test Loss:  35.19936277839588\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:13<01:59,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  148.2196903533768\n","Test Loss:  27.011484575195936\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:15<01:56,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  144.21653632013476\n","Test Loss:  29.496140068251407\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:18<01:54,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  141.40898455226852\n","Test Loss:  24.907939698314294\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:21<01:51,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  133.46955997863552\n","Test Loss:  25.010510215332033\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:23<01:49,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  133.71462936507305\n","Test Loss:  23.852551243500784\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:26<01:46,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  135.45185289043002\n","Test Loss:  24.54022236657329\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:29<01:44,  2.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  136.46317681481014\n","Test Loss:  24.16115226203692\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:32<01:41,  2.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  130.35815853683744\n","Test Loss:  24.443509141216055\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:34<01:38,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  126.0577744168695\n","Test Loss:  25.329381980467588\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:37<01:46,  2.87s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  127.92381093971198\n","Test Loss:  24.00700526870787\n","Early stopping with best_loss:  23.852551243500784 and test_loss for this epoch:  24.00700526870787 ...\n","mae_validation:\n","0.4765499581013024\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:22,  2.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  233.87154794298112\n","Test Loss:  38.677222111378796\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:21,  2.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  174.8993896698812\n","Test Loss:  32.23325043742079\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:08<02:17,  2.92s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  155.04014126496622\n","Test Loss:  32.24957354896469\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:11<02:13,  2.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  150.3671631513571\n","Test Loss:  34.29760420968523\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:14<02:10,  2.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  141.69682760653086\n","Test Loss:  29.97291748202406\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:17<02:07,  2.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  149.4962433402543\n","Test Loss:  32.01438634842634\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:20<02:04,  2.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  136.73739657628175\n","Test Loss:  27.239424084546044\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:23<02:01,  2.88s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  135.3853104081354\n","Test Loss:  29.388161521521397\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:26<01:58,  2.88s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  125.1279181157588\n","Test Loss:  27.032388244348112\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:28<01:55,  2.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  132.39032305200817\n","Test Loss:  34.09364773705602\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:31<01:52,  2.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  126.85811173479306\n","Test Loss:  34.62647643103264\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:34<01:51,  2.93s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  126.47854531076155\n","Test Loss:  25.35991465859115\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:37<01:47,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  120.4028431319166\n","Test Loss:  25.348763519199565\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:40<01:44,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  128.6886630784138\n","Test Loss:  25.530584608059144\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:43<01:41,  2.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  121.16283475325326\n","Test Loss:  25.404786311701173\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:46<01:38,  2.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  122.58835150452796\n","Test Loss:  31.69081104779616\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:49<01:35,  2.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  121.46106474974658\n","Test Loss:  23.099930837517604\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:52<01:32,  2.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  120.40868077945197\n","Test Loss:  22.41821792628616\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:55<01:29,  2.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  119.82920224124973\n","Test Loss:  27.024051252461504\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:57<01:26,  2.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  117.34590083220974\n","Test Loss:  25.00934090797091\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [01:00<01:24,  2.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  115.27121842911583\n","Test Loss:  27.324404578190297\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 22/50 [01:03<01:21,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  116.76284709293395\n","Test Loss:  26.931619783106726\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 22/50 [01:06<01:24,  3.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  107.95201787957922\n","Test Loss:  25.044588630378712\n","Early stopping with best_loss:  22.41821792628616 and test_loss for this epoch:  25.044588630378712 ...\n","mae_validation:\n","0.46619873258377764\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:01,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  340.30944479373284\n","Test Loss:  52.10441560961772\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:04<01:58,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  191.81033215601929\n","Test Loss:  47.54737921219203\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:07<01:56,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  160.93917093228083\n","Test Loss:  36.568520720727975\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:09<01:54,  2.50s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  153.99446976184845\n","Test Loss:  35.03544463269645\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:12<01:51,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  146.73899292660644\n","Test Loss:  33.86934163598926\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:14<01:48,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  148.34176778845722\n","Test Loss:  32.0557468625484\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:17<01:45,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  143.2635666949791\n","Test Loss:  36.118738751043566\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:19<01:42,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  138.41712796408683\n","Test Loss:  34.16852487133292\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:22<01:41,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  140.07167026537354\n","Test Loss:  35.25714076246368\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:24<01:38,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  137.96175301400945\n","Test Loss:  32.752194471715484\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:27<01:36,  2.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  133.18872068848577\n","Test Loss:  29.639474511961453\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:29<01:33,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  131.50917581497924\n","Test Loss:  27.615615183938644\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:32<01:30,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  126.52185242134146\n","Test Loss:  29.411048958369065\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:34<01:28,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  129.29021375929005\n","Test Loss:  28.502352355979383\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:36<01:25,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  126.64199772139546\n","Test Loss:  28.82915167650208\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:39<01:23,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  124.85505767184077\n","Test Loss:  30.11211442935746\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:41<01:20,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  128.76753363679745\n","Test Loss:  26.434817888890393\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:44<01:18,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  126.08944175735814\n","Test Loss:  27.971490575524513\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:46<01:16,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  123.45924653607653\n","Test Loss:  28.623590218208847\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:49<01:14,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  120.36154481914127\n","Test Loss:  27.292460658412892\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [00:51<01:11,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  123.46540697661112\n","Test Loss:  29.990984049945837\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [00:54<01:14,  2.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  121.21144776436267\n","Test Loss:  28.59729007698479\n","Early stopping with best_loss:  26.434817888890393 and test_loss for this epoch:  28.59729007698479 ...\n","mae_validation:\n","0.5141895143144564\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:06,  2.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  283.8659689039923\n","Test Loss:  47.37635300133843\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:02,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  184.82163591030985\n","Test Loss:  36.26679372205399\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:07<01:59,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  161.4964244137518\n","Test Loss:  31.837229901429964\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:10<01:56,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  151.92312714102445\n","Test Loss:  30.39136484998744\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:12<01:54,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  150.27878833800787\n","Test Loss:  32.28017908951733\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:15<01:51,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  142.79350940941367\n","Test Loss:  30.106687112536747\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:17<01:49,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  136.823921282019\n","Test Loss:  30.26889057486551\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:20<01:46,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  134.08069670881378\n","Test Loss:  27.71822165499907\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:22<01:43,  2.51s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  129.18020536744734\n","Test Loss:  28.311808288504835\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:25<01:40,  2.51s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  127.59692219889257\n","Test Loss:  29.259480589884333\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:27<01:38,  2.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  126.15161105949664\n","Test Loss:  27.583414254768286\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:30<01:35,  2.51s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  126.32391536451178\n","Test Loss:  30.080197667819448\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:32<01:33,  2.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  124.16834628098877\n","Test Loss:  26.237949304981157\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:35<01:31,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  124.49522845912725\n","Test Loss:  27.33941648271866\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:37<01:28,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  125.65176664985484\n","Test Loss:  28.09549454797525\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:40<01:26,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  117.66802773799282\n","Test Loss:  26.48066270365962\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:43<01:23,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  117.64580482942984\n","Test Loss:  30.126297098759096\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:45<01:28,  2.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  119.47905963871744\n","Test Loss:  26.25526810984593\n","Early stopping with best_loss:  26.237949304981157 and test_loss for this epoch:  26.25526810984593 ...\n","mae_validation:\n","0.49810033812826054\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:06,  2.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  259.1428226218559\n","Test Loss:  33.859218512894586\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:03,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  179.1627502485062\n","Test Loss:  33.006501255207695\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:07<01:59,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  156.9583472998347\n","Test Loss:  30.888179604167817\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:10<01:57,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  152.05883342865855\n","Test Loss:  25.6733742984361\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:12<01:53,  2.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  154.21921363670845\n","Test Loss:  25.464749621867668\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:15<01:51,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  143.87431852746522\n","Test Loss:  24.859123109752545\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:17<01:50,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  139.5829711254919\n","Test Loss:  23.47157653607428\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:20<01:48,  2.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  135.403937617084\n","Test Loss:  24.797391174768563\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:23<01:45,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  131.43939072848298\n","Test Loss:  23.948866598351742\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:25<01:43,  2.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  131.55367558443686\n","Test Loss:  27.42423669609707\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:28<01:40,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  131.35068070847774\n","Test Loss:  26.171292742539663\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:30<01:37,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  126.08456068352098\n","Test Loss:  21.155531045922544\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:33<01:34,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  123.76424797848449\n","Test Loss:  23.204698737012222\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:35<01:31,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  124.63192829283071\n","Test Loss:  21.114258736750344\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:38<01:29,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  124.14684219891205\n","Test Loss:  21.387070924596628\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:40<01:27,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  125.27401420538081\n","Test Loss:  22.620074561971705\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:43<01:24,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  119.58712048262532\n","Test Loss:  21.413766885525547\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:46<01:22,  2.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  122.60679680510657\n","Test Loss:  21.311661692452617\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:48<01:26,  2.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  119.11086396849714\n","Test Loss:  23.477674267604016\n","Early stopping with best_loss:  21.114258736750344 and test_loss for this epoch:  23.477674267604016 ...\n","mae_validation:\n","0.43550520957993427\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:20,  2.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  234.04631235287525\n","Test Loss:  38.09028511494398\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:17,  2.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  177.86082861409523\n","Test Loss:  33.484978995402344\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:08<02:13,  2.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  155.3663216897985\n","Test Loss:  29.913066962151788\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:11<02:11,  2.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  144.44530607637716\n","Test Loss:  28.180654538766248\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:14<02:09,  2.87s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  145.58605928288307\n","Test Loss:  26.805566716415342\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:17<02:05,  2.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  139.0149287750828\n","Test Loss:  30.816290758550167\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:19<02:02,  2.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  131.85448426939547\n","Test Loss:  29.0015627919056\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:22<01:59,  2.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  126.78578703847597\n","Test Loss:  27.473314050759654\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:25<01:56,  2.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  132.99832962924847\n","Test Loss:  26.613026995270047\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:28<01:53,  2.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  127.31213145225774\n","Test Loss:  27.394882309949026\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:31<01:51,  2.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  125.30129377485719\n","Test Loss:  28.86361291864887\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:34<01:48,  2.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  120.5612149485969\n","Test Loss:  26.05052411853103\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:37<01:45,  2.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  118.36253309436142\n","Test Loss:  30.52995037427172\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:39<01:42,  2.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  122.45064472706872\n","Test Loss:  25.017733077751473\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:42<01:39,  2.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  122.472850695136\n","Test Loss:  27.135759614116978\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 16/50 [00:45<01:36,  2.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  116.428942447179\n","Test Loss:  30.97138777881628\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 17/50 [00:48<01:33,  2.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  115.68295273231342\n","Test Loss:  26.110231645638123\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 18/50 [00:51<01:30,  2.84s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  121.32444351966842\n","Test Loss:  24.14018286249484\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 19/50 [00:54<01:28,  2.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  118.3344993571518\n","Test Loss:  23.821688058698783\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 20/50 [00:56<01:25,  2.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  110.95767498285568\n","Test Loss:  24.837857525330037\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 21/50 [00:59<01:23,  2.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  112.900311905134\n","Test Loss:  24.404729704750935\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 22/50 [01:02<01:20,  2.88s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  116.12514787667897\n","Test Loss:  25.416894565161783\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 23/50 [01:05<01:16,  2.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  116.5368599841313\n","Test Loss:  27.484060690767365\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 23/50 [01:08<01:20,  2.97s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  112.15413674598676\n","Test Loss:  24.568061927624512\n","Early stopping with best_loss:  23.821688058698783 and test_loss for this epoch:  24.568061927624512 ...\n","mae_validation:\n","0.4603104201455664\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<01:57,  2.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  339.2547300597653\n","Test Loss:  51.44656971032964\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:04<01:53,  2.36s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  187.88406253128778\n","Test Loss:  42.624103213485796\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:07<01:50,  2.35s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  161.35019831859972\n","Test Loss:  37.4905835147656\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:09<01:48,  2.36s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  151.23899549993803\n","Test Loss:  33.08091758971568\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:11<01:46,  2.37s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  149.89631970768096\n","Test Loss:  36.20005061873235\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:14<01:43,  2.36s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  146.41722808370832\n","Test Loss:  35.9916339663323\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:16<01:41,  2.36s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  146.2308848951361\n","Test Loss:  32.39108405483421\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:18<01:38,  2.36s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  137.8614160399884\n","Test Loss:  28.777004087227397\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:21<01:37,  2.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  136.64058575162198\n","Test Loss:  29.34185319172684\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:23<01:35,  2.40s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  131.60946942510782\n","Test Loss:  31.350341188837774\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:26<01:33,  2.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  130.69954901613528\n","Test Loss:  33.141117276245495\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:28<01:30,  2.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  131.21578229671286\n","Test Loss:  30.189133540843613\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:30<01:37,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  128.224091470649\n","Test Loss:  30.045803059096215\n","Early stopping with best_loss:  28.777004087227397 and test_loss for this epoch:  30.045803059096215 ...\n","mae_validation:\n","0.558291277413182\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<01:58,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  293.1277795047499\n","Test Loss:  40.57070995878894\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:04<01:57,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  179.7348400913179\n","Test Loss:  34.35734986310126\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:07<01:55,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  162.66223772545345\n","Test Loss:  29.91614433855284\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:09<01:52,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  153.06418640894117\n","Test Loss:  31.56117991701467\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:12<01:50,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  141.8710062794853\n","Test Loss:  29.721798788697924\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:14<01:47,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  145.0699583516689\n","Test Loss:  29.08892363938503\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:17<01:44,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  138.86104599857936\n","Test Loss:  26.802289156228653\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:19<01:42,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  134.52230342058465\n","Test Loss:  25.210179893882014\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:22<01:41,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  130.18866776279174\n","Test Loss:  28.981413449218962\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:24<01:38,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  130.67234045400983\n","Test Loss:  24.817143931722967\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:26<01:35,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  126.10654039151268\n","Test Loss:  23.483464737713803\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:29<01:33,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  126.06018928159028\n","Test Loss:  25.390392409521155\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:31<01:31,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  124.50767219014233\n","Test Loss:  25.568711720465217\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 14/50 [00:34<01:28,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  121.1367722254945\n","Test Loss:  24.56137475220021\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:36<01:25,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  121.68121035999502\n","Test Loss:  25.533526811457705\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 15/50 [00:39<01:31,  2.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  118.11100376774993\n","Test Loss:  28.102381544245873\n","Early stopping with best_loss:  23.483464737713803 and test_loss for this epoch:  28.102381544245873 ...\n","mae_validation:\n","0.47624048098515204\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:06,  2.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  239.20901117334142\n","Test Loss:  48.75593345123343\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:02,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  165.91411282058107\n","Test Loss:  40.787710616976256\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:07<01:58,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  144.59801918483572\n","Test Loss:  38.103742110251915\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:10<01:54,  2.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  140.0560705959506\n","Test Loss:  35.46147621551063\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:12<01:53,  2.51s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  126.42972508037928\n","Test Loss:  34.216752026288304\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:15<01:49,  2.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  123.53653986600693\n","Test Loss:  38.11369160749018\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:17<01:46,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  124.49550360286958\n","Test Loss:  33.651583906204905\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:19<01:43,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  121.94549742563686\n","Test Loss:  34.01158354134532\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:22<01:40,  2.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  122.43932250270154\n","Test Loss:  31.64770644973032\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:24<01:37,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  122.21242357848678\n","Test Loss:  32.76231749994622\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:27<01:35,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  115.3407043783227\n","Test Loss:  33.604017270496115\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:29<01:32,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  117.65267441487231\n","Test Loss:  33.24727919790894\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:32<01:30,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  109.99394635122735\n","Test Loss:  36.57387516327435\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 13/50 [00:34<01:38,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  115.56124029267812\n","Test Loss:  33.1509558375692\n","Early stopping with best_loss:  31.64770644973032 and test_loss for this epoch:  33.1509558375692 ...\n","mae_validation:\n","0.6083764865335278\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","  2%|▏         | 1/50 [00:02<02:14,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  235.9503542269813\n","Test Loss:  39.32427736907266\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 2/50 [00:05<02:12,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  157.94720768544357\n","Test Loss:  45.78362866298994\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 3/50 [00:08<02:10,  2.78s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  154.85840703512076\n","Test Loss:  30.266631695907563\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 4/50 [00:11<02:07,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  140.52517041936517\n","Test Loss:  33.073480540537275\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 5/50 [00:13<02:03,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  134.86083884054096\n","Test Loss:  28.54888944805134\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 6/50 [00:16<02:01,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  128.20260837912792\n","Test Loss:  29.962366987019777\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 7/50 [00:19<01:58,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  129.79353840684053\n","Test Loss:  28.446821725810878\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 8/50 [00:22<01:55,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss:  125.85822088521672\n","Test Loss:  27.52471433527535\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 9/50 [00:24<01:52,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 1 of 5\n","Train Loss:  122.06578196494957\n","Test Loss:  29.612447366962442\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 10/50 [00:27<01:49,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 2 of 5\n","Train Loss:  124.87337530654622\n","Test Loss:  29.072372628899757\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 11/50 [00:30<01:46,  2.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 3 of 5\n","Train Loss:  119.26585673238151\n","Test Loss:  28.0877639297687\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:32<01:44,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 4 of 5\n","Train Loss:  113.64201565776602\n","Test Loss:  33.38978977734223\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 12/50 [00:35<01:52,  2.97s/it]"]},{"output_type":"stream","name":"stdout","text":["Counter 5 of 5\n","Train Loss:  116.98026511908392\n","Test Loss:  28.598745115334168\n","Early stopping with best_loss:  27.52471433527535 and test_loss for this epoch:  28.598745115334168 ...\n","mae_validation:\n","0.5333742985039519\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"output_type":"execute_result","data":{"text/plain":["   Model      lr batch_size seq_length hidden_layer_size num_layers  \\\n","3   LSTM  0.0001          2          4               256          1   \n","13  LSTM  0.0001          2         10                64          1   \n","26  LSTM  0.0005          2          8               128          1   \n","14  LSTM  0.0001          2         10               128          1   \n","2   LSTM  0.0001          2          4               128          1   \n","1   LSTM  0.0001          2          4                64          1   \n","11  LSTM  0.0001          2          8               256          1   \n","27  LSTM  0.0005          2          8               256          1   \n","19  LSTM  0.0005          2          4               256          1   \n","20  LSTM  0.0005          2          6                32          1   \n","0   LSTM  0.0001          2          4                32          1   \n","21  LSTM  0.0005          2          6                64          1   \n","23  LSTM  0.0005          2          6               256          1   \n","15  LSTM  0.0001          2         10               256          1   \n","9   LSTM  0.0001          2          8                64          1   \n","7   LSTM  0.0001          2          6               256          1   \n","29  LSTM  0.0005          2         10                64          1   \n","22  LSTM  0.0005          2          6               128          1   \n","10  LSTM  0.0001          2          8               128          1   \n","12  LSTM  0.0001          2         10                32          1   \n","8   LSTM  0.0001          2          8                32          1   \n","17  LSTM  0.0005          2          4                64          1   \n","18  LSTM  0.0005          2          4               128          1   \n","6   LSTM  0.0001          2          6               128          1   \n","5   LSTM  0.0001          2          6                64          1   \n","25  LSTM  0.0005          2          8                64          1   \n","24  LSTM  0.0005          2          8                32          1   \n","31  LSTM  0.0005          2         10               256          1   \n","16  LSTM  0.0005          2          4                32          1   \n","28  LSTM  0.0005          2         10                32          1   \n","4   LSTM  0.0001          2          6                32          1   \n","30  LSTM  0.0005          2         10               128          1   \n","\n","          lossfunc  mae_validation  mae_validation_1w  mae_validation_2w  \\\n","3   SmoothL1Loss()        0.426852           0.352895           0.395088   \n","13  SmoothL1Loss()        0.435373           0.329440           0.404648   \n","26  SmoothL1Loss()        0.435505           0.369385           0.450823   \n","14  SmoothL1Loss()        0.436795           0.378866           0.480702   \n","2   SmoothL1Loss()        0.439042           0.346694           0.401489   \n","1   SmoothL1Loss()        0.440515           0.303749           0.409141   \n","11  SmoothL1Loss()        0.457840           0.311857           0.439973   \n","27  SmoothL1Loss()        0.460310           0.352440           0.405706   \n","19  SmoothL1Loss()        0.462349           0.378433           0.419067   \n","20  SmoothL1Loss()        0.463938           0.397804           0.412176   \n","0   SmoothL1Loss()        0.464772           0.337532           0.407699   \n","21  SmoothL1Loss()        0.465347           0.355826           0.369638   \n","23  SmoothL1Loss()        0.466199           0.387392           0.425529   \n","15  SmoothL1Loss()        0.466804           0.351329           0.400969   \n","9   SmoothL1Loss()        0.467254           0.411430           0.491755   \n","7   SmoothL1Loss()        0.473991           0.351607           0.464153   \n","29  SmoothL1Loss()        0.476240           0.353054           0.499141   \n","22  SmoothL1Loss()        0.476550           0.372144           0.443574   \n","10  SmoothL1Loss()        0.477137           0.331465           0.469184   \n","12  SmoothL1Loss()        0.483024           0.362525           0.504956   \n","8   SmoothL1Loss()        0.483506           0.381431           0.449010   \n","17  SmoothL1Loss()        0.485846           0.375607           0.484300   \n","18  SmoothL1Loss()        0.488707           0.465943           0.514321   \n","6   SmoothL1Loss()        0.490433           0.291622           0.406788   \n","5   SmoothL1Loss()        0.494584           0.413815           0.480104   \n","25  SmoothL1Loss()        0.498100           0.351849           0.460041   \n","24  SmoothL1Loss()        0.514190           0.359696           0.507152   \n","31  SmoothL1Loss()        0.533374           0.395636           0.512391   \n","16  SmoothL1Loss()        0.557384           0.423985           0.511819   \n","28  SmoothL1Loss()        0.558291           0.438428           0.544349   \n","4   SmoothL1Loss()        0.558995           0.407561           0.554945   \n","30  SmoothL1Loss()        0.608376           0.414908           0.515664   \n","\n","    mae_validation_3w  mae_validation_4w Target  \n","3            0.530987           0.591012   rate  \n","13           0.453385           0.543349   rate  \n","26           0.557627           0.656678   rate  \n","14           0.577748           0.567730   rate  \n","2            0.469782           0.580520   rate  \n","1            0.491859           0.594494   rate  \n","11           0.503870           0.588642   rate  \n","27           0.487385           0.639533   rate  \n","19           0.513607           0.603954   rate  \n","20           0.506332           0.606075   rate  \n","0            0.486225           0.613726   rate  \n","21           0.490508           0.668149   rate  \n","23           0.543252           0.624078   rate  \n","15           0.524578           0.579201   rate  \n","9            0.532778           0.667935   rate  \n","7            0.599402           0.611114   rate  \n","29           0.590657           0.700102   rate  \n","22           0.498280           0.510959   rate  \n","10           0.525895           0.619795   rate  \n","12           0.574154           0.605740   rate  \n","8            0.519395           0.611599   rate  \n","17           0.620943           0.730378   rate  \n","18           0.572934           0.680925   rate  \n","6            0.523998           0.670938   rate  \n","5            0.571804           0.710071   rate  \n","25           0.533022           0.646347   rate  \n","24           0.560379           0.678729   rate  \n","31           0.614436           0.652828   rate  \n","16           0.662924           0.746738   rate  \n","28           0.686212           0.817922   rate  \n","4            0.677329           0.775544   rate  \n","30           0.650835           0.806946   rate  "],"text/html":["\n","  <div id=\"df-d3184e9f-f2c7-49ce-ad42-3c39f4a987eb\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model</th>\n","      <th>lr</th>\n","      <th>batch_size</th>\n","      <th>seq_length</th>\n","      <th>hidden_layer_size</th>\n","      <th>num_layers</th>\n","      <th>lossfunc</th>\n","      <th>mae_validation</th>\n","      <th>mae_validation_1w</th>\n","      <th>mae_validation_2w</th>\n","      <th>mae_validation_3w</th>\n","      <th>mae_validation_4w</th>\n","      <th>Target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3</th>\n","      <td>LSTM</td>\n","      <td>0.0001</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>256</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.426852</td>\n","      <td>0.352895</td>\n","      <td>0.395088</td>\n","      <td>0.530987</td>\n","      <td>0.591012</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>LSTM</td>\n","      <td>0.0001</td>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>64</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.435373</td>\n","      <td>0.329440</td>\n","      <td>0.404648</td>\n","      <td>0.453385</td>\n","      <td>0.543349</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>LSTM</td>\n","      <td>0.0005</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>128</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.435505</td>\n","      <td>0.369385</td>\n","      <td>0.450823</td>\n","      <td>0.557627</td>\n","      <td>0.656678</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>LSTM</td>\n","      <td>0.0001</td>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>128</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.436795</td>\n","      <td>0.378866</td>\n","      <td>0.480702</td>\n","      <td>0.577748</td>\n","      <td>0.567730</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>LSTM</td>\n","      <td>0.0001</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>128</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.439042</td>\n","      <td>0.346694</td>\n","      <td>0.401489</td>\n","      <td>0.469782</td>\n","      <td>0.580520</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>LSTM</td>\n","      <td>0.0001</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>64</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.440515</td>\n","      <td>0.303749</td>\n","      <td>0.409141</td>\n","      <td>0.491859</td>\n","      <td>0.594494</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>LSTM</td>\n","      <td>0.0001</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>256</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.457840</td>\n","      <td>0.311857</td>\n","      <td>0.439973</td>\n","      <td>0.503870</td>\n","      <td>0.588642</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>LSTM</td>\n","      <td>0.0005</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>256</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.460310</td>\n","      <td>0.352440</td>\n","      <td>0.405706</td>\n","      <td>0.487385</td>\n","      <td>0.639533</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>LSTM</td>\n","      <td>0.0005</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>256</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.462349</td>\n","      <td>0.378433</td>\n","      <td>0.419067</td>\n","      <td>0.513607</td>\n","      <td>0.603954</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>LSTM</td>\n","      <td>0.0005</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>32</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.463938</td>\n","      <td>0.397804</td>\n","      <td>0.412176</td>\n","      <td>0.506332</td>\n","      <td>0.606075</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>LSTM</td>\n","      <td>0.0001</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>32</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.464772</td>\n","      <td>0.337532</td>\n","      <td>0.407699</td>\n","      <td>0.486225</td>\n","      <td>0.613726</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>LSTM</td>\n","      <td>0.0005</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>64</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.465347</td>\n","      <td>0.355826</td>\n","      <td>0.369638</td>\n","      <td>0.490508</td>\n","      <td>0.668149</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>LSTM</td>\n","      <td>0.0005</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>256</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.466199</td>\n","      <td>0.387392</td>\n","      <td>0.425529</td>\n","      <td>0.543252</td>\n","      <td>0.624078</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>LSTM</td>\n","      <td>0.0001</td>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>256</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.466804</td>\n","      <td>0.351329</td>\n","      <td>0.400969</td>\n","      <td>0.524578</td>\n","      <td>0.579201</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>LSTM</td>\n","      <td>0.0001</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>64</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.467254</td>\n","      <td>0.411430</td>\n","      <td>0.491755</td>\n","      <td>0.532778</td>\n","      <td>0.667935</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>LSTM</td>\n","      <td>0.0001</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>256</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.473991</td>\n","      <td>0.351607</td>\n","      <td>0.464153</td>\n","      <td>0.599402</td>\n","      <td>0.611114</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>LSTM</td>\n","      <td>0.0005</td>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>64</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.476240</td>\n","      <td>0.353054</td>\n","      <td>0.499141</td>\n","      <td>0.590657</td>\n","      <td>0.700102</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>LSTM</td>\n","      <td>0.0005</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>128</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.476550</td>\n","      <td>0.372144</td>\n","      <td>0.443574</td>\n","      <td>0.498280</td>\n","      <td>0.510959</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>LSTM</td>\n","      <td>0.0001</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>128</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.477137</td>\n","      <td>0.331465</td>\n","      <td>0.469184</td>\n","      <td>0.525895</td>\n","      <td>0.619795</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>LSTM</td>\n","      <td>0.0001</td>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>32</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.483024</td>\n","      <td>0.362525</td>\n","      <td>0.504956</td>\n","      <td>0.574154</td>\n","      <td>0.605740</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>LSTM</td>\n","      <td>0.0001</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>32</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.483506</td>\n","      <td>0.381431</td>\n","      <td>0.449010</td>\n","      <td>0.519395</td>\n","      <td>0.611599</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>LSTM</td>\n","      <td>0.0005</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>64</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.485846</td>\n","      <td>0.375607</td>\n","      <td>0.484300</td>\n","      <td>0.620943</td>\n","      <td>0.730378</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>LSTM</td>\n","      <td>0.0005</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>128</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.488707</td>\n","      <td>0.465943</td>\n","      <td>0.514321</td>\n","      <td>0.572934</td>\n","      <td>0.680925</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>LSTM</td>\n","      <td>0.0001</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>128</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.490433</td>\n","      <td>0.291622</td>\n","      <td>0.406788</td>\n","      <td>0.523998</td>\n","      <td>0.670938</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>LSTM</td>\n","      <td>0.0001</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>64</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.494584</td>\n","      <td>0.413815</td>\n","      <td>0.480104</td>\n","      <td>0.571804</td>\n","      <td>0.710071</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>LSTM</td>\n","      <td>0.0005</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>64</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.498100</td>\n","      <td>0.351849</td>\n","      <td>0.460041</td>\n","      <td>0.533022</td>\n","      <td>0.646347</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>LSTM</td>\n","      <td>0.0005</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>32</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.514190</td>\n","      <td>0.359696</td>\n","      <td>0.507152</td>\n","      <td>0.560379</td>\n","      <td>0.678729</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>LSTM</td>\n","      <td>0.0005</td>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>256</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.533374</td>\n","      <td>0.395636</td>\n","      <td>0.512391</td>\n","      <td>0.614436</td>\n","      <td>0.652828</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>LSTM</td>\n","      <td>0.0005</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>32</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.557384</td>\n","      <td>0.423985</td>\n","      <td>0.511819</td>\n","      <td>0.662924</td>\n","      <td>0.746738</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>LSTM</td>\n","      <td>0.0005</td>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>32</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.558291</td>\n","      <td>0.438428</td>\n","      <td>0.544349</td>\n","      <td>0.686212</td>\n","      <td>0.817922</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>LSTM</td>\n","      <td>0.0001</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>32</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.558995</td>\n","      <td>0.407561</td>\n","      <td>0.554945</td>\n","      <td>0.677329</td>\n","      <td>0.775544</td>\n","      <td>rate</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>LSTM</td>\n","      <td>0.0005</td>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>128</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.608376</td>\n","      <td>0.414908</td>\n","      <td>0.515664</td>\n","      <td>0.650835</td>\n","      <td>0.806946</td>\n","      <td>rate</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3184e9f-f2c7-49ce-ad42-3c39f4a987eb')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d3184e9f-f2c7-49ce-ad42-3c39f4a987eb button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d3184e9f-f2c7-49ce-ad42-3c39f4a987eb');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-7abd658b-4919-4a51-9b25-693d22b641f8\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7abd658b-4919-4a51-9b25-693d22b641f8')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-7abd658b-4919-4a51-9b25-693d22b641f8 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_d3ed6eff-9428-4a77-ad8b-4c381a6c7f17\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('summarytable')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_d3ed6eff-9428-4a77-ad8b-4c381a6c7f17 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('summarytable');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"summarytable","summary":"{\n  \"name\": \"summarytable\",\n  \"rows\": 32,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"LSTM\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0002032002032003048,\n        \"min\": 0.0001,\n        \"max\": 0.0005,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0005\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"seq_length\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 4,\n        \"max\": 10,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hidden_layer_size\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 32,\n        \"max\": 256,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_layers\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lossfunc\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"SmoothL1Loss()\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mae_validation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.041654716101401264,\n        \"min\": 0.4268521621794503,\n        \"max\": 0.6083764865335278,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          0.558291277413182\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mae_validation_1w\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03886053058273342,\n        \"min\": 0.2916221387661982,\n        \"max\": 0.46594252283168863,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          0.43842831202407223\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mae_validation_2w\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0492452352727053,\n        \"min\": 0.36963830503060785,\n        \"max\": 0.5549447768999782,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          0.5443487187288139\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mae_validation_3w\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06115602593967201,\n        \"min\": 0.45338492699538924,\n        \"max\": 0.6862116901025697,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          0.6862116901025697\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mae_validation_4w\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07247013413811015,\n        \"min\": 0.5109592069932654,\n        \"max\": 0.8179222217521308,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          0.8179222217521308\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"rate\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["df_summary = pd.read_csv('/content/drive/MyDrive/Flu Forecasting/LSTM/US result/2023-12-01summarytest.csv')\n","df_summary = df_summary.sort_values(by='mae_validation', ascending=True)\n","df_summary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":118},"id":"lfrkE31iZCCZ","executionInfo":{"status":"ok","timestamp":1712716548908,"user_tz":-480,"elapsed":620,"user":{"displayName":"Shawn XU","userId":"03794891599117456463"}},"outputId":"17b8568e-98af-482f-da74-0e18a498be89"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Unnamed: 0 Model      lr  batch_size  seq_length  week_range  \\\n","0           0  LSTM  0.0001           2           8          64   \n","\n","   hidden_layer_size  num_layers        lossfunc  mae_validation  \\\n","0                128           1  SmoothL1Loss()        0.420783   \n","\n","   mae_validation_1w  mae_validation_2w  mae_validation_3w  mae_validation_4w  \\\n","0           0.311044           0.395818           0.511261           0.591123   \n","\n","  Target  \n","0   rate  "],"text/html":["\n","  <div id=\"df-13a9c8ec-4823-40b2-a3dc-481130987690\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Model</th>\n","      <th>lr</th>\n","      <th>batch_size</th>\n","      <th>seq_length</th>\n","      <th>week_range</th>\n","      <th>hidden_layer_size</th>\n","      <th>num_layers</th>\n","      <th>lossfunc</th>\n","      <th>mae_validation</th>\n","      <th>mae_validation_1w</th>\n","      <th>mae_validation_2w</th>\n","      <th>mae_validation_3w</th>\n","      <th>mae_validation_4w</th>\n","      <th>Target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>LSTM</td>\n","      <td>0.0001</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>64</td>\n","      <td>128</td>\n","      <td>1</td>\n","      <td>SmoothL1Loss()</td>\n","      <td>0.420783</td>\n","      <td>0.311044</td>\n","      <td>0.395818</td>\n","      <td>0.511261</td>\n","      <td>0.591123</td>\n","      <td>rate</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13a9c8ec-4823-40b2-a3dc-481130987690')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-13a9c8ec-4823-40b2-a3dc-481130987690 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-13a9c8ec-4823-40b2-a3dc-481130987690');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","  <div id=\"id_44537bab-901a-4fc5-be35-9de7dfe06cd7\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_summary')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_44537bab-901a-4fc5-be35-9de7dfe06cd7 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_summary');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_summary","summary":"{\n  \"name\": \"df_summary\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"LSTM\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0001,\n        \"max\": 0.0001,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"seq_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 8,\n        \"max\": 8,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"week_range\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hidden_layer_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 128,\n        \"max\": 128,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lossfunc\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"SmoothL1Loss()\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mae_validation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.4207833804050701,\n        \"max\": 0.4207833804050701,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.4207833804050701\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mae_validation_1w\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.3110439403373207,\n        \"max\": 0.3110439403373207,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.3110439403373207\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mae_validation_2w\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.3958184278592329,\n        \"max\": 0.3958184278592329,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.3958184278592329\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mae_validation_3w\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.5112614270195714,\n        \"max\": 0.5112614270195714,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5112614270195714\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mae_validation_4w\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.591122712033144,\n        \"max\": 0.591122712033144,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.591122712033144\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"rate\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["##  Forecasting 1000 times"],"metadata":{"id":"9KIatH2JbfEA"}},{"cell_type":"code","source":["variable_combinations = []\n","#summarytable = df_summary\n","filtered_df_summary = summarytable[summarytable['week_range'] != 32]\n","\n","# Group by 'region' and then find the row with minimum 'mae_validation' for each group\n","min_mae_row = filtered_df_summary.loc[filtered_df_summary['mae_validation'].idxmin()]\n","\n","# Extract the relevant details\n","variable_combinations= {\n","    'lr': min_mae_row['lr'],\n","    'week_range': int(min_mae_row['week_range']),\n","    'seq_length': int(min_mae_row['seq_length']),\n","    'hidden_layer_size': int(min_mae_row['hidden_layer_size']),\n","    'num_layers': int(min_mae_row['num_layers'])\n","}\n","variable_combinations"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EUaMnfULbT5W","executionInfo":{"status":"ok","timestamp":1712646253692,"user_tz":-480,"elapsed":6,"user":{"displayName":"Shawn XU","userId":"03794891599117456463"}},"outputId":"47a2daf7-e2f3-4dc8-bfe2-9a5bdb9d2c59"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'lr': 0.0001,\n"," 'week_range': 64,\n"," 'seq_length': 8,\n"," 'hidden_layer_size': 128,\n"," 'num_layers': 1}"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["\n","#test_week = '2023-10-01'\n","#test_week = '2023-11-01'\n","#test_week = '2023-12-01'\n","test_week = '2024-01-01'\n","#test_week = '2024-02-01'\n","result = LSTM_Prediction(test_week,variable_combinations)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n11OMVQADX3n","executionInfo":{"status":"ok","timestamp":1712646400841,"user_tz":-480,"elapsed":147153,"user":{"displayName":"Shawn XU","userId":"03794891599117456463"}},"outputId":"e8d9ba2c-d3ef-4d67-ae8e-a9570be862a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]}]},{"cell_type":"code","source":["from datetime import datetime, timedelta\n","\n","\n","start_date = datetime.strptime('2023-10-07', '%Y-%m-%d')\n","#start_date = datetime.strptime('2023-12-23', '%Y-%m-%d')\n","end_date = datetime.strptime('2024-02-24', '%Y-%m-%d')\n","current_date = start_date\n","dates_list = []\n","\n","while current_date <= end_date:\n","    dates_list.append(current_date.strftime('%Y-%m-%d'))\n","    current_date += timedelta(days=7)\n","dates_list\n","test_weeks = dates_list\n","for test_week in test_weeks:\n","    print(test_week)\n","    file_path = f'/content/drive/MyDrive/Covid forecasting/Result/US level/raw output/{test_week}summarytest.csv'\n","\n","    # Load the summary table, sort it, and filter\n","    df_summary = pd.read_csv(file_path)\n","    df_summary = df_summary.sort_values(by='mae_validation', ascending=True)\n","    filtered_df_summary = df_summary\n","    min_mae_row = filtered_df_summary.loc[filtered_df_summary['mae_validation'].idxmin()]\n","\n","    # Extract the relevant details\n","    variable_combinations= {\n","        'lr': min_mae_row['lr'],\n","        'seq_length': int(min_mae_row['seq_length']),\n","        'hidden_layer_size': int(min_mae_row['hidden_layer_size']),\n","    }\n","    result = LSTM_Prediction(test_week,variable_combinations)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"16AzJSHzbZO9f98fabRgm6IVUnMmHl9nB"},"id":"KYkyhEjU2qL1","outputId":"27f05870-c36a-4ad7-a3d3-b5d08a0b4d91","executionInfo":{"status":"ok","timestamp":1718337042114,"user_tz":-480,"elapsed":48421,"user":{"displayName":"Shawn XU","userId":"03794891599117456463"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["from google.colab import runtime\n","runtime.unassign()"],"metadata":{"id":"V1L1A2VuLdhf"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4","toc_visible":true,"authorship_tag":"ABX9TyNsvp+FJud2muXTiUCYeWiK"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}